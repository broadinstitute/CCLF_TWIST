{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: double check repo\n",
    "# JKBio repo commit: 912087536d3cf6a7f1cbb00f9b131bc645780ee9\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "# pathtoJK = \"../JKBio\"\n",
    "# sys.path.insert(0, pathtoJK)\n",
    "pathtoJK_parent = \"../\"\n",
    "sys.path.append(pathtoJK_parent)\n",
    "from JKBio import terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "from src.helper import *\n",
    "\n",
    "from JKBio.utils import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "**Note that the comment \"# TO EDIT:\" is included before any line of code that needs to be changed between different runs of this notebook**\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch in *chronological* order (e.g. CCLF_TWIST1 before CCLF_TWIST2)\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "samplesetnames = ['CCLF_TWIST35']\n",
    "\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "# TO EDIT:\n",
    "gsheeturllist = ['https://docs.google.com/spreadsheets/d/1v7QcP6ChUKM6jD4ngMMBb1_gzVT26hgeMHDThz_WWlI/edit#gid=0']\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"terra-broad-cancer-prod/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preliminary versions of the sample and metadata tables\n",
    "newsamples, newmetadata = create_preliminary_sample_and_metadata_tables(wto, wfrom, external_sheets_url_list=gsheeturllist, cohorts2id_url=cohorts2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required metadata columns\n",
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Patient ID <- not sure about adding this requirement, but it will be used when plotting the CNV heat maps\n",
    "- Sample Type\n",
    "- ~~Tumor Type~~ <- this won't be populated for normals.\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- this only works if the normals also have a primary disease associated with them, which they should. Only the technical controls won't have this information.\n",
    "- ~~Media on Tube~~ <- tissue samples won't have a media but we do want to include them\n",
    "- Collection\n",
    "- Tissue Site <- This column should eventually be populated\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)\n",
    "\n",
    "# specify required metadata columns\n",
    "tolook = ['Collaborator Participant ID','Exported DNA SM-ID', 'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site', 'Original Material Type', 'Material Type','Primary Disease', 'Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any samples are missing some of the required metadata, stop now and ask the CCLF team to fill out the missing values in the External Sheet.\n",
    "check_required_metadata_columns(df, tolook, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep samples that have all the appropriate metadata information\n",
    "df = check_required_metadata_columns(df, required_metadata_cols=tolook, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample df to upload to Terra\n",
    "sample_info = create_sample_df_for_terra(df, cohorts2id_url=cohorts2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should be what you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "\n",
    "## check: create directory \"data/sample_infos\" if does not exist\n",
    "filepath = 'data/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file you just saved\n",
    "filepath = 'data/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpairs = create_pairs_table(sample_info, wto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))\n",
    "    wto.upload_samples(sample_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uploading pairs...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairs per batch dictionary\n",
    "dict_pairs_per_batch = create_dict_of_pairs_per_sampleset(newpairs, sample_info, samplesetnames, save=True)\n",
    "\n",
    "# Load from saved file\n",
    "dict_pairs_per_batch = np.load('dict_pairs_per_batch.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uploading to Terra takes time. Right now, I iterate over cohorts and upload during the iteration. This means that if 2 batches have samples belonging to the same cohort, I upload to that cohort's sample set in Terra 2 times. This is inefficient. We could greatly speed this up by only uploading the cohort-level pairsets and samplesets at the end. That would mean tracking for each cohort all of the new samples (across each batch), then updating Terra at the end.\n",
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {}\n",
    "for i, current_batch in enumerate(samplesetnames):\n",
    "    # upload a pair set for each batch\n",
    "    terra.addToPairSet(proc_workspace, samplesetnames_pairs[i], dict_pairs_per_batch[current_batch])\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            terra.addToSampleSet(proc_workspace, val, cohortsamples)\n",
    "            \n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            terra.addToPairSet(proc_workspace,val, pairsamples)\n",
    "            \n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets for each batch...\")\n",
    "# want to create a sample set for each batch\n",
    "for i, current_batch in enumerate(samplesetnames):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == current_batch]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if _['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if _['sample_type'] == \"Tumor\"]\n",
    "    # create 3 batch-level sample sets: all, tumors, normals\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_all[i], samples=batch_sample_info.index.tolist())\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_tumors[i], samples=batch_tumorsid)\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_normals[i], samples=batch_normalsid)\n",
    "\n",
    "print(\"creating sample sets for all samples in workspace, and all normals in workspace...\")\n",
    "# create sample set for all normals in workspace (will use all combined normals in PoN for mutation calling)\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"])\n",
    "terra.addToSampleSet(proc_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "terra.addToSampleSet(proc_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Push to Git repo (CCLF_TWIST) now!\n",
    "This way, other people can easily take over the process of running the pipelines and feel confident that they have the most up-to-date version of this Jupyter notebook.\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(proc_workspace, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(proc_workspace, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(proc_workspace, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updates the config for each batch id\n",
    "CreatePanelOfNormalsGATK_PANCAN = []\n",
    "DepthOfCovQC_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config for workflow that creates the PON for CNV calling\n",
    "    createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "    # edit the config\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    createPON_config['outputs']\n",
    "    # update the config in Terra\n",
    "    wto.update_config(createPON_config)\n",
    "    \n",
    "    # create batch-specific PON to be used for CNVs\n",
    "    CreatePanelOfNormalsGATK_PANCAN.append(wto.create_submission(\"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals[ind]))\n",
    "    DepthOfCovQC_PANCAN.append(wto.create_submission(\"DepthOfCovQC_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config\n",
    "    CNV_config = wto.get_config('CallSomaticCNV_PANCAN')\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon']\n",
    "\n",
    "    # edit the config\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    CNV_config['inputs']\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(CNV_config)\n",
    "    CallSomaticCNV_PANCAN.append(wto.create_submission(\"CallSomaticCNV_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples', use_callcache = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(proc_workspace, '8b11f656-c517-49f4-ab3c-d71f13c7ae31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info, all_pairsets, cohorts_per_batch, cohort_pairsets = regenerate_variables(workspace=wto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNV map for each batch\n",
    "terra.createManySubmissions(proc_workspace, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all, use_callcache = False)\n",
    "# create CNV map for each cohort\n",
    "terra.createManySubmissions(proc_workspace, \"PlotSomaticCNVMaps_PANCAN\", list(all_changed_cohorts), use_callcache = False)\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(proc_workspace, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(proc_workspace, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=True)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current config\n",
    "mutect1_config = wto.get_config('CreatePoNSNV_Mutect1')\n",
    "mutect2_config = wto.get_config('CreatePoN_SNV_MuTect2')\n",
    "\n",
    "# edit the config\n",
    "mutect1_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1\"'\n",
    "mutect2_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2\"'\n",
    "mutect1_config['outputs']['CreatePanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "mutect2_config['outputs']['CreatePanelOfNormals.createPanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "\n",
    "# update the config in Terra\n",
    "wto.update_config(mutect1_config)\n",
    "wto.update_config(mutect2_config)\n",
    "\n",
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: It may be okay if some samples fail the MutationCalling_Tumors_TWIST workflow. Samples will fail if no mutations made it through Mutect1 and Mutect2's filters.\n",
    "The MutationCalling_Tumors_TWIST pipeline has been updated to use GATK4, and there are many more pre-filters for Mutect2 that greatly reduce the computation time required. As part of this change, however, we discovered that the next step (FilterMutectCalls) will fail if the vcf it gets from Mutect2 is empty. This can happen if all the variants are filtered out. Thus, long story short, if the sample fails at the FilterMutectCalls step and the log file shows that there were no variants left after Mutect1 and Mutect2, then this failure is not something to worry about.\n",
    "\n",
    "The details can be found at https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2 by searching for \"Read filters\". In addition to the prefilters described in that section, Mutect2 also prefilters sites that are in the matched normal and the PoN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = []\n",
    "MutationCalling_Tumors_TWIST = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    \n",
    "    # get config \n",
    "    mutcall_tumor = wto.get_config('MutationCalling_Tumors_TWIST')\n",
    "\n",
    "    # edit the config\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "    \n",
    "    # check config\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'])\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutcall_tumor)\n",
    "\n",
    "    # create submission\n",
    "    SNV_PostProcessing_Normals += [wto.create_submission(\"SNV_PostProcessing_Normals\", samplesetnames_normals[ind])]\n",
    "    \n",
    "    MutationCalling_Tumors_TWIST += [wto.create_submission(\"MutationCalling_Tumors_TWIST\", samplesetnames_pairs[ind], etype='pair_set', expression='this.pairs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNV_PostProcessing_Normals = [','.join(SNV_PostProcessing_Normals)]\n",
    "# MutationCalling_Tumors_TWIST = [','.join(MutationCalling_Tumors_TWIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'SNV_PostProcessing_Normals' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: you might see that some of the cohorts fail on this workflow. That can be expected: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(proc_workspace, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: delete after 11/16/2020\n",
    "# # Code to re-generate all of the cohort-level plots and TSVs.\n",
    "# all_cohorts = wto.get_samples()['cohorts'].unique().tolist()\n",
    "# all_cohorts = [a for a in all_cohorts if a != 'nan' and isinstance(a, str)]\n",
    "# len(all_cohorts)\n",
    "\n",
    "# # create CNV map for each cohort\n",
    "# terra.createManySubmissions(proc_workspace, \"PlotSomaticCNVMaps_PANCAN\", list(all_cohorts), use_callcache = False)\n",
    "\n",
    "\n",
    "# all_pair_sets = wto.get_entities('pair_set').index.tolist()\n",
    "# all_pair_cohorts = [a for a in all_cohorts if a in all_cohorts_pairs]\n",
    "# # create aggregate SNV tsvs for each cohort\n",
    "# terra.createManySubmissions(proc_workspace, \"SNVPostProcessing_TWIST\", list(all_pair_cohorts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregate SNV tsvs for each batch\n",
    "terra.createManySubmissions(proc_workspace, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "# create aggregate SNV tsvs for each cohort\n",
    "terra.createManySubmissions(proc_workspace, \"SNVPostProcessing_TWIST\", list(cohort_pairsets))\n",
    "print(\"Submitted final jobs for SNV pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprinting (FNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the FNG pileup counts for each sample\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(proc_workspace, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"FNG_Compile_db_slow_download\" command is problematic currently\n",
    "**NOTE**: The \"FNG_Compile_db_slow_download\" command is problematic currently in this workspace because this workspace only contains TWIST samples, but we want to be able to look at fingerprinting data from both TSCA and TWIST. Currently, Gwen merges the previous fingerprinting_db.txt file with the newly created fingerprinting_db.txt. We'll have to repeat this merging procedure unless we edit the workflow. Gwen has started this process, but hasn't finished the edits (just need to build the proper docker container). So for now (1/15/20), still need to do the merging locally (unfortunately).\n",
    "\n",
    "See the R file: \"FNG_TWIST_and_TSCA_merge\" in cclf_ccle. (TODO: resolve reference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the proper \"super\" set of samples to run through the FNG compiler\n",
    "# if processing multiple batches at once, this will correspond to a sample \"super\" set containing the sample IDs from all the new batches. If processing a single batch, this will just be that batch's sample set.\n",
    "samples_to_add = []\n",
    "for set_name in samplesetnames_all:\n",
    "#     samples_to_add += wto.get_sample_attributes_in_set(set_name).index.tolist()\n",
    "    samples_to_add += wto.get_sample_sets().loc[set_name, 'samples']\n",
    "samples_to_add\n",
    "print(samples_to_add)\n",
    "\n",
    "fng_sampleset_id = \"_\".join(samplesetnames)\n",
    "print(fng_sampleset_id)\n",
    "\n",
    "terra.addToSampleSet(workspace = proc_workspace, samplesetid = fng_sampleset_id, samples = samples_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the output config to create the new FNG database\n",
    "# get current config for the FNG compiling workflow\n",
    "fngCompile_config = wto.get_config('FNG_Compile_db_slow_download')\n",
    "# edit the config\n",
    "# TODO: would be nice to be able to change the name of the outputted FNG database. Right now, all named the same.\n",
    "# fngCompile_config['inputs']['FNG_Compile_db.compile_db.output_file_name'] = '\"fingerprinting_db_through_' + samplesetnames[-1] + '.txt\"'\n",
    "fngCompile_config['outputs']['FNG_Compile_db.compile_db.fingerprinting_db'] = 'workspace.fingerprinting_db_through_' + samplesetnames[-1]\n",
    "fngCompile_config['outputs']['FNG_Compile_db.compile_db.fingerprinting_db_current'] = 'workspace.fingerprinting_db'\n",
    "fngCompile_config['outputs']\n",
    "\n",
    "print(fngCompile_config)\n",
    "# update the config in Terra\n",
    "wto.update_config(fngCompile_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FNG db using Method Version 7\n",
    "# TODO: update method to change the output file name to something more descriptive\n",
    "# pass in a sample set containing all of the new samples you're processing\n",
    "# do not use call cache; we need to see if the github repo has been updated and thus must clone each time\n",
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", fng_sampleset_id, use_callcache=False)\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: this is where the merging work in R needs to be performed on local.\n",
    "Would be fairly simple to write this as a python script instead... and that way, I wouldn't have to keep on downloading and uploading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each batch, query the FNG database\n",
    "FNG_Query_db = terra.createManySubmissions(proc_workspace, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra. Update the Asana task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If got a new cohort label / abbreviation and need to update data that already exists in Terra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from Gsheet metadata\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "\n",
    "display(metadata.head())\n",
    "\n",
    "# Pull relevant sample_info from Terra\n",
    "sample_info = wto.get_samples()\n",
    "sample_info = sample_info[sample_info[\"batch\"].isin(samplesetnames)]\n",
    "display(sample_info.head())\n",
    "# display(sample_info.loc[:,[\"cohorts\", \"Collection\"]])\n",
    "\n",
    "# Merge new Metadata with stuff existing in Terra (in particular, we often want to update the Collections and cohorts columns)\n",
    "updated = pd.concat([sample_info.drop(columns=['Collection']), metadata[\"Collection\"].reindex(sample_info.index)], axis=1)\n",
    "updated.head()\n",
    "updated.columns.tolist()\n",
    "updated = updated.reindex(columns=(['Collection', 'cohorts'] + list([a for a in updated.columns if a not in ['Collection', 'cohorts']]) ))\n",
    "updated\n",
    "\n",
    "updated = getCohortAbbreviations(updated)\n",
    "updated\n",
    "print(\"Final 'updated' df, looking at just the cohorts and Collection columns:\")\n",
    "display(updated.loc[:,[\"cohorts\", \"Collection\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete\n",
    "sample_info = updated\n",
    "\n",
    "# Now, go run the code chunks in the normal pipeline for \"Creating the pairs\", and \"Create pair sets and sample sets\"\n",
    "# Then you'll be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably also need to make a sample set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
