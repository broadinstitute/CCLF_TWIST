{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "import ipdb\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "samplesetnames = [\"CCLF_TWIST1\",\"CCLF_TWIST2\",\"CCLF_TWIST3\",\"CCLF_TWIST4\"]\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "date=\"2019\" # not using currently; could also get this from release_date column (in Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq). Might be useful to include?\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "# proc_workspace=\"CCLF_Targeted\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\"\n",
    "#mapping abbreviations to full names/descriptions\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\",\n",
    "\"https://docs.google.com/spreadsheets/d/1S3DqBdVkd9dLP1PDYcdSWuD2Iy2gJpzuYBhvmP37UxU\",\n",
    "\"https://docs.google.com/spreadsheets/d/1kVIeIw66AxWLhAZlqUnAY17S87Rtfhijf1o3x0hG3Jw\",\n",
    "\"https://docs.google.com/spreadsheets/d/1tZQpxag7BO46pei3s_KaoHvxwN9EVESk3xYvzW7f7Uo/\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we look at all the samples we already have in the workspace\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "\n",
    "# get the data from google sheets\n",
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]\n",
    "\n",
    "# add a column with batch information (e.g. TWIST1 vs TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False)\n",
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the existing Batch Number column is clearly not well-maintained. This is why we add in a column with batch information when creating sample_info.\n",
    "metadata['Batch Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with batch information (e.g. CCLF_TWIST1 vs CCLF_TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch'}).drop(['level_1'], axis = 'columns')\n",
    "\n",
    "# we look at all the samples we already have\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID']) \n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata\n",
    "\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these as we dont have metadata for them: ' + '\\n' + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")\n",
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples['bait_set'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(newmetadata[['batch','external_id_validation','External ID']].to_string())\n",
    "# get all the external IDs into one column:\n",
    "newmetadata['external_id_validation'] = newmetadata['external_id_validation'].fillna(newmetadata['External ID'])\n",
    "print(newmetadata[['batch','external_id_validation','External ID']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('creating new sample information df')\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)\n",
    "# from this new set we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']]\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "# sample_info['pt_id'] = df['PT_ID'].astype(str) # add the patient ID; currently not in the External Sheets; Remi will add\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "# sample_info['external_id_validation'] = [i +'_'+ str(refsamples[refsamples['external_id_validation'] == i].shape[1]) if refsamples[refsamples['external_id_validation'] == i].shape[0] > 0 else i for i in sample_info['reference_id']]\n",
    "\n",
    "sample_info['external_id_validation'] = 'NA'\n",
    "for i in range(len(sample_info['reference_id'])):\n",
    "    # external id for the sample; using str.contains instead of ==\n",
    "    # replace any \"/\" that exist with \"_\"; otherwise get errors because looks like new directory when try to build file paths\n",
    "    ext_id_for_sample = df[df.index == sample_info['reference_id'][i]]['external_id_validation'].values[0] \n",
    "    ext_id_for_sample = [ext_id_for_sample.replace('/', '_') for ext_id in ext_id_for_sample]\n",
    "    if refsamples[refsamples['external_id_validation'].str.contains(ext_id_for_sample)].shape[0] > 0: # maybe do this for all so we're consistent?\n",
    "        # tack on a number to distinguish external IDs that we have run more than once\n",
    "        num_in_workspace = refsamples[refsamples.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_already_seen_here = sample_info[sample_info.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_to_add = num_in_workspace + num_already_seen_here + 1\n",
    "        sample_info['external_id_validation'][i] = ext_id_for_sample +'_'+ str(num_to_add)\n",
    "    else:\n",
    "        sample_info['external_id_validation'][i] = ext_id_for_sample\n",
    "\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "filepath = './sample_info.tsv' # edit this if you want this to save to a different location\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Sample Type\n",
    "- Tumor Type\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease\n",
    "- Media on Tube\n",
    "- Collection\n",
    "- Tissue Site\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Since they don\\'t have full data, we are dropping: \\n' + \n",
    "      str(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID','Sample Type','Tumor Type',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Media on Tube','Collection','Tissue Site']].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "df = df.iloc[[j for j,i in enumerate(df[['Exported DNA SM-ID','Collaborator Participant ID',\n",
    "                                         'Stock DNA SM-ID','Sample Type','Tumor Type',\n",
    "                                         'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                         'Media on Tube','Collection','Tissue Site']].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(),sample_info.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace.\n",
    "\n",
    "**Note:** all the entities (e.g. sample, sample set, participant tsv) need to exist! Else it will raise an error and block further uploads to Terra. You can do this by just uploading TSVs with NA. The below code does this automatically for the sample TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))\n",
    "    \n",
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)\n",
    "\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch; we do not actually use this for anything. Could be used for QC.\n",
    "for i in range(len(samplesetnames)):\n",
    "    \n",
    "    wto.update_pair_set(samplesetnames_pairs[i], newpairs.index.tolist())\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(wto_namespace_workspace, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(wto_namespace_workspace,val, pairsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_all[i], sample_ids=batch_sample_info.index.tolist())\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_tumors[i], sample_ids=batch_tumorsid)\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_normals[i], sample_ids=batch_normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if val.sample_type == \"Normal\"]) # add pre-existing normals\n",
    "\n",
    "try:\n",
    "    terra.addToSampleSet(wto_namespace_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "try:\n",
    "    terra.addToSampleSet(wto_namespace_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, \n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration. This occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\\n",
    "    or interact with terra submissions from the Terra website. \\\n",
    "    https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(wto_namespace_workspace, combined_list)\n",
    "# terra.waitForSubmission(wto_namespace_workspace, CalculateTargetCoverage_PANCAN, DepthOfCov_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing to use just the normals from the batch, not all normals\n",
    "CreatePanelOfNormalsGATK_PANCAN = terra.createManySubmissions(wto, \"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals)\n",
    "DepthOfCovQC_PANCAN = terra.createManySubmissions(wto, \"DepthOfCovQC_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(wto_namespace_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_config('CallSomaticCNV_PANCAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = terra.createManySubmissions(wto, \"CallSomaticCNV_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache = False)\n",
    "\n",
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# had errors when using call caching on TWIST1-3. No errors for TWIST4\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=False)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")\n",
    "\n",
    "# CreatePoNSNV_Mutect1 = terra.createManySubmissions(wto, \"CreatePoNSNV_Mutect1\", 'All_normals_TWIST')\n",
    "# CreatePoN_SNV_MuTect2 = terra.createManySubmissions(wto, \"CreatePoN_SNV_MuTect2\", 'All_normals_TWIST')\n",
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "combined_list = CreatePoNSNV_Mutect1 + CreatePoN_SNV_MuTect2\n",
    "terra.waitForSubmission(wto_namespace_workspace, combined_list)\n",
    "# terra.waitForSubmission(wto_namespace_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-create sample_info by pulling in sample data from the Terra workspace\n",
    "sample_info = wto.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create / re-create cohorts_per_batch dictionary\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "all_changed_cohorts = set()\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch[batch_name] = cohorts_in_batch\n",
    "    all_changed_cohorts.update(cohorts_in_batch) # add all the new cohorts in this batch to the full list\n",
    "# cohorts_per_batch\n",
    "all_changed_cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: the workflow will fail when running on a cohort that has only 1 sample (requires 2+)\n",
    "\n",
    "# create CNV map for each cohort (regardless of the batch)\n",
    "for val in all_changed_cohorts:\n",
    "    wto.create_submission(\"PlotSomaticCNVMaps_PANCAN\", val)\n",
    "\n",
    "# create CNV map for each batch\n",
    "PlotSomaticCNVMaps_PANCAN = terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = terra.createManySubmissions(wto, \"SNV_PostProcessing_Normals\", samplesetnames_normals)\n",
    "MutationCalling_Tumors_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Tumors_TWIST\", samplesetnames_pairs, \n",
    "                                              entity='pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'SNV_PostProcessing' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(wto_namespace_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## note: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(wto, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs, \n",
    "#                                               entity='pair_set', expression='this.pairs')\n",
    "SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# sometimes get space errors when run FNG_Compile_Pileup_Cnt if use 4 GB; changed to 10 GB\n",
    "to_run = ['SM-IF6GU', 'SM-IF6H4', 'SM-IF6HX']\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(wto_namespace_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
