{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "samplesetnames = [\"CCLF_TWIST1\",\"CCLF_TWIST2\",\"CCLF_TWIST3\",\"CCLF_TWIST4\", \"CCLF_TWIST5\"]\n",
    "# samplesetnames = [\"CCLF_TWIST1\"]\n",
    "\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\"\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\", #TWIST1\n",
    "\"https://docs.google.com/spreadsheets/d/1S3DqBdVkd9dLP1PDYcdSWuD2Iy2gJpzuYBhvmP37UxU\", # TWIST2\n",
    "\"https://docs.google.com/spreadsheets/d/1kVIeIw66AxWLhAZlqUnAY17S87Rtfhijf1o3x0hG3Jw\", # TWIST3\n",
    "\"https://docs.google.com/spreadsheets/d/1tZQpxag7BO46pei3s_KaoHvxwN9EVESk3xYvzW7f7Uo/\", # TWIST4\n",
    "\"https://docs.google.com/spreadsheets/d/1iqVDNPJcMLbNgfdmoBIHwaDF4yaXlyeVFxEwu_fWsEo\"] # TWIST5                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA'], 'external_id_validation':['NA'], 'sample_type':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at all the samples we already have in the TWIST workspace\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "\n",
    "# get the External sheet data from google sheets\n",
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a column with batch information (e.g. CCLF_TWIST1 vs CCLF_TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "print(len(metadata))\n",
    "\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "\n",
    "# we look at all the samples we already have in Terra\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID'])\n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata\n",
    "\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these samples from the data workspace as we don\\'t have metadata for them: ' + '\\n' \n",
    "          + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")\n",
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: this should match some of the data in the External Sheet\n",
    "print(newmetadata[['batch','external_id_validation']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think this cell and the following cell needs to be moved prior to the creation of the sample_info dataframe - we didn't remove *any* samples from TWIST1-4 that were missing these metadata columns\n",
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Patient ID <- not sure about adding this requirement, but it will be used when plotting the CNV heat maps\n",
    "- Sample Type\n",
    "- ~~Tumor Type~~ <- this won't be populated for normals.\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- this only works if the normals also have a primary disease associated with them, which they should. Only the technical controls won't have this information.\n",
    "- ~~Media on Tube~~ <- tissue samples won't have a media but we do want to include them\n",
    "- Collection\n",
    "- Tissue Site <- This column should eventually be populated\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Since they don\\'t have full data, we will be dropping ' + str(len(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist())) + ' samples: \\n' + \n",
    "      str(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "\n",
    "\n",
    "\n",
    "# examine which columns in the External Sheet had missing information\n",
    "print('\\nNumber of NAs for each required column:')\n",
    "print(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have now dropped the samples for which we didn\\'t have full data')\n",
    "# only keep samples that have all the appropriate information\n",
    "df = df.iloc[[j for j,i in enumerate(df[['Exported DNA SM-ID','Collaborator Participant ID',\n",
    "                                         'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                         'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                         'Collection']].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('creating new sample information df')\n",
    "# from this filtered set of samples (df) we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']]\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['patient_id'] = df['Participant ID'].astype(str) ## cehck: this is a new column in the external sheet. The name may change.\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "\n",
    "# start building external_id_validation column\n",
    "sample_info['external_id_validation'] = 'nan'\n",
    "# for each SM-ID:\n",
    "for i in range(len(sample_info['reference_id'])):\n",
    "    # get the external id for the sample\n",
    "    ext_id_for_sample = df[df.index == sample_info['reference_id'][i]]['external_id_validation'].values[0]\n",
    "    # replace any \"/\" that exist with \"_\"; otherwise get errors because looks like new directory when try to build file paths\n",
    "    ext_id_for_sample = ext_id_for_sample.replace('/', '_') #[ext_id_for_sample.replace('/', '_') for ext_id in ext_id_for_sample]\n",
    "    \n",
    "    # tack on a number to distinguish external IDs that we have run more than once\n",
    "    # using str.contains because we want to ignore the tacked on numbers we've added to the ext_id (e.g. _1, _2)\n",
    "    # num of samples with this ext_id already in the workspace\n",
    "    num_in_workspace = refsamples[refsamples.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "    # num of samples with this ext_id that we've already seen in the data we're adding\n",
    "    try:\n",
    "        num_already_seen_here = sample_info[sample_info.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_to_add = num_in_workspace + num_already_seen_here + 1\n",
    "    except:\n",
    "        num_to_add = num_in_workspace + 1\n",
    "    sample_info['external_id_validation'][i] = ext_id_for_sample + '_' + str(num_to_add)\n",
    "\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should be the sample TSV you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "\n",
    "## check: create dir if does not exist\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file you just saved\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace.\n",
    "\n",
    "**Note:** all the entities (e.g. sample, sample set, participant tsv) need to exist! Else it will raise an error and block further uploads to Terra. You can do this by just uploading TSVs with NA. The below code does this automatically for the sample TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair => case_sample => look into samples => retrieve the batch, assign to the key\n",
    "dict_pairs_per_batch = {}\n",
    "for samplesetname in samplesetnames:\n",
    "    dict_pairs_per_batch[samplesetname] = []\n",
    "    \n",
    "for pair_id, pair in newpairs.iterrows():\n",
    "    case_sample = pair['case_sample']\n",
    "    # Retrieve the batch which sample belongs to\n",
    "    pair_s_batch = sample_info.loc[case_sample]['batch']\n",
    "    dict_pairs_per_batch[pair_s_batch].append(pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch; we do not actually use this for anything. Could be used for QC.\n",
    "for i in range(len(samplesetnames)):\n",
    "    current_batch = samplesetnames[i]\n",
    "    terra.addToPairSet(wto, samplesetnames_pairs[i], dict_pairs_per_batch[current_batch])\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(proc_workspace, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(proc_workspace,val, pairsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if _['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if _['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    ## check: worried that I'll just overwrite whatever samples sets I've made previously.\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_all[i], samples=batch_sample_info.index.tolist())\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_tumors[i], samples=batch_tumorsid)\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_normals[i], samples=batch_normalsid)\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_all[i], sample_ids=batch_sample_info.index.tolist())\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_tumors[i], sample_ids=batch_tumorsid)\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_normals[i], sample_ids=batch_normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"]) # add pre-existing normals\n",
    "\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create sample set for TWIST1-5\n",
    "# normal_samples = []\n",
    "# for samplesetid in samplesetnames_normals:\n",
    "#     prevsamples = wto.get_sample_sets()['samples'][samplesetid]\n",
    "#     normal_samples.extend(prevsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terra.addToSampleSet(proc_workspace, \"CCLF_TWIST1-5_normals\", normal_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set batch ID / batch number\n",
    "# # e.g. 'TWIST1'\n",
    "# batch_id = 'TWIST1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wto.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "# terra.waitForSubmission(proc_workspace, CalculateTargetCoverage_PANCAN, DepthOfCov_PANCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the output config \"normals_pon attribute\" CreatePanelOfNormalsGATK_PANCAN\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_id_normal = \"TWIST1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Update the config for TWIST1-5 aggregated PON\n",
    "# # get current config\n",
    "# createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "# # edit the config\n",
    "# createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id_normal\n",
    "# createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id_normal\n",
    "# createPON_config['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # update the config in Terra\n",
    "# wto.update_config(createPON_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create PON from all existing normals\n",
    "# CreatePanelOfNormalsGATK_PANCAN = terra.createManySubmissions(wto, \"CreatePanelOfNormalsGATK_PANCAN\", \"CCLF_TWIST1-5_normals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_ids = [\"TWIST1\", \"TWIST2\", \"TWIST3\", \"TWIST4\", \"TWIST5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Update the config for each batch id\n",
    "CreatePanelOfNormalsGATK_PANCAN = []\n",
    "DepthOfCovQC_PANCAN = []\n",
    "for ind, batch_id in enumerate(batch_ids):\n",
    "    # get current config\n",
    "    createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "    # edit the config\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    createPON_config['outputs']\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(createPON_config)\n",
    "    \n",
    "    # create batch-specific PON \n",
    "    CreatePanelOfNormalsGATK_PANCAN += terra.createManySubmissions(wto, \"CreatePanelOfNormalsGATK_PANCAN\", [samplesetnames_normals[ind]])\n",
    "    DepthOfCovQC_PANCAN += terra.createManySubmissions(wto, \"DepthOfCovQC_PANCAN\", [samplesetnames_all[ind]], \n",
    "                                                  entity='sample_set', expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the inputs config for CallSomaticCNV_PANCAN so that it uses the correct CallSomaticCNV.normals_pon (batch-specific PONs)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = []\n",
    "for ind, batch_id in enumerate(batch_ids):\n",
    "    # get current config\n",
    "    CNV_config = wto.get_config('CallSomaticCNV_PANCAN')\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon']\n",
    "\n",
    "    # edit the config\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    CNV_config['inputs']\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(CNV_config)\n",
    "\n",
    "    CallSomaticCNV_PANCAN += terra.createManySubmissions(wto, \"CallSomaticCNV_PANCAN\", [samplesetnames_all[ind]], \n",
    "                                                  entity='sample_set', expression='this.samples', use_callcache = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNV map for each batch\n",
    "PlotSomaticCNVMaps_PANCAN = terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# had errors when using call caching on TWIST1-3. No errors for TWIST4\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=False)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the \"PoN_name\" config (input) and the \"normals_pon_vcf\" config (output) for both CreatePoNSNV_Mutect1 and CreatePoN_SNV_MuTect2\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind, batch_id in enumerate(batch_ids):\n",
    "    # get current config\n",
    "    mutect1_config = wto.get_config('CreatePoNSNV_Mutect1')\n",
    "    mutect2_config = wto.get_config('CreatePoN_SNV_MuTect2')\n",
    "    \n",
    "    # print current version\n",
    "    print(mutect1_config['methodRepoMethod']['methodVersion'])\n",
    "    print(mutect2_config['methodRepoMethod']['methodVersion'])\n",
    "\n",
    "    # edit the config\n",
    "    mutect1_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + batch_id + '_all_vcf_mutect1\"'\n",
    "    mutect2_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + batch_id + '_all_vcf_mutect2\"'\n",
    "    mutect1_config['outputs']['CreatePanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect1'\n",
    "    mutect2_config['outputs']['CreatePanelOfNormals.createPanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect2'\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutect1_config)\n",
    "    wto.update_config(mutect2_config)\n",
    "\n",
    "    # create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 += [wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")]\n",
    "CreatePoN_SNV_MuTect2 += [wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")]\n",
    "\n",
    "    # CreatePoNSNV_Mutect1 += terra.createManySubmissions(wto, \"CreatePoNSNV_Mutect1\", 'All_normals_TWIST')\n",
    "    # CreatePoN_SNV_MuTect2 += terra.createManySubmissions(wto, \"CreatePoN_SNV_MuTect2\", 'All_normals_TWIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "combined_list = CreatePoNSNV_Mutect1 + CreatePoN_SNV_MuTect2\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "# terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-create sample_info by pulling in sample data from the Terra workspace\n",
    "sample_info = wto.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create / re-create cohorts_per_batch dictionary\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "all_changed_cohorts = set()\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch[batch_name] = cohorts_in_batch\n",
    "    all_changed_cohorts.update(cohorts_in_batch) # add all the new cohorts in this batch to the full list\n",
    "# cohorts_per_batch\n",
    "all_changed_cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the config for MutationCalling_Tumors_TWIST so that it uses the correct pon_mutect1 and pon_mutect2 (cumulative PONs from the first batch through the current batch)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = []\n",
    "MutationCalling_Tumors_TWIST = []\n",
    "for ind, batch_id in enumerate(batch_ids):\n",
    "    \n",
    "    # get config \n",
    "    mutcall_tumor = wto.get_config('MutationCalling_Tumors_TWIST')\n",
    "\n",
    "    # edit the config\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect1'\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect2'\n",
    "    \n",
    "    # check config\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'])\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutcall_tumor)\n",
    "\n",
    "    # create submission\n",
    "    SNV_PostProcessing_Normals += terra.createManySubmissions(wto, \"SNV_PostProcessing_Normals\", [samplesetnames_normals[ind]])\n",
    "    \n",
    "    MutationCalling_Tumors_TWIST += terra.createManySubmissions(wto, \"MutationCalling_Tumors_TWIST\", [samplesetnames_pairs[ind]],\n",
    "                                                                entity='pair_set', expression='this.pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"waiting for 'SNV_PostProcessing_Normals' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission bca94918-7ce5-4645-92e9-676d605728b0.\n",
      "Successfully created submission 39ff7c3f-6a65-49c0-a583-09b63c5b1516.\n",
      "Successfully created submission fe16eccd-122b-4cad-8ff3-b3fffa4ddd9a.\n",
      "Successfully created submission 2358a407-60fb-46e8-a7dc-b2eb5e23109e.\n",
      "Successfully created submission 794786ab-5cc3-431c-a7ca-4cc6627b7cba.\n",
      "waiting for 'FilterGermlineEvents_TumorSample'\n",
      "status is: Failed for 0 jobs in submission 0. 2 mn elapsed.\r"
     ]
    }
   ],
   "source": [
    "## note: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(wto, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs, \n",
    "#                                               entity='pair_set', expression='this.pairs')\n",
    "SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# sometimes get space errors when run FNG_Compile_Pileup_Cnt if use 4 GB; changed to 10 GB\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
