{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "**Note that the comment \"# TO EDIT:\" is included before any line of code that needs to be changed between different runs of this notebook**\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "# TO EDIT:\n",
    "samplesetnames = [\"CCLF_TWIST6\"]\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "# TO EDIT:\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LK-BL6uF9mrzeCQaGb5rmyt1wr6X5JXeIPM9vo8VAL4\"]\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA'], 'external_id_validation':['NA'], 'sample_type':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at all the samples we already have in the TWIST workspace\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "\n",
    "# get the External sheet data from google sheets\n",
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with batch information (e.g. CCLF_TWIST1 vs CCLF_TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "print(len(metadata))\n",
    "\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "\n",
    "# we look at all the samples we already have in Terra\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID'])\n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata\n",
    "\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these samples from the data workspace as we don\\'t have metadata for them: ' + '\\n' \n",
    "          + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")\n",
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should match some of the data in the External Sheet\n",
    "print(newmetadata[['batch','external_id_validation']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think this cell and the following cell needs to be moved prior to the creation of the sample_info dataframe - we didn't remove *any* samples from TWIST1-4 that were missing these metadata columns\n",
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Patient ID <- not sure about adding this requirement, but it will be used when plotting the CNV heat maps\n",
    "- Sample Type\n",
    "- ~~Tumor Type~~ <- this won't be populated for normals.\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- this only works if the normals also have a primary disease associated with them, which they should. Only the technical controls won't have this information.\n",
    "- ~~Media on Tube~~ <- tissue samples won't have a media but we do want to include them\n",
    "- Collection\n",
    "- Tissue Site <- This column should eventually be populated\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolook = ['Collaborator Participant ID','Exported DNA SM-ID', 'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site', 'Original Material Type', 'Material Type','Primary Disease', 'Collection']\n",
    "\n",
    "print('Since they don\\'t have full data, we will be dropping ' + str(len(df.iloc[[j for j,i in enumerate(df[tolook].isna().values.sum(1)) if i !=0]].index.tolist())) + ' samples: \\n' +  str(df.iloc[[j for j, i in enumerate(df[tolook].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "\n",
    "# examine which columns in the External Sheet had missing information\n",
    "print('\\nNumber of NAs for each required column:')\n",
    "print(df[tolook].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have now dropped the samples for which we didn\\'t have full data')\n",
    "# only keep samples that have all the appropriate information\n",
    "df = df.iloc[[j for j,i in enumerate(df[tolook].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('creating new sample information df')\n",
    "# from this filtered set of samples (df) we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']].copy()\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['patient_id'] = df['Participant ID'].astype(str) ## cehck: this is a new column in the external sheet. The name may change.\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "\n",
    "# start building external_id_validation column\n",
    "sample_info['external_id_validation'] = 'nan'\n",
    "# for each SM-ID:\n",
    "for i in range(len(sample_info['reference_id'])):\n",
    "    # get the external id for the sample\n",
    "    ext_id_for_sample = df[df.index == sample_info['reference_id'][i]]['external_id_validation'].values[0]\n",
    "    # replace any \"/\" that exist with \"_\"; otherwise get errors because looks like new directory when try to build file paths\n",
    "    ext_id_for_sample = ext_id_for_sample.replace('/', '_') #[ext_id_for_sample.replace('/', '_') for ext_id in ext_id_for_sample]\n",
    "    \n",
    "    # tack on a number to distinguish external IDs that we have run more than once\n",
    "    # using str.contains because we want to ignore the tacked on numbers we've added to the ext_id (e.g. _1, _2)\n",
    "    # num of samples with this ext_id already in the workspace\n",
    "    num_in_workspace = refsamples[refsamples.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "    # num of samples with this ext_id that we've already seen in the data we're adding\n",
    "    try:\n",
    "        num_already_seen_here = sample_info[sample_info.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_to_add = num_in_workspace + num_already_seen_here + 1\n",
    "    except:\n",
    "        num_to_add = num_in_workspace + 1\n",
    "    sample_info['external_id_validation'][i] = ext_id_for_sample + '_' + str(num_to_add)\n",
    "\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should be the sample TSV you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "\n",
    "## check: create dir if does not exist\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file you just saved\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace.\n",
    "\n",
    "**Note:** all the entities (e.g. sample, sample set, participant tsv) need to exist! Else it will raise an error and block further uploads to Terra. You can do this by just uploading TSVs with NA. The below code does this automatically for the sample TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair => case_sample => look into samples => retrieve the batch, assign to the key\n",
    "dict_pairs_per_batch = {}\n",
    "for samplesetname in samplesetnames:\n",
    "    dict_pairs_per_batch[samplesetname] = []\n",
    "    \n",
    "for pair_id, pair in newpairs.iterrows():\n",
    "    case_sample = pair['case_sample']\n",
    "    # Retrieve the batch which sample belongs to\n",
    "    pair_s_batch = sample_info.loc[case_sample]['batch']\n",
    "    dict_pairs_per_batch[pair_s_batch].append(pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {}\n",
    "for i, current_batch in enumerate(samplesetnames):\n",
    "    terra.addToPairSet(wto.namespace + '/' + wto.workspace, samplesetnames_pairs[i], dict_pairs_per_batch[current_batch])\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(proc_workspace, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(proc_workspace,val, pairsamples)\n",
    "            except KeyError: \n",
    "                # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i, current_batch in enumerate(samplesetnames):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == current_batch\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if _['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if _['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    ## check: worried that I'll just overwrite whatever samples sets I've made previously.\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_all[i], samples=batch_sample_info.index.tolist())\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_tumors[i], samples=batch_tumorsid)\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_normals[i], samples=batch_normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"])\n",
    "\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the output config \"normals_pon attribute\" CreatePanelOfNormalsGATK_PANCAN\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the config for each batch id\n",
    "CreatePanelOfNormalsGATK_PANCAN = []\n",
    "DepthOfCovQC_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config for workflow that creates the PON for CNV calling\n",
    "    createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "    # edit the config\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    createPON_config['outputs']\n",
    "    # update the config in Terra\n",
    "    wto.update_config(createPON_config)\n",
    "    \n",
    "    # create batch-specific PON to be used for CNVs\n",
    "    CreatePanelOfNormalsGATK_PANCAN.append(wto.create_submission(\"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals[ind]))\n",
    "    DepthOfCovQC_PANCAN.append(wto.create_submission(\"DepthOfCovQC_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\n",
      "1.0 of jobs Succeeded in submission 0.\n",
      "1.0 of jobs Succeeded in submission 1.sion 1. 3 mn elapsed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the inputs config for CallSomaticCNV_PANCAN so that it uses the correct CallSomaticCNV.normals_pon (batch-specific PONs)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated configuration nci-mimoun-bi-org/CallSomaticCNV_PANCAN\n",
      "Successfully created submission 8d553b03-093f-4304-b10b-cda1de1e83bf.\n"
     ]
    }
   ],
   "source": [
    "CallSomaticCNV_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config\n",
    "    CNV_config = wto.get_config('CallSomaticCNV_PANCAN')\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon']\n",
    "\n",
    "    # edit the config\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    CNV_config['inputs']\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(CNV_config)\n",
    "    CallSomaticCNV_PANCAN.append(wto.create_submission(\"CallSomaticCNV_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples', use_callcache = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for 'CallSomaticCNV_PANCAN'\n",
      "1.0 of jobs Succeeded in submission 0.sion 0. 7 mn elapsed..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "## create / re-create cohorts_per_batch dictionary\n",
    "# will use \"cohort_pairsets\" to create cohort-specific SNV tsv and CN heat map\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "all_changed_cohorts = set()\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = set()\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.update([val])\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch[batch_name] = cohorts_in_batch\n",
    "    all_changed_cohorts.update(cohorts_in_batch) # add all the new cohorts in this batch to the full list\n",
    "\n",
    "# list of the cohort pairsets affected by the new samples we added\n",
    "all_pairsets = wto.get_pair_sets().index.tolist()\n",
    "cohort_pairsets = set(all_changed_cohorts) - (set(all_changed_cohorts) - set(all_pairsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission 12871234-4f1f-40f3-bf88-9a32754a3c19.\n",
      "Successfully created submission 0776e962-edc1-4d56-bebc-35bbceffe36f.\n",
      "Successfully created submission 1904700b-ad0f-471b-ad18-c009429f7080.\n",
      "Successfully created submission e8baea0d-47bc-4ac2-b2ad-27f70d9e4f03.\n",
      "Successfully created submission fadcc408-c999-4f13-a418-e722161a0b66.\n",
      "Successfully created submission 3c43b712-3641-402d-bae4-ee47325fa757.\n",
      "Successfully created submission 5d1dd2ba-d040-4314-b023-87a70e8bb901.\n",
      "Successfully created submission 3ae6e7bd-001b-4b01-aa5b-72a3878b97d0.\n",
      "Successfully created submission ce137feb-fad5-4f2d-b39f-65154e84fe78.\n",
      "submitted final jobs for CNV pipeline\n",
      "you don't need to wait before moving onto the next cell\n"
     ]
    }
   ],
   "source": [
    "# create CNV map for each batch\n",
    "terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "# create CNV map for each cohort\n",
    "terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", list(all_changed_cohorts))\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission d0cc312a-8a5d-4295-ba7c-6238ec92de13.\n",
      "waiting for 'MutationCalling_Normals_TWIST'\n",
      "status is: Failed for 0 jobs in submission 0. 66 mn elapsed.\r"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    602\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: (54, 'ECONNRESET')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    640\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 641\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    602\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZeroReturnError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5d2615c39cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                               entity='sample_set', expression='this.samples')\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"waiting for 'MutationCalling_Normals_TWIST'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mterra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitForSubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_workspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutationCalling_Normals_TWIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dev/JKBio/TerraFunction.py\u001b[0m in \u001b[0;36mwaitForSubmission\u001b[0;34m(workspace, submissions, raise_errors)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"workflows\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mwcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Aborted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Failed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Succeeded'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/dalmatian/base.py\u001b[0m in \u001b[0;36mget_submission\u001b[0;34m(self, submission_id)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"\"\"Get submission metadata\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirecloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/firecloud/api.py\u001b[0m in \u001b[0;36mget_submission\u001b[0;34m(namespace, workspace, submission_id)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     uri = \"workspaces/{0}/{1}/submissions/{2}\".format(namespace,\n\u001b[1;32m   1098\u001b[0m                                             workspace, submission_id)\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m__get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_workflow_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkflow_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/firecloud/api.py\u001b[0m in \u001b[0;36m__get\u001b[0;34m(methcall, headers, root_url, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fiss_agent_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__SESSION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfcconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FISSFC call: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/dalmatian/wmanager.py\u001b[0m in \u001b[0;36m_firecloud_api_timeout_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         **{\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'timeout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtimeout_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         }\n\u001b[1;32m     77\u001b[0m     )\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         response = super(AuthorizedSession, self).request(\n\u001b[0;32m--> 208\u001b[0;31m             method, url, data=data, headers=request_headers, **kwargs)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# If the response indicated that the credentials needed to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))"
     ]
    }
   ],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission 1096b9af-583b-47db-8907-9f25ed0f6089.\n",
      "waiting for 'SNV_FilterGermline'\n",
      "1.0 of jobs Succeeded in submission 0.sion 0. 6 mn elapsed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# had errors when using call caching on TWIST1-3. No errors for TWIST4\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=False)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the \"PoN_name\" config (input) and the \"normals_pon_vcf\" config (output) for both CreatePoNSNV_Mutect1 and CreatePoN_SNV_MuTect2\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated configuration nci-mimoun-bi-org/CreatePoNSNV_Mutect1\n",
      "Successfully updated configuration nci-mimoun-bi-org/CreatePoN_SNV_MuTect2\n",
      "Successfully created submission fc67f570-cf43-4ccc-84e1-2dbaff110e12.\n",
      "Successfully created submission d6635126-5514-4be3-84a1-44063e40489e.\n"
     ]
    }
   ],
   "source": [
    "# get current config\n",
    "mutect1_config = wto.get_config('CreatePoNSNV_Mutect1')\n",
    "mutect2_config = wto.get_config('CreatePoN_SNV_MuTect2')\n",
    "\n",
    "# edit the config\n",
    "mutect1_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1\"'\n",
    "mutect2_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2\"'\n",
    "mutect1_config['outputs']['CreatePanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "mutect2_config['outputs']['CreatePanelOfNormals.createPanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "\n",
    "# update the config in Terra\n",
    "wto.update_config(mutect1_config)\n",
    "wto.update_config(mutect2_config)\n",
    "\n",
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\n",
      "1.0 of jobs Succeeded in submission 0.sion 0. 8 mn elapsed.\n",
      "1.0 of jobs Succeeded in submission 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fc67f570-cf43-4ccc-84e1-2dbaff110e12'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreatePoNSNV_Mutect1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the config for MutationCalling_Tumors_TWIST so that it uses the correct pon_mutect1 and pon_mutect2 (cumulative PONs from the first batch through the current batch)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace.Cum_PoN_CCLF_TWIST6_all_vcf_mutect1\n",
      "workspace.Cum_PoN_CCLF_TWIST6_all_vcf_mutect2\n",
      "Successfully updated configuration nci-mimoun-bi-org/MutationCalling_Tumors_TWIST\n",
      "Successfully created submission 884426e6-e8e5-4a16-b232-ccbe295914e3.\n",
      "Successfully created submission 8637a74e-f93d-4ba6-8e2a-6e1b2efeac67.\n"
     ]
    }
   ],
   "source": [
    "SNV_PostProcessing_Normals = []\n",
    "MutationCalling_Tumors_TWIST = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    \n",
    "    # get config \n",
    "    mutcall_tumor = wto.get_config('MutationCalling_Tumors_TWIST')\n",
    "\n",
    "    # edit the config\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect1'\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect2'\n",
    "    \n",
    "    # check config\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'])\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutcall_tumor)\n",
    "\n",
    "    # create submission\n",
    "    SNV_PostProcessing_Normals += wto.create_submission(\"SNV_PostProcessing_Normals\", samplesetnames_normals[ind])\n",
    "    \n",
    "    MutationCalling_Tumors_TWIST += wto.create_submission(\"MutationCalling_Tumors_TWIST\", samplesetnames_pairs[ind], etype='pair_set', expression='this.pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = [''.join(SNV_PostProcessing_Normals)]\n",
    "MutationCalling_Tumors_TWIST = [''.join(MutationCalling_Tumors_TWIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'SNV_PostProcessing_Normals' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(wto, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregate SNV tsvs for each batch\n",
    "terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "# create aggregate SNV tsvs for each cohort\n",
    "terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", list(cohort_pairsets))\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# sometimes get space errors when run FNG_Compile_Pileup_Cnt if use 4 GB; changed to 10 GB\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
