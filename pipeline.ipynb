{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesetname=\"trial1\"\n",
    "date=\"2019\"\n",
    "data_namespace=\"broad-genomics-delivery\"\n",
    "data_workspace=\"Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "proc_namespace=\"nci-mimoun-bi-org\"\n",
    "proc_workspace=\"PANCAN_TWIST copy\"\n",
    "source=\"CCLF\"\n",
    "site=\"HT33MBCX2\"\n",
    "tsca_id=\"TSCA45\"\n",
    "TSCA_version=\"TSCA Rapid Cancer Detection Panel v2\"\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\"\n",
    "gsheeturllist=[\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\", \"https://docs.google.com/spreadsheets/d/128dkFhL1A0GqTjmR7iMvBZE8j6ymO8krBL9WX-wUAn4\"]\n",
    "wfrom = dm.WorkspaceManager(data_namespace, data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_namespace, proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we look at all the samples we already have\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "metadata = pd.concat([sheets.get(url).sheets[0].to_frame() for url in gsheeturllist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we look at all the samples we already have\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "metadata = pd.concat([sheets.get(url).sheets[0].to_frame() for url in gsheeturllist])\n",
    "\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "metadata = metadata.dropna(0, subset=['Collaborator Sample ID'])\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [val['Collaborator Participant ID'] + '-' + val['Sample Type'] + '-' + val['Exported DNA SM-ID'] for i, val in metadata.iterrows()]\n",
    "\n",
    "\n",
    "sample_id = [val[\"individual_alias\"] + '-' + val['sample_type'] + '-' + i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "samples1.index = sample_id\n",
    "\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "\n",
    "# usefull to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these as we dont have metadata for them:' + str(newsamples[~newsamples.index.isin(tokeep)]))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples = samples1\n",
    "newmetadata = metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('creating new df')\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)\n",
    "# from this new set we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']]\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID']\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID']\n",
    "sample_info['participant'] = df['Collaborator Participant ID']\n",
    "sample_info['aggregation_product_name_validation'] = [TSCA_version] * sample_info.shape[0]\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "sample_info['external_id_validation'] = [i +'_'+ str(refsamples[refsamples['external_id_validation'] == i].shape[1]) if refsamples[refsamples['external_id_validation'] == i].shape[0] > 0 else i for i in sample_info['reference_id']]\n",
    "sample_info['bsp_sample_id_validation'] = df.index\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID']\n",
    "sample_info['sample_type'] = df['Sample Type']\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type']\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type']\n",
    "sample_info['processed_subtype_validation'] = df['Material Type']\n",
    "sample_info['primary_disease'] = df['Primary Disease']\n",
    "sample_info['media'] = df['Media on Tube']\n",
    "sample_info['Collection'] = df['Collection']\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        raise \"we do not have a correponsding cohort for this collection\"\n",
    "    cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site']\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info['sample_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': []}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "            'sample_type'] == 'Tumor'].index.tolist():\n",
    "        normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "          'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "        newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "        newpairs['case_sample'].append(tumor_id)\n",
    "        newpairs['control_sample'].append(normal_id)\n",
    "        newpairs['participant'].append(val)\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "            'sample_type'] == 'Tumor'].index.tolist():\n",
    "        normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "          'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "        newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "        newpairs['case_sample'].append(tumor_id)\n",
    "        newpairs['control_sample'].append(normal_id)\n",
    "        newpairs['participant'].append(val)\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploads to Terra\n",
    "\n",
    "## all the entities need to exist! Else it will raise an error and block further uploads to Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all the entities need to exist! Else it will raise an error and block further uploads to Terra\")\n",
    "print(\"uploading new samples\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))\n",
    "    \n",
    "print(\"creating pairs and pairsets\")\n",
    "wto.upload_entities('pair', newpairs)\n",
    "wto.update_pair_set(samplesetname+'_pairs', newpairs.index)\n",
    "cohorts_in_batch = []\n",
    "cohorts_with_pairs = []\n",
    "for val in cohorts['ID'].values:\n",
    "    cohortsamples=sample_info[sample_info[\"cohorts\"] == val].index.tolist()\n",
    "    tumorsamplesincohort = sample_info[sample_info[\"cohorts\"] == val][sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "    pairsamples=newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "    if len(cohortsamples)>0:\n",
    "        cohorts_in_batch.append(val)\n",
    "        try:\n",
    "            terra.addToSampleSet(wto, val, cohortsamples)\n",
    "        except KeyError: # we may not have this set yet\n",
    "            wto.update_sample_set(val, cohortsamples)\n",
    "    if len(pairsamples)>0:\n",
    "        cohorts_with_pairs.append(val)\n",
    "        try:\n",
    "            terra.addToPairSet(wto,val, pairsamples)\n",
    "        except KeyError: # we may not have this set yet\n",
    "            wto.update_pair_set(val, pairsamples)\n",
    "print(\"creating a sample set\")\n",
    "wto.update_sample_set(sample_set_id=samplesetname + \"_all\", sample_ids=sample_info.index.tolist())\n",
    "wto.update_sample_set(sample_set_id=samplesetname + \"_tumors\", sample_ids=tumorsid)\n",
    "wto.update_sample_set(sample_set_id=samplesetname + \"_normals\", sample_ids=normalsid)\n",
    "normalsid.extend([k for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"])\n",
    "# Same as cum pon but better\n",
    "wto.update_sample_set(sample_set_id=\"All_normals\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples()..index.tolist()\n",
    "all_samples.remove('NA')\n",
    "wto.update_sample_set(sample_set_id=\"All_samples\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\\n",
    "    or interact with terra submissions from the Terra website. \\\n",
    "    https://app.terra.bio/#workspaces/\"+proc_namespace.replace(\" \", \"%20\")+\"/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = wto.create_submission(\"RenameBAM_TWIST\", samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(wto, [RenameBAM_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = wto.create_submission('CalculateTargetCoverage_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = wto.create_submission('DepthOfCov_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CalculateTargetCoverage_PANCAN' & 'DepthOfCov_PANCAN'\")\n",
    "terra.waitForSubmission(wto, [CalculateTargetCoverage_PANCAN, DepthOfCov_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreatePanelOfNormalsGATK_PANCAN = wto.create_submission('CreatePanelOfNormalsGATK_PANCAN', 'All_normals')\n",
    "DepthOfCovQC_PANCAN = wto.create_submission('DepthOfCovQC_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "terra.waitForSubmission(wto, [DepthOfCovQC_PANCAN, CreatePanelOfNormalsGATK_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = wto.create_submission('CallSomaticCNV_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(wto, [CallSomaticCNV_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = wto.create_submission(\"MutationCalling_Normals_TWIST\", samplesetname + \"_normals\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(wto, [MutationCalling_Normals_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FilterGermlineVariants_NormalSample_TWIST = wto.create_submission('FilterGermlineVariants_NormalSample_TWIST', samplesetname + \"_normals\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(wto, [FilterGermlineVariants_NormalSample_TWIST])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals\")\n",
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(wto, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotSomaticCNVMaps_PANCAN = wto.create_submission('PlotSomaticCNVMaps_PANCAN', samplesetname + \"_all\")\n",
    "for val in cohorts_in_batch:\n",
    "    wto.create_submission(\"PlotSomaticCNVMaps_PANCAN\", val)\n",
    "print(\"submitted final jobs for CNV pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = wto.create_submission('SNV_PostProcessing_Normals', samplesetname + \"_normals\")\n",
    "MutationCalling_Tumors_TWIST = wto.create_submission('MutationCalling_Tumors_TWIST', samplesetname+'_pairs', 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'SNV_PostProcessing' & 'MutationCalling_Tumors_TWIST'\")\n",
    "terra.waitForSubmission(wto, [SNV_PostProcessing_Normals, MutationCalling_Tumors_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## two cohorts have not worked because they contained just one acceptable cell line (the workflow needs cohorts with at least 2 acceptable CL, here both had one rejected one)\n",
    "FilterGermlineEvents_TumorSample = wto.create_submission('FilterGermlineEvents_TumorSample', samplesetname+'_pairs', 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(wto, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNVPostProcessing_TWIST = wto.create_submission('SNVPostProcessing_TWIST', samplesetname+'_pairs', \"pair_set\")\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "FNG_Compile_Pileup_Cnt = wto.create_submission(\"FNG_Compile_Pileup_Cnt\", samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(wto, [FNG_Compile_Pileup_Cnt])\n",
    "\n",
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(wto, [FNG_Compile_db_slow_download])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = wto.create_submission(\"FNG_Query_db\", samplesetname + \"_all\")\n",
    "print(\"Submitted final FNG Job\")\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload nice folders with data per sample or per cohort or per any list provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = \"CCLF_TSCA_2_0_2\"\n",
    "namespace = \"nci-mimoun-bi-org\"\n",
    "wm = dm.WorkspaceManager(namespace,workspace)\n",
    "pathto_cnvpng='segmented_copy_ratio_img'\n",
    "pathto_stats='sample_statistics'\n",
    "is_from_pairs=True\n",
    "pathto_snv='filtered_variants'\n",
    "pathto_seg='cnv_calls'\n",
    "datadir='gs://cclf_results/targeted/kim_sept/'\n",
    "#specificlist= pd.read_csv(\"\")[\"\"].tolist() \n",
    "#specificlist= wm.get_sample_sets(), ...\n",
    "specificlist=['CCLF_PEDS1012-Tumor-SM-E7S13',\n",
    "'CCLF_PEDS1012-Tumor-SM-E7S1F',\n",
    "'CCLF_PEDS1012-Tumor-SM-E7S1R',\n",
    "'PEDS172-Tumor-SM-DB2K3',\n",
    "'PEDS172-Tumor-SM-DB3R7',\n",
    "'PEDS182-Tumor-SM-DHZ8V',\n",
    "'PEDS196-Tumor-SM-DNUN4',\n",
    "'PEDS196-Tumor-SM-DNUN5',\n",
    "'PEDS204-Tumor-SM-DO3D5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = wm.get_samples()\n",
    "samples = samples[samples.index.isin(specificlist)] \n",
    "if is_from_pairs:\n",
    "    pairs = wm.get_pairs()\n",
    "    pairs = pairs[pairs['case_sample'].isin(specificlist)] \n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, val in samples.iterrows():\n",
    "    os.system('gsutil cp '+val[pathto_seg]+' '+datadir+i+'/')\n",
    "    os.system('gsutil cp '+val[pathto_cnvpng]+' '+datadir+i+'/')\n",
    "    os.system('gsutil cp '+val[pathto_stats]+' '+datadir+i+'/')\n",
    "    if is_from_pairs:\n",
    "        snvs = pairs[pairs[\"case_sample\"]==i][pathto_snv]\n",
    "        for snv in snvs:\n",
    "            if snv is not np.nan:\n",
    "                os.system('gsutil cp '+snv+' '+datadir+i+'/')\n",
    "                break\n",
    "    else:\n",
    "        os.system('gsutil cp '+val[pathto_snv]+' '+datadir+i+'/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
