{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import sound alert dependencies\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# play sound alert when function is called\n",
    "def allDone():\n",
    "#     display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))    \n",
    "    framerate = 4410\n",
    "    play_time_seconds = 1\n",
    "\n",
    "    t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "    audio_data = np.sin(2*np.pi*300*t) + np.sin(2*np.pi*240*t)\n",
    "    display(Audio(audio_data, rate=framerate, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# files_to_copy = [\"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/8bcb9995-e552-4915-942e-f347e3feb6d9/call-callSomaticCNV/SM-J1OYQ.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/1cb49263-91ee-4377-9688-934f29ba42a5/call-callSomaticCNV/SM-J1OYR.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/b897c3a9-1dd8-4466-bc65-bd1720165c9a/call-callSomaticCNV/SM-J1OYS.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/e4da1585-abf0-4dcd-8d67-98c53119bf73/call-callSomaticCNV/SM-J1OYT.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/0f52e177-ae13-489e-a446-dc1cb91a35bc/call-callSomaticCNV/SM-J1OYU.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/03612d07-c048-42bd-ac48-e2c01efdc48e/call-callSomaticCNV/SM-J1OYV.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/447a63e9-452f-4bf7-acbb-0c482e838a95/call-callSomaticCNV/SM-J1OYW.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/3b25baed-7060-4626-866a-1c296fdd604f/call-callSomaticCNV/SM-J1OYX.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/8f9c6aeb-3377-4415-973a-95d4b2e0fde0/call-callSomaticCNV/SM-J1OYY.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/4f142a02-4051-4b62-856f-a62a68650889/call-callSomaticCNV/SM-J1OYZ.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/a9757332-fa81-42ea-b18e-5ae6a7a3851e/call-callSomaticCNV/SM-J1OZ1.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/4e7d9e4b-61d9-4a45-8b43-d6d697d83fe8/call-callSomaticCNV/SM-J1OZ2.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/b4ed4b00-f7cd-4eb1-a86d-c81f7570c8f9/call-callSomaticCNV/SM-J1OZ3.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/4e0c00be-8b1e-42fb-8cda-30a5247aca4d/call-callSomaticCNV/SM-J1OZ4.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/aeffd379-8303-4ee7-bb5a-cd5cbecddf6e/call-callSomaticCNV/SM-J1OZ5.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/3c41e567-fec2-4184-b09a-237f02ead1f5/call-callSomaticCNV/SM-J1OZ7.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/d9df914e-bfc6-4a5b-940d-53f4ac0aaba0/call-callSomaticCNV/SM-J1OZ8.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/13c97055-6d04-4f06-9621-195013a1f823/call-callSomaticCNV/SM-J1OZ9.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/df005587-cafd-423f-9893-70bb787eca55/call-callSomaticCNV/SM-J1OZA.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/67de487b-17b2-464e-a29d-b6bfb59550bf/call-callSomaticCNV/SM-J1OZB.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/e9ae7923-9c4a-4acc-8145-272f8892684e/call-callSomaticCNV/SM-J1OZC.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/56af0079-dd28-456a-a30d-13bf56ade4bd/call-callSomaticCNV/SM-J1OZD.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/b28ed484-ae8c-4356-9890-c074f97db940/call-callSomaticCNV/SM-J1OZE.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/e6fbf76d-83fc-4d95-8d63-e790da290457/call-callSomaticCNV/SM-J1OZF.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/39a7dbaf-1e25-49f4-8932-c37eaa7c0583/call-callSomaticCNV/SM-J1OZG.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/69a42f3c-36f5-4a56-a792-8c6567f6edc4/call-callSomaticCNV/SM-J1OZH.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/babf7c05-de52-48dd-8f88-c028b8743bd2/call-callSomaticCNV/SM-J1OZI.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/2dedc905-7875-4bdf-85b5-22fc000aa868/call-callSomaticCNV/SM-J1OZJ.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/5ed3c2fd-7867-463c-8244-64476f8f9861/call-callSomaticCNV/SM-J1OZK.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/33c617ac-94b8-4064-9351-d5b09d6d493b/call-callSomaticCNV/SM-J1OZL.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/3ccf3019-7fda-41c2-ba31-f412d964b4b4/call-callSomaticCNV/SM-J1OZM.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/21816df3-b46b-4864-b1f6-9583c24f5d09/call-callSomaticCNV/SM-J1OZN.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/f4c13eb1-5992-4b2e-9755-a631217e4ecd/call-callSomaticCNV/SM-J1OZO.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/f052a9cd-2e4e-49b0-9086-53e85ed34c29/call-callSomaticCNV/SM-J1OZP.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/534f4dd9-d7ba-4c39-8bcb-6526117e2084/call-callSomaticCNV/SM-J1OZQ.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/0ac13577-0d5f-427d-8877-d6024188188a/call-callSomaticCNV/SM-J1OZR.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/c9c4595a-3344-4cb3-975f-72db8f091794/call-callSomaticCNV/SM-J1OZS.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/52fd15c3-003c-4c83-bb45-f64db851ee9d/call-callSomaticCNV/SM-J1OZT.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/4ccc2a9f-ed57-4446-a69a-b29f5a55c02e/call-callSomaticCNV/SM-J1OZU.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/20799101-8f7e-4987-a069-4011b05918c9/call-callSomaticCNV/SM-J1OZV.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/9d45f453-1ffa-49b9-8b04-7ac3eef1e144/call-callSomaticCNV/SM-J1OZW.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/92057510-ef9b-4305-94ac-e6106ec00f98/call-callSomaticCNV/SM-J1OZX.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/d09a9813-41d0-4d7d-9668-9f14e50a6dcc/call-callSomaticCNV/SM-J1OZY.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/162e151a-63af-46c7-9fe6-a2988a5019e3/call-callSomaticCNV/SM-J1OZZ.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/29db95dc-48d8-4268-8d17-04a0e68db0b7/call-callSomaticCNV/SM-J1P11.tumor.tn.tsv\", \"gs://fc-ed07d172-7980-475e-81de-108a694a3532/e6bbe0e1-0cb4-4e71-a553-bc38503e53d9/CallSomaticCNV/5c9e2d3c-6e0b-4ed4-a678-582b42051093/call-callSomaticCNV/SM-J1P12.tumor.tn.tsv\"]\n",
    "\n",
    "# for file in files_to_copy:\n",
    "#   ! gsutil cp {file} /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Plot unsegmented CNV Calls\n",
    "# sample_types = ['Normal']*23+['Tumor']*23\n",
    "# ! python /Users/gmiller/Documents/Work/GitHub/TSCA_docker/20191008_TSCA/EDITED2_plot_somatic_cnv_calls.py  --tsca_id CCLF_TWIST2_all \\\n",
    "#                                       --tn_files /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYQ.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYR.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYS.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYT.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYU.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYV.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYW.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYX.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYY.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYZ.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ1.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ2.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ3.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ4.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ5.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ7.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ8.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZ9.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZA.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZB.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZC.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZD.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZE.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZF.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZG.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZH.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZI.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZJ.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZK.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZL.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZM.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZN.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZO.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZP.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZQ.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZR.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZS.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZT.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZU.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZV.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZW.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZX.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZY.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OZZ.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1P11.tumor.tn.tsv /Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1P12.tumor.tn.tsv \\\n",
    "#                                       --external_ids CCLF_cRCRF1040T_Primary CCLF_BU1017N_Primary CCLF_cRCRF1060T_Primary CCLF_AB1065T_OPAC_P8_3D CCLF_KL1288T_Primary CCLF_KL1283T_Primary CCLF_cRCRF1040T_RETM/M87/BCXJ_3D_P3 CCLF_KL1282T_SMGM/M87/BCXJ_3D_P3 CCLF_BU1013T_SMGM/M87/BCXJ_3D_P4 CCLF_cRCRF1060T_RETM/SMGM/BCXJ_3D_P3 CCLF_cRCRF1060T_RETM/AR5/BCXJ_3D_P3 CCLF_cRCRF1060T_SMGM/M87/BCXJ_3D_P3 CCLF_PEDS1143N_Primary CCLF_RCRF1097T_Primary CCLF_pEDS1140T_Primary CCLF_cRCRF1081T_Primary CCLF_PEDS1153T_Primary CCLF_cRCRF1085T_AR5_2D_P6 CCLF_SS1020T_Primary CCLF_PEDS1153N_Primary CCLF_cRCRF1085T_SMGM_2D_P6 CCLF_SS1020N_Primary CCLF_KL1294T_Primary CCLF_KL1235T_Primary CCLF_AB1097T_ASC_Primary CCLF_RCRF1070T_Primary CCLF_RCRF1064T_Primary CCLF_PEDS1064N_CM_2D_P8 CCLF_CY1006T_Primary SP014N_GL_Primary CCLF_CY1007T_Primary CCLF_CY1015T_Primary CCLF_PEDS1141N_Primary CCLF_PEDS1143T_Primary CCLF_PEDS1141T_Primary CCLF_CY1006T_SMGM_2D_P5 CCLF_RCRF1102N_Primary CCLF_KL1292T_Primary CCLF_CY1007T_AR5/_2D_P5 CCLF_PEDS1156T_Primary CCLF_CY1015T_SMGM_2D_P4 CCLF_RCRF1099T_Primary CCLF_PEDS1154T_Primary CCLF_CY1015T_RETM_2D_P4 CCLF_PEDS1155T_Primary CCLF_RCRF1102T_Primary \\\n",
    "#                                       --sample_ids SM-J1OYQ SM-J1OYR SM-J1OYS SM-J1OYT SM-J1OYU SM-J1OYV SM-J1OYW SM-J1OYX SM-J1OYY SM-J1OYZ SM-J1OZ1 SM-J1OZ2 SM-J1OZ3 SM-J1OZ4 SM-J1OZ5 SM-J1OZ7 SM-J1OZ8 SM-J1OZ9 SM-J1OZA SM-J1OZB SM-J1OZC SM-J1OZD SM-J1OZE SM-J1OZF SM-J1OZG SM-J1OZH SM-J1OZI SM-J1OZJ SM-J1OZK SM-J1OZL SM-J1OZM SM-J1OZN SM-J1OZO SM-J1OZP SM-J1OZQ SM-J1OZR SM-J1OZS SM-J1OZT SM-J1OZU SM-J1OZV SM-J1OZW SM-J1OZX SM-J1OZY SM-J1OZZ SM-J1P11 SM-J1P12 \\\n",
    "#                                       --depth_of_cov_qcs pass pass pass fail pass pass pass pass fail pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass \\\n",
    "#                                       --sample_types {sample_types}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = pd.read_table( '/Users/gmiller/Documents/Work/GitHub/CCLF_TWIST/temp/TWIST2/SM-J1OYQ.tumor.tn.tsv', index_col=None, header=0, comment='#', usecols=['contig', 'start', 'stop', 'name'])\n",
    "# tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "# samplesetnames = [\"CCLF_TWIST1\",\"CCLF_TWIST2\",\"CCLF_TWIST3\",\"CCLF_TWIST4\"]\n",
    "samplesetnames = [\"CCLF_TWIST1\"]\n",
    "\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\"\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\"] # TWIST1\n",
    "# gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\", #TWIST1\n",
    "# \"https://docs.google.com/spreadsheets/d/1S3DqBdVkd9dLP1PDYcdSWuD2Iy2gJpzuYBhvmP37UxU\", # TWIST2\n",
    "# \"https://docs.google.com/spreadsheets/d/1kVIeIw66AxWLhAZlqUnAY17S87Rtfhijf1o3x0hG3Jw\", # TWIST3\n",
    "# \"https://docs.google.com/spreadsheets/d/1tZQpxag7BO46pei3s_KaoHvxwN9EVESk3xYvzW7f7Uo/\", # TWIST4\n",
    "# \"https://docs.google.com/spreadsheets/d/1iqVDNPJcMLbNgfdmoBIHwaDF4yaXlyeVFxEwu_fWsEo\" # TWIST5\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we look at all the samples we already have in the TWIST workspace\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "\n",
    "# get the data from google sheets\n",
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a column with batch information (e.g. CCLF_TWIST1 vs CCLF_TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "print(len(metadata))\n",
    "\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "\n",
    "# we look at all the samples we already have in Terra\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID'])\n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata\n",
    "\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these samples from the data workspace as we don\\'t have metadata for them: ' + '\\n' \n",
    "          + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")\n",
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: this should match some of the data in the External Sheet\n",
    "print(newmetadata[['batch','external_id_validation']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think this cell and the following cell needs to be moved prior to the creation of the sample_info dataframe - we didn't remove *any* samples from TWIST1-4 that were missing these metadata columns\n",
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Patient ID <- not sure about adding this requirement, but it will be used when plotting the CNV heat maps\n",
    "- Sample Type\n",
    "- ~~Tumor Type~~ <- this won't be populated for normals.\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- this only works if the normals also have a primary disease associated with them, which they should. Only the technical controls won't have this information.\n",
    "- ~~Media on Tube~~ <- tissue samples won't have a media but we do want to include them\n",
    "- Collection\n",
    "- Tissue Site <- This column should eventually be populated\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Since they don\\'t have full data, we will be dropping ' + str(len(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist())) + ' samples: \\n' + \n",
    "      str(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "\n",
    "\n",
    "\n",
    "# examine which columns in the External Sheet had missing information\n",
    "print('\\nNumber of NAs for each required column:')\n",
    "print(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have now dropped the samples for which we didn\\'t have full data')\n",
    "# only keep samples that have all the appropriate information\n",
    "df = df.iloc[[j for j,i in enumerate(df[['Exported DNA SM-ID','Collaborator Participant ID',\n",
    "                                         'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                         'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                         'Collection']].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('creating new sample information df')\n",
    "# from this filtered set of samples (df) we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']]\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['patient_id'] = df['Participant ID'].astype(str) ## cehck: this is a new column in the external sheet. The name may change.\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "\n",
    "# start building external_id_validation column\n",
    "sample_info['external_id_validation'] = 'nan'\n",
    "# for each SM-ID:\n",
    "for i in range(len(sample_info['reference_id'])):\n",
    "    # get the external id for the sample\n",
    "    ext_id_for_sample = df[df.index == sample_info['reference_id'][i]]['external_id_validation'].values[0]\n",
    "    # replace any \"/\" that exist with \"_\"; otherwise get errors because looks like new directory when try to build file paths\n",
    "    ext_id_for_sample = ext_id_for_sample.replace('/', '_') #[ext_id_for_sample.replace('/', '_') for ext_id in ext_id_for_sample]\n",
    "    \n",
    "    # tack on a number to distinguish external IDs that we have run more than once\n",
    "    # using str.contains because we want to ignore the tacked on numbers we've added to the ext_id (e.g. _1, _2)\n",
    "    # num of samples with this ext_id already in the workspace\n",
    "    num_in_workspace = refsamples[refsamples.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "    # num of samples with this ext_id that we've already seen in the data we're adding\n",
    "    try:\n",
    "        num_already_seen_here = sample_info[sample_info.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_to_add = num_in_workspace + num_already_seen_here + 1\n",
    "    except:\n",
    "        num_to_add = num_in_workspace + 1\n",
    "    sample_info['external_id_validation'][i] = ext_id_for_sample + '_' + str(num_to_add)\n",
    "\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should be the sample TSV you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "filepath = './temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file you just saved\n",
    "filepath = './temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace.\n",
    "\n",
    "**Note:** all the entities (e.g. sample, sample set, participant tsv) need to exist! Else it will raise an error and block further uploads to Terra. You can do this by just uploading TSVs with NA. The below code does this automatically for the sample TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch; we do not actually use this for anything. Could be used for QC.\n",
    "for i in range(len(samplesetnames)):\n",
    "    \n",
    "    wto.update_pair_set(samplesetnames_pairs[i], newpairs.index.tolist())\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(proc_workspace, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(proc_workspace,val, pairsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if _['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if _['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    ## check: worried that I'll just overwrite whatever samples sets I've made previously.\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_all[i], samples=batch_sample_info.index.tolist())\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_tumors[i], samples=batch_tumorsid)\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_normals[i], samples=batch_normalsid)\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_all[i], sample_ids=batch_sample_info.index.tolist())\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_tumors[i], sample_ids=batch_tumorsid)\n",
    "#     wto.update_sample_set(sample_set_id=samplesetnames_normals[i], sample_ids=batch_normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"]) # add pre-existing normals\n",
    "\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wto.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "# terra.waitForSubmission(proc_workspace, CalculateTargetCoverage_PANCAN, DepthOfCov_PANCAN)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\")\n",
    "Go into Terra > Workflows > CreatePanelOfNormalsGATK_PANCAN > outputs. Change the values for normals_pon and combined_normals to have the appropriate batch ID. For example, workspace.pon_normals_{batch-ID-here}. E.g. workspace.pon_normals_TWIST1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_config('CreatePanelOfNormalsGATK_PANCAN')['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing to use just the normals from the batch, not all normals\n",
    "CreatePanelOfNormalsGATK_PANCAN = terra.createManySubmissions(wto, \"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals)\n",
    "DepthOfCovQC_PANCAN = terra.createManySubmissions(wto, \"DepthOfCovQC_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: make sure you change the inputs config for CallSomaticCNV_PANCAN so that it uses the correct CallSomaticCNV.normals_pon (batch-specific PONs)\n",
    "Go into Terra > Workflows > CreatePanelOfNormalsGATK_PANCAN > inputs. Change the values for normals_pon to have the appropriate batch ID. For example, workspace.pon_normals_{batch-ID-here}. E.g. workspace.pon_normals_TWIST1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_config('CallSomaticCNV_PANCAN')['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = terra.createManySubmissions(wto, \"CallSomaticCNV_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache = True)\n",
    "\n",
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# had errors when using call caching on TWIST1-3. No errors for TWIST4\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=False)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: edit the \"PoN_name\" config (input) and the \"normals_pon_vcf\" config (output) for both CreatePoNSNV_Mutect1 and CreatePoN_SNV_MuTect2\n",
    "Go into Terra > Workflows > CreatePanelOfNormalsGATK_PANCAN > inputs. Change the values for PoN_name. For example, \"Cum_PoN_TWIST1_all_vcf_mutect1\". You should only need to change the batch number.\n",
    "\n",
    "Go into Terra > Workflows > CreatePanelOfNormalsGATK_PANCAN > outputs. Change the values for normals_pon_vcf. For example, workspace.Cum_PoN_TWIST1_all_vcf_mutect1. You should only need to change the batch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_config('CreatePoNSNV_Mutect1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.get_config('CreatePoN_SNV_MuTect2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")\n",
    "\n",
    "# CreatePoNSNV_Mutect1 = terra.createManySubmissions(wto, \"CreatePoNSNV_Mutect1\", 'All_normals_TWIST')\n",
    "# CreatePoN_SNV_MuTect2 = terra.createManySubmissions(wto, \"CreatePoN_SNV_MuTect2\", 'All_normals_TWIST')\n",
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "combined_list = [CreatePoNSNV_Mutect1] + [CreatePoN_SNV_MuTect2]\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "# terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-create sample_info by pulling in sample data from the Terra workspace\n",
    "sample_info = wto.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create / re-create cohorts_per_batch dictionary\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "all_changed_cohorts = set()\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch[batch_name] = cohorts_in_batch\n",
    "    all_changed_cohorts.update(cohorts_in_batch) # add all the new cohorts in this batch to the full list\n",
    "# cohorts_per_batch\n",
    "all_changed_cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: the workflow will fail when running on a cohort that has only 1 sample (requires 2+)\n",
    "\n",
    "# create CNV map for each cohort (regardless of the batch)\n",
    "for val in all_changed_cohorts:\n",
    "    wto.create_submission(\"PlotSomaticCNVMaps_PANCAN\", val)\n",
    "\n",
    "# create CNV map for each batch\n",
    "PlotSomaticCNVMaps_PANCAN = terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: make sure you change the config for MutationCalling_Tumors_TWIST so that it uses the correct pon_mutect1 and pon_mutect2 (cumulative PONs from the first batch through the current batch)\n",
    "~~Go into Terra > Workflows > CreatePanelOfNormalsGATK_PANCAN > outputs. Change the values for normals_pon_vcf. For example, workspace.Cum_PoN_TWIST1_all_vcf_mutect1. You should only need to change the batch number.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wto.get_config('MutationCalling_Tumors_TWIST')['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "print(wto.get_config('MutationCalling_Tumors_TWIST')['inputs']['MutationCalling_Tumor.pon_mutect1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = terra.createManySubmissions(wto, \"SNV_PostProcessing_Normals\", samplesetnames_normals)\n",
    "MutationCalling_Tumors_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Tumors_TWIST\", samplesetnames_pairs, \n",
    "                                              entity='pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'SNV_PostProcessing' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## note: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(wto, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs, \n",
    "#                                               entity='pair_set', expression='this.pairs')\n",
    "SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# sometimes get space errors when run FNG_Compile_Pileup_Cnt if use 4 GB; changed to 10 GB\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "allDone()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
