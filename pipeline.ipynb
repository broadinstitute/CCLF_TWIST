{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "import ipdb\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "samplesetnames = [\"CCLF_TWIST1\",\"CCLF_TWIST2\",\"CCLF_TWIST3\",\"CCLF_TWIST4\"]\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "date=\"2019\" # not using currently; could also get this from release_date column (in Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq)\n",
    "data_namespace=\"broad-genomics-delivery\"\n",
    "data_workspace=\"Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "proc_namespace=\"nci-mimoun-bi-org\"\n",
    "# proc_workspace=\"CCLF_Targeted\"\n",
    "proc_workspace =\"PANCAN_TWIST copy\" ## check: change later\n",
    "source=\"CCLF\"\n",
    "site=\"HT33MBCX2\" # is this used? where to get this info?\n",
    "tsca_id=\"TWIST1-4\" # is this used?\n",
    "\n",
    "TSCA_version=\"TWISTv1\" # where can we find this information? The bait_set column in Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\" # cohort dict\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LR8OFylVClxf0kmZpAdlVjrn3RBcfZKpNoDYtKdnHB8\", # PanCan 1\n",
    "\"https://docs.google.com/spreadsheets/d/1S3DqBdVkd9dLP1PDYcdSWuD2Iy2gJpzuYBhvmP37UxU\", # TWIST 2 New\n",
    "\"https://docs.google.com/spreadsheets/d/1kVIeIw66AxWLhAZlqUnAY17S87Rtfhijf1o3x0hG3Jw\", # TWIST 3 New\n",
    "\"https://docs.google.com/spreadsheets/d/1tZQpxag7BO46pei3s_KaoHvxwN9EVESk3xYvzW7f7Uo/\" # TWIST 4\n",
    "                ]\n",
    "wfrom = dm.WorkspaceManager(data_namespace, data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_namespace, proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at all the samples we already have\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the data from google sheets\n",
    "# add a column with batch information (e.g. TWIST1 vs TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False)\n",
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(metadata.columns.values.tolist())\n",
    "# the existing Batch Number column is clearly not well-maintained. This is why I add in a column with batch information.\n",
    "metadata['Batch Number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with batch information (e.g. TWIST1 vs TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = [\"CCLF_TWIST1\",\"CCLF_TWIST2\",\"CCLF_TWIST3\",\"CCLF_TWIST4\"])\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch'})\n",
    "metadata = metadata.drop(['level_1'], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we look at all the samples we already have\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID']) \n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering on what already exists in the processing workspace (refids)\n",
    "# but what if a sample was run on TSCA and is later run on TWIST? We may need to think this through more.\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these as we dont have metadata for them: ' + '\\n' + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples['bait_set'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('creating new df')\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)\n",
    "# from this new set we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']]\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "# sample_info['aggregation_product_name_validation'] = [TSCA_version] * sample_info.shape[0]\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "sample_info['external_id_validation'] = [i +'_'+ str(refsamples[refsamples['external_id_validation'] == i].shape[1]) if refsamples[refsamples['external_id_validation'] == i].shape[0] > 0 else i for i in sample_info['reference_id']]\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Since they don\\'t have full data, we are dropping: \\n' + \n",
    "      str(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID','Sample Type','Tumor Type',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Media on Tube','Collection','Tissue Site']].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "df = df.iloc[[j for j,i in enumerate(df[['Exported DNA SM-ID','Collaborator Participant ID',\n",
    "                                         'Stock DNA SM-ID','Sample Type','Tumor Type',\n",
    "                                         'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                         'Media on Tube','Collection','Tissue Site']].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "sample_info.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refsamples['sample_type'].head()\n",
    "# refsamples.columns.values.tolist()\n",
    "\n",
    "# for k, val in refsamples.iterrows():\n",
    "#     print(val.sample_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploads to Terra\n",
    "\n",
    "## all the entities (e.g. sample tsv) need to exist! Else it will raise an error and block further uploads to Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test / scratch\n",
    "# print(sample_info[sample_info['batch'] == 'CCLF_TWIST1']['batch'].head())\n",
    "# for i in range(len(samplesetnames)):\n",
    "#     batch_sample_info = sample_info[sample_info['batch'] == i]\n",
    "#     print(\"Printing \"+samplesetnames[i])\n",
    "#     print(tmp['batch'].head())\n",
    "\n",
    "# batch_sample_info = sample_info[sample_info['batch'] == 'CCLF_TWIST1' ]\n",
    "# for val in cohorts['ID'].values:\n",
    "#     cohortsamples=batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "#     print(batch_sample_info['cohorts'].unique())\n",
    "#     print(batch_sample_info[batch_sample_info[\"cohorts\"] == val])\n",
    "#     print('\\n')\n",
    "# print(cohortsamples)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All the entities need to exist! Else it will raise an error and block further uploads to Terra\")\n",
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))\n",
    "    \n",
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)\n",
    "# wto.update_pair_set(samplesetname+'_pairs', newpairs.index.tolist())\n",
    "\n",
    "# want to create a pair set for each batch\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "for i in range(len(samplesetnames)):\n",
    "    \n",
    "    wto.update_pair_set(samplesetnames_pairs[i], newpairs.index.tolist())\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: do we use this for anything?\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(wto, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(wto,val, pairsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "    \n",
    "# cohorts_in_batch = []\n",
    "# cohorts_with_pairs = []\n",
    "# for val in cohorts['ID'].values:\n",
    "#     cohortsamples=sample_info[sample_info[\"cohorts\"] == val].index.tolist()\n",
    "#     tumorsamplesincohort = sample_info[sample_info[\"cohorts\"] == val][sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "#     pairsamples=newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "#     if len(cohortsamples)>0:\n",
    "#         cohorts_in_batch.append(val)\n",
    "#         try:\n",
    "#             terra.addToSampleSet(wto, val, cohortsamples)\n",
    "#         except KeyError: # we may not have this set yet\n",
    "#             wto.update_sample_set(val, cohortsamples)\n",
    "#     if len(pairsamples)>0:\n",
    "#         cohorts_with_pairs.append(val)\n",
    "#         try:\n",
    "#             terra.addToPairSet(wto,val, pairsamples)\n",
    "#         except KeyError: # we may not have this set yet\n",
    "#             wto.update_pair_set(val, pairsamples)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for i, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [i for i, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for i, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [i for i, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_all[i], sample_ids=batch_sample_info.index.tolist())\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_tumors[i], sample_ids=batch_tumorsid)\n",
    "    wto.update_sample_set(sample_set_id=samplesetnames_normals[i], sample_ids=batch_normalsid)\n",
    "# wto.update_sample_set(sample_set_id=samplesetname + \"_all\", sample_ids=sample_info.index.tolist())\n",
    "# wto.update_sample_set(sample_set_id=samplesetname + \"_tumors\", sample_ids=tumorsid)\n",
    "# wto.update_sample_set(sample_set_id=samplesetname + \"_normals\", sample_ids=normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]) # add pre-existing normals\n",
    "terra.addToSampleSet(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "terra.addToSampleSet(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\\n",
    "    or interact with terra submissions from the Terra website. \\\n",
    "    https://app.terra.bio/#workspaces/\"+proc_namespace.replace(\" \", \"%20\")+\"/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "# RenameBAM_TWIST = wto.create_submission(\"RenameBAM_TWIST\", samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(wto, [RenameBAM_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CalculateTargetCoverage_PANCAN = wto.create_submission('CalculateTargetCoverage', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "# DepthOfCov_PANCAN = wto.create_submission('DepthOfCov_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "\n",
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "terra.waitForSubmission(wto, [CalculateTargetCoverage_PANCAN, DepthOfCov_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreatePanelOfNormalsGATK_PANCAN = wto.create_submission('CreatePanelOfNormalsGATK_PANCAN', 'All_normals_TWIST')\n",
    "# DepthOfCovQC_PANCAN = wto.create_submission('DepthOfCovQC_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "\n",
    "## changing to use just the normals from the batch, not all normals\n",
    "CreatePanelOfNormalsGATK_PANCAN = terra.createManySubmissions(wto, \"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals)\n",
    "DepthOfCovQC_PANCAN = terra.createManySubmissions(wto, \"DepthOfCovQC_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "terra.waitForSubmission(wto, [DepthOfCovQC_PANCAN, CreatePanelOfNormalsGATK_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DepthOfCovQC_PANCAN = wto.create_submission('DepthOfCovQC_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "# print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "# terra.waitForSubmission(wto, [DepthOfCovQC_PANCAN, CreatePanelOfNormalsGATK_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CallSomaticCNV_PANCAN = wto.create_submission('CallSomaticCNV_PANCAN', samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "\n",
    "CallSomaticCNV_PANCAN = terra.createManySubmissions(wto, \"CallSomaticCNV_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(wto, [CallSomaticCNV_PANCAN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MutationCalling_Normals_TWIST = wto.create_submission(\"MutationCalling_Normals_TWIST\", samplesetname + \"_normals\", 'sample_set', expression='this.samples')\n",
    "\n",
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(wto, [MutationCalling_Normals_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FilterGermlineVariants_NormalSample_TWIST = wto.create_submission('FilterGermlineVariants_NormalSample_TWIST', samplesetname + \"_normals\", 'sample_set', expression='this.samples')\n",
    "\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(wto, [FilterGermlineVariants_NormalSample_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "# CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")\n",
    "\n",
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = terra.createManySubmissions(wto, \"CreatePoNSNV_Mutect1\", 'All_normals_TWIST')\n",
    "CreatePoN_SNV_MuTect2 = terra.createManySubmissions(wto, \"CreatePoN_SNV_MuTect2\", 'All_normals_TWIST')\n",
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(wto, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotSomaticCNVMaps_PANCAN = wto.create_submission('PlotSomaticCNVMaps_PANCAN', samplesetname + \"_all\")\n",
    "# for val in cohorts_in_batch:\n",
    "#     wto.create_submission(\"PlotSomaticCNVMaps_PANCAN\", val)\n",
    "\n",
    "# create CNV map for each batch\n",
    "PlotSomaticCNVMaps_PANCAN = terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "\n",
    "# create CNV map for each cohort in a batch\n",
    "for batch, cohorts in cohorts_per_batch.items():\n",
    "    for val in cohorts:\n",
    "        wto.create_submission(\"PlotSomaticCNVMaps_PANCAN\", val)\n",
    "print(\"submitted final jobs for CNV pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNV_PostProcessing_Normals = wto.create_submission('SNV_PostProcessing_Normals', samplesetname + \"_normals\")\n",
    "# MutationCalling_Tumors_TWIST = wto.create_submission('MutationCalling_Tumors_TWIST', samplesetname+'_pairs', 'pair_set', expression='this.pairs')\n",
    "\n",
    "SNV_PostProcessing_Normals = terra.createManySubmissions(wto, \"SNV_PostProcessing_Normals\", samplesetnames_normals)\n",
    "MutationCalling_Tumors_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Tumors_TWIST\", samplesetnames_pairs, \n",
    "                                              entity='pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'SNV_PostProcessing' & 'MutationCalling_Tumors_TWIST'\")\n",
    "terra.waitForSubmission(wto, [SNV_PostProcessing_Normals, MutationCalling_Tumors_TWIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[b['case_sample'].isin(a)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.delete_entity('pair',b[b['case_sample'].isin(a)].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.delete_sample(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## two cohorts have not worked because they contained just one acceptable cell line (the workflow needs cohorts with at least 2 acceptable CL, here both had one rejected one)\n",
    "# FilterGermlineEvents_TumorSample = wto.create_submission('FilterGermlineEvents_TumorSample', samplesetname+'_pairs', 'pair_set', expression='this.pairs')\n",
    "\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions('FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(wto, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = wto.get_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['case_sample'].isin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.loc[b['case_sample'].isin(a),'match_type'] = [\"Tumor_NA\" if '_NA' in k else \"Tumor_Normal\" for k in b[b['case_sample'].isin(a)].index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wto.upload_entities('pair', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNVPostProcessing_TWIST = wto.create_submission('SNVPostProcessing_TWIST', samplesetname+'_pairs', \"pair_set\")\n",
    "SNVPostProcessing_TWIST = terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs, \n",
    "                                              entity='pair_set', expression='this.pairs')\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# FNG_Compile_Pileup_Cnt = wto.create_submission(\"FNG_Compile_Pileup_Cnt\", samplesetname + \"_all\", 'sample_set', expression='this.samples')\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(wto, [FNG_Compile_Pileup_Cnt])\n",
    "\n",
    "\n",
    "## check: I don't think we have a \"All_samples\" set yet... we do have a \"All_samples_TWIST\" though.\n",
    "# FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples\")\n",
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(wto, [FNG_Compile_db_slow_download])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNG_Query_db = wto.create_submission(\"FNG_Query_db\", samplesetname + \"_all\")\n",
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and upload nice folders with data per sample or per cohort or per any list provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace1 = \"CCLF_TSCA_2_0_2\"\n",
    "namespace1 = \"nci-mimoun-bi-org\"\n",
    "wm1 = dm.WorkspaceManager(namespace1,workspace1)\n",
    "pathto_cnvpng='segmented_copy_ratio_img'\n",
    "pathto_stats='sample_statistics'\n",
    "is_from_pairs=True\n",
    "pathto_snv='filtered_variants'\n",
    "pathto_seg='cnv_calls'\n",
    "datadir='gs://cclf_results/targeted/kim_sept/'\n",
    "\n",
    "specificlist=['CCLF_PEDS1012',\n",
    "'PEDS172',\n",
    "'PEDS182',\n",
    "'PEDS196',\n",
    "'PEDS204']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cclf.getReport(datadir='gs://cclf_results/targeted/kim_sept_2/',\n",
    "               specificlist=specificlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
