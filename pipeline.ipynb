{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jkobject/JKBIO\n",
    "# JKBio repo commit: 912087536d3cf6a7f1cbb00f9b131bc645780ee9 (9120875)\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "pathtoJK_parent = \"../depmap_omics/\"\n",
    "sys.path.append(pathtoJK_parent)\n",
    "from mgenepy import terra\n",
    "from mgenepy.utils import *\n",
    "from src.helper import *\n",
    "\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "To generate the required files to read in from Google sheets:\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** Each time, the `samplesetnames` and the `gsheeturllist` need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch in *chronological* order (e.g. CCLF_TWIST1 before CCLF_TWIST2)\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "samplesetnames = ['CCLF_TWIST65']\n",
    "\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "# TO EDIT:\n",
    "gsheeturllist = ['https://docs.google.com/spreadsheets/d/1K2rWwE4XeO-YJDGoMOmf3J-8hieSV99mzxeciW-kRXQ/edit?gid=0#gid=0']\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"cancer-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq_DRAGEN\"\n",
    "\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "# TODO: these are hard-coded in the helper.py file. However, I don't know how these columns are being used (if at all) and I think these may be meaningless.\n",
    "# source=\"CCLF\"\n",
    "# picard_aggregation_type_validation=\"PCR\"\n",
    "\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection errors? Reload the chunk below\n",
    "For example,\n",
    "`ConnectionError: ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows from the external ID gsheets: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/CCLF_TWIST/src/helper.py:124: FutureWarning: In a future version of pandas all arguments of DataFrame.dropna will be keyword-only.\n",
      "  newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped samples: {nan}\n",
      "Note that we expect to drop one per batch, the technical sequencing control.\n",
      "New length after dropping any samples with no value for the Collaborator Sample ID, Sample Type, or Exported DNA SM-ID: 47\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create preliminary versions of the sample and metadata tables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m newsamples, newmetadata \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_preliminary_sample_and_metadata_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwfrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplesetnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexternal_sheets_url_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgsheeturllist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcohorts2id_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcohorts2id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/CCLF_TWIST/src/helper.py:135\u001b[0m, in \u001b[0;36mcreate_preliminary_sample_and_metadata_tables\u001b[0;34m(wto, wfrom, samplesetnames, external_sheets_url_list, cohorts2id_url, forcekeep)\u001b[0m\n\u001b[1;32m    132\u001b[0m ttype \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    133\u001b[0m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollaborator Sample ID\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Type\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExported DNA SM-ID\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m--> 135\u001b[0m samples1\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m samples1\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    137\u001b[0m samples1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividual_alias\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_type\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m samples1\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples already in Terra: \u001b[39m\u001b[38;5;124m\"\u001b[39m, samples1\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(refids)\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/projects/CCLF_TWIST/src/helper.py:135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m ttype \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    133\u001b[0m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollaborator Sample ID\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Type\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExported DNA SM-ID\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m--> 135\u001b[0m samples1\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m [\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m samples1\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    137\u001b[0m samples1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividual_alias\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_type\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m samples1\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples already in Terra: \u001b[39m\u001b[38;5;124m\"\u001b[39m, samples1\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(refids)\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# create preliminary versions of the sample and metadata tables\n",
    "newsamples, newmetadata = create_preliminary_sample_and_metadata_tables(wto, wfrom, samplesetnames, external_sheets_url_list=gsheeturllist, cohorts2id_url=cohorts2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/projects/CCLF_TWIST/src/helper.py\u001b[0m(135)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    133 \u001b[0;31m    \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Collaborator Sample ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sample Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Exported DNA SM-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    134 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 135 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    137 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"individual_alias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  print(samples1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'samples1' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  samples1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'samples1' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/projects/CCLF_TWIST/src/helper.py\u001b[0m(135)\u001b[0;36mcreate_preliminary_sample_and_metadata_tables\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    133 \u001b[0;31m    \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Collaborator Sample ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sample Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Exported DNA SM-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    134 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 135 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    137 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"individual_alias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  samples1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    bai_file  \\\n",
      "sample_id                                                      \n",
      "SM-JKMLF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLH   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-LYFLX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-M1DNP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYJ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-NEJCO   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYC   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYD   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYE   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUQ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUR   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUS   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUT   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUU   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUV   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUW   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV1   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV7   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "\n",
      "             bait_set_capture  \\\n",
      "sample_id                       \n",
      "SM-JKMLF   BroadPanCancer2019   \n",
      "SM-JKMLG   BroadPanCancer2019   \n",
      "SM-JKMLH   BroadPanCancer2019   \n",
      "SM-LYFLX   BroadPanCancer2019   \n",
      "SM-M1DNP   BroadPanCancer2019   \n",
      "SM-N1LYI   BroadPanCancer2019   \n",
      "SM-N1LYJ   BroadPanCancer2019   \n",
      "SM-NEJCO   BroadPanCancer2019   \n",
      "SM-OHWXY   BroadPanCancer2019   \n",
      "SM-OHWXZ   BroadPanCancer2019   \n",
      "SM-OHWY2   BroadPanCancer2019   \n",
      "SM-OHWY3   BroadPanCancer2019   \n",
      "SM-OHWY4   BroadPanCancer2019   \n",
      "SM-OHWY5   BroadPanCancer2019   \n",
      "SM-OHWY6   BroadPanCancer2019   \n",
      "SM-OHWY8   BroadPanCancer2019   \n",
      "SM-OHWY9   BroadPanCancer2019   \n",
      "SM-OHWYA   BroadPanCancer2019   \n",
      "SM-OHWYB   BroadPanCancer2019   \n",
      "SM-OHWYC   BroadPanCancer2019   \n",
      "SM-OHWYD   BroadPanCancer2019   \n",
      "SM-OHWYE   BroadPanCancer2019   \n",
      "SM-OHWYF   BroadPanCancer2019   \n",
      "SM-OHWYG   BroadPanCancer2019   \n",
      "SM-OHWYI   BroadPanCancer2019   \n",
      "SM-OIKUP   BroadPanCancer2019   \n",
      "SM-OIKUQ   BroadPanCancer2019   \n",
      "SM-OIKUR   BroadPanCancer2019   \n",
      "SM-OIKUS   BroadPanCancer2019   \n",
      "SM-OIKUT   BroadPanCancer2019   \n",
      "SM-OIKUU   BroadPanCancer2019   \n",
      "SM-OIKUV   BroadPanCancer2019   \n",
      "SM-OIKUW   BroadPanCancer2019   \n",
      "SM-OIKUX   BroadPanCancer2019   \n",
      "SM-OIKUY   BroadPanCancer2019   \n",
      "SM-OIKUZ   BroadPanCancer2019   \n",
      "SM-OIKV1   BroadPanCancer2019   \n",
      "SM-OIKV2   BroadPanCancer2019   \n",
      "SM-OIKV3   BroadPanCancer2019   \n",
      "SM-OIKV4   BroadPanCancer2019   \n",
      "SM-OIKV5   BroadPanCancer2019   \n",
      "SM-OIKV6   BroadPanCancer2019   \n",
      "SM-OIKV7   BroadPanCancer2019   \n",
      "SM-OIKV8   BroadPanCancer2019   \n",
      "SM-OIKV9   BroadPanCancer2019   \n",
      "SM-OIKVA   BroadPanCancer2019   \n",
      "SM-OIKVB   BroadPanCancer2019   \n",
      "\n",
      "                                                    bam_file  \\\n",
      "sample_id                                                      \n",
      "SM-JKMLF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLH   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-LYFLX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-M1DNP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYJ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-NEJCO   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYC   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYD   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYE   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUQ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUR   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUS   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUT   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUU   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUV   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUW   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV1   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV7   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "\n",
      "                                            bam_md5_sum_path  chimera_rate  \\\n",
      "sample_id                                                                    \n",
      "SM-JKMLF   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.12   \n",
      "SM-JKMLG   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.91   \n",
      "SM-JKMLH   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.06   \n",
      "SM-LYFLX   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.08   \n",
      "SM-M1DNP   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.01   \n",
      "SM-N1LYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.19   \n",
      "SM-N1LYJ   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.99   \n",
      "SM-NEJCO   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.23   \n",
      "SM-OHWXY   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.22   \n",
      "SM-OHWXZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.16   \n",
      "SM-OHWY2   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.03   \n",
      "SM-OHWY3   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.90   \n",
      "SM-OHWY4   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.89   \n",
      "SM-OHWY5   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.85   \n",
      "SM-OHWY6   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.23   \n",
      "SM-OHWY8   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.24   \n",
      "SM-OHWY9   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.18   \n",
      "SM-OHWYA   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.96   \n",
      "SM-OHWYB   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.24   \n",
      "SM-OHWYC   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.93   \n",
      "SM-OHWYD   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.90   \n",
      "SM-OHWYE   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.15   \n",
      "SM-OHWYF   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.04   \n",
      "SM-OHWYG   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.23   \n",
      "SM-OHWYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.10   \n",
      "SM-OIKUP   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.06   \n",
      "SM-OIKUQ   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.07   \n",
      "SM-OIKUR   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.07   \n",
      "SM-OIKUS   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.09   \n",
      "SM-OIKUT   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.98   \n",
      "SM-OIKUU   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.93   \n",
      "SM-OIKUV   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.08   \n",
      "SM-OIKUW   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.33   \n",
      "SM-OIKUX   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.10   \n",
      "SM-OIKUY   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.79   \n",
      "SM-OIKUZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.22   \n",
      "SM-OIKV1   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.01   \n",
      "SM-OIKV2   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.93   \n",
      "SM-OIKV3   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.08   \n",
      "SM-OIKV4   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.76   \n",
      "SM-OIKV5   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.81   \n",
      "SM-OIKV6   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.07   \n",
      "SM-OIKV7   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.05   \n",
      "SM-OIKV8   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.18   \n",
      "SM-OIKV9   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.95   \n",
      "SM-OIKVA   gs://datarepo-37304743-bucket/48b21b72-1929-48...          0.79   \n",
      "SM-OIKVB   gs://datarepo-37304743-bucket/48b21b72-1929-48...          1.17   \n",
      "\n",
      "          collaborator_participant_id collaborator_sample_id  \\\n",
      "sample_id                                                      \n",
      "SM-JKMLF                CCLF_RCRF1003      CCLF_RCRF1003N_GL   \n",
      "SM-JKMLG               CCLF_cRCRF1045     CCLF_cRCRF1045N_GL   \n",
      "SM-JKMLH               CCLF_cRCRF1047     CCLF_cRCRF1066N_GL   \n",
      "SM-LYFLX                CCLF_RCRF1060         CCLF_RCRF1060N   \n",
      "SM-M1DNP                CCLF_RCRF1022         CCLF_RCRF1180N   \n",
      "SM-N1LYI                CCLF_RCRF1189         CCLF_RCRF1189N   \n",
      "SM-N1LYJ                CCLF_RCRF1190         CCLF_RCRF1190N   \n",
      "SM-NEJCO                  CCLF_NB1015           CCLF_NB1015N   \n",
      "SM-OHWXY                  CCLF_SJ1063         CCLF_SJ1063T_2   \n",
      "SM-OHWXZ                  CCLF_KL1081           CCLF_KL1089T   \n",
      "SM-OHWY2                  CCLF_GB1003           CCLF_GB1003T   \n",
      "SM-OHWY3                  CCLF_SJ1006         CCLF_SJ1006T_1   \n",
      "SM-OHWY4                      COLO007                COL007T   \n",
      "SM-OHWY5                      COLO007                COL007T   \n",
      "SM-OHWY6                        AB053                 AB053T   \n",
      "SM-OHWY8                  CCLF_SK1182           CCLF_SK1182T   \n",
      "SM-OHWY9                  CCLF_SK1183           CCLF_SK1183T   \n",
      "SM-OHWYA                  CCLF_SJ1010         CCLF_SJ1010T_1   \n",
      "SM-OHWYB                  CCLF_GB1008           CCLF_GB1008T   \n",
      "SM-OHWYC                      COLO007                COL007T   \n",
      "SM-OHWYD                      COLO007                COL007T   \n",
      "SM-OHWYE                  CCLF_KL1023           CCLF_KL1023T   \n",
      "SM-OHWYF                  CCLF_KL1023           CCLF_KL1023T   \n",
      "SM-OHWYG                  CCLF_SK1183           CCLF_SK1183T   \n",
      "SM-OHWYI                  CCLF_SJ1049         CCLF_SJ1049T_1   \n",
      "SM-OIKUP                  CCLF_AB1090           CCLF_AB1090T   \n",
      "SM-OIKUQ                  CCLF_SJ1004         CCLF_SJ1004T_2   \n",
      "SM-OIKUR                 CCLF_MDA1135          CCLF_MDA1135T   \n",
      "SM-OIKUS                        BT921                  BT921   \n",
      "SM-OIKUT                  CCLF_AB1168           CCLF_AB1168T   \n",
      "SM-OIKUU                  CCLF_SJ1009         CCLF_SJ1009T_2   \n",
      "SM-OIKUV                  CCLF_GB1004           CCLF_GB1004T   \n",
      "SM-OIKUW                  CCLF_SJ1067         CCLF_SJ1067T_1   \n",
      "SM-OIKUX                  CCLF_KL1338           CCLF_KL1338T   \n",
      "SM-OIKUY                 CCLF_MDA1031      CCLF_MDA1031T_AR5   \n",
      "SM-OIKUZ                  CCLF_SJ1037         CCLF_SJ1037T_2   \n",
      "SM-OIKV1                CCLF_PEDS1104         CCLF_PEDS1113T   \n",
      "SM-OIKV2                  CCLF_SK1176           CCLF_SK1176T   \n",
      "SM-OIKV3                  CCLF_SK1176           CCLF_SK1176T   \n",
      "SM-OIKV4                  CCLF_SJ1067         CCLF_SJ1067T_1   \n",
      "SM-OIKV5                  CCLF_SJ1067         CCLF_SJ1067T_1   \n",
      "SM-OIKV6                  CCLF_SJ1002         CCLF_SJ1002T_1   \n",
      "SM-OIKV7                  CCLF_AB1004           CCLF_AB1004T   \n",
      "SM-OIKV8                  CCLF_AB1052       CCLF_AB1053T_ASC   \n",
      "SM-OIKV9                  CCLF_SJ1048         CCLF_SJ1048T_1   \n",
      "SM-OIKVA                  CCLF_SJ1011         CCLF_SJ1020T_2   \n",
      "SM-OIKVB                CCLF_RCRF1053        CCLF_cRCRF1001T   \n",
      "\n",
      "                              coverage_region_1_metrics_file  \\\n",
      "sample_id                                                      \n",
      "SM-JKMLF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLH   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-LYFLX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-M1DNP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYJ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-NEJCO   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYC   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYD   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYE   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUQ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUR   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUS   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUT   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUU   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUV   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUW   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV1   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV7   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "\n",
      "                              coverage_region_2_metrics_file  \\\n",
      "sample_id                                                      \n",
      "SM-JKMLF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-JKMLH   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-LYFLX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-M1DNP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-N1LYJ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-NEJCO   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWXZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWY9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYC   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYD   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYE   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYF   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYG   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OHWYI   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUP   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUQ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUR   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUS   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUT   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUU   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUV   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUW   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUX   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUY   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKUZ   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV1   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV2   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV3   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV4   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV5   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV6   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV7   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV8   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKV9   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVA   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "SM-OIKVB   gs://datarepo-37304743-bucket/48b21b72-1929-48...   \n",
      "\n",
      "          crosscheck_pass_fail  ...   original_material_type participant_id  \\\n",
      "sample_id                       ...                                           \n",
      "SM-JKMLF                        ...  Whole Blood:Whole Blood       PT-248HW   \n",
      "SM-JKMLG                        ...  Whole Blood:Whole Blood       PT-2H9KW   \n",
      "SM-JKMLH                        ...  Whole Blood:Whole Blood       PT-2HF4C   \n",
      "SM-LYFLX                        ...      Tissue:Fresh Tissue       PT-2DWMB   \n",
      "SM-M1DNP                        ...      Tissue:Fresh Tissue       PT-28FWN   \n",
      "SM-N1LYI                        ...      Tissue:Fresh Tissue       PT-3MGXR   \n",
      "SM-N1LYJ                        ...      Tissue:Fresh Tissue       PT-3MGXT   \n",
      "SM-NEJCO                        ...      Tissue:Fresh Tissue       PT-3EI5T   \n",
      "SM-OHWXY                        ...  Cells:Cell Line, Viable       PT-3P1PQ   \n",
      "SM-OHWXZ                        ...  Cells:Cell Line, Viable       PT-25H83   \n",
      "SM-OHWY2                        ...      Tissue:Fresh Tissue       PT-3Y9US   \n",
      "SM-OHWY3                        ...      Tissue:Fresh Tissue       PT-3DTKJ   \n",
      "SM-OHWY4                        ...  Cells:Cell Line, Viable       PT-1I4WO   \n",
      "SM-OHWY5                        ...  Cells:Cell Line, Viable       PT-1I4WO   \n",
      "SM-OHWY6                        ...            Cells:Growing       PT-238DV   \n",
      "SM-OHWY8                        ...      Tissue:Fresh Tissue       PT-44IXO   \n",
      "SM-OHWY9                        ...      Tissue:Fresh Tissue       PT-44IXQ   \n",
      "SM-OHWYA                        ...      Tissue:Fresh Tissue       PT-3DTKR   \n",
      "SM-OHWYB                        ...     Bodily Fluid:Ascites       PT-457I9   \n",
      "SM-OHWYC                        ...  Cells:Cell Line, Viable       PT-1I4WO   \n",
      "SM-OHWYD                        ...  Cells:Cell Line, Viable       PT-1I4WO   \n",
      "SM-OHWYE                        ...  Cells:Cell Line, Viable       PT-238AW   \n",
      "SM-OHWYF                        ...  Cells:Cell Line, Viable       PT-238AW   \n",
      "SM-OHWYG                        ...      Tissue:Fresh Tissue       PT-44IXQ   \n",
      "SM-OHWYI                        ...  Cells:Cell Line, Viable       PT-3P1OX   \n",
      "SM-OIKUP                        ...  Cells:Cell Line, Viable       PT-2FK5B   \n",
      "SM-OIKUQ                        ...      Tissue:Fresh Tissue       PT-3DTKF   \n",
      "SM-OIKUR                        ...      Tissue:Fresh Tissue       PT-3CX8D   \n",
      "SM-OIKUS                        ...  Cells:Cell Line, Viable       PT-1WX5W   \n",
      "SM-OIKUT                  true  ...  Cells:Cell Line, Viable       PT-35UDD   \n",
      "SM-OIKUU                        ...      Tissue:Fresh Tissue       PT-3DTKP   \n",
      "SM-OIKUV                        ...      Tissue:Fresh Tissue       PT-3ZINH   \n",
      "SM-OIKUW                        ...  Cells:Cell Line, Viable       PT-3P1PY   \n",
      "SM-OIKUX                        ...  Cells:Cell Line, Viable       PT-2NPEJ   \n",
      "SM-OIKUY                        ...  Cells:Cell Line, Viable       PT-338BW   \n",
      "SM-OIKUZ                        ...  Cells:Cell Line, Viable       PT-3MDJC   \n",
      "SM-OIKV1                        ...  Cells:Cell Line, Viable       PT-2CJLL   \n",
      "SM-OIKV2                        ...  Whole Blood:Whole Blood       PT-3QKBT   \n",
      "SM-OIKV3                        ...  Whole Blood:Whole Blood       PT-3QKBT   \n",
      "SM-OIKV4                        ...  Cells:Cell Line, Viable       PT-3P1PY   \n",
      "SM-OIKV5                        ...  Cells:Cell Line, Viable       PT-3P1PY   \n",
      "SM-OIKV6                        ...      Tissue:Fresh Tissue       PT-3DTKB   \n",
      "SM-OIKV7                        ...      Tissue:Fresh Tissue       PT-248NV   \n",
      "SM-OIKV8                        ...      Tissue:Fresh Tissue       PT-29B72   \n",
      "SM-OIKV9                        ...  Cells:Cell Line, Viable       PT-3P1OV   \n",
      "SM-OIKVA                        ...      Tissue:Fresh Tissue       PT-3DTKT   \n",
      "SM-OIKVB                        ...      Tissue:Fresh Tissue       PT-2CT34   \n",
      "\n",
      "                 pdo percent_mapped reported_sex  research_project  \\\n",
      "sample_id                                                            \n",
      "SM-JKMLF   PDO-50107          99.49       Female           RP-3781   \n",
      "SM-JKMLG   PDO-50107          99.43         Male           RP-3781   \n",
      "SM-JKMLH   PDO-50107          99.47       Female           RP-3781   \n",
      "SM-LYFLX   PDO-50107          99.45                        RP-3781   \n",
      "SM-M1DNP   PDO-50107          99.53       Female           RP-3781   \n",
      "SM-N1LYI   PDO-50107          99.49       Female           RP-3781   \n",
      "SM-N1LYJ   PDO-50107          99.56       Female           RP-3781   \n",
      "SM-NEJCO   PDO-50107          99.48         Male           RP-3781   \n",
      "SM-OHWXY   PDO-50107          99.44         Male           RP-3781   \n",
      "SM-OHWXZ   PDO-50107          99.49                        RP-3781   \n",
      "SM-OHWY2   PDO-50107          99.49       Female           RP-3781   \n",
      "SM-OHWY3   PDO-50107          72.07       Female           RP-3781   \n",
      "SM-OHWY4   PDO-50107          56.05                        RP-3781   \n",
      "SM-OHWY5   PDO-50107          59.30                        RP-3781   \n",
      "SM-OHWY6   PDO-50107          99.50                        RP-3781   \n",
      "SM-OHWY8   PDO-50107          99.62         Male           RP-3781   \n",
      "SM-OHWY9   PDO-50107          99.49         Male           RP-3781   \n",
      "SM-OHWYA   PDO-50107          99.57         Male           RP-3781   \n",
      "SM-OHWYB   PDO-50107          99.43                        RP-3781   \n",
      "SM-OHWYC   PDO-50107          56.23                        RP-3781   \n",
      "SM-OHWYD   PDO-50107          55.99                        RP-3781   \n",
      "SM-OHWYE   PDO-50107          99.51                        RP-3781   \n",
      "SM-OHWYF   PDO-50107          99.49                        RP-3781   \n",
      "SM-OHWYG   PDO-50107          99.45         Male           RP-3781   \n",
      "SM-OHWYI   PDO-50107          99.52       Female           RP-3781   \n",
      "SM-OIKUP   PDO-50107          99.52         Male           RP-3781   \n",
      "SM-OIKUQ   PDO-50107          99.52       Female           RP-3781   \n",
      "SM-OIKUR   PDO-50107          99.41         Male           RP-3781   \n",
      "SM-OIKUS   PDO-50107          99.52                        RP-3781   \n",
      "SM-OIKUT   PDO-50107          99.53       Female           RP-3781   \n",
      "SM-OIKUU   PDO-50107          56.81         Male           RP-3781   \n",
      "SM-OIKUV   PDO-50107          99.55       Female           RP-3781   \n",
      "SM-OIKUW   PDO-50107          54.41         Male           RP-3781   \n",
      "SM-OIKUX   PDO-50107          99.46         Male           RP-3781   \n",
      "SM-OIKUY   PDO-50107          99.47         Male           RP-3781   \n",
      "SM-OIKUZ   PDO-50107          99.52       Female           RP-3781   \n",
      "SM-OIKV1   PDO-50107          99.41         Male           RP-3781   \n",
      "SM-OIKV2   PDO-50107          98.85         Male           RP-3781   \n",
      "SM-OIKV3   PDO-50107          97.65         Male           RP-3781   \n",
      "SM-OIKV4   PDO-50107          56.61         Male           RP-3781   \n",
      "SM-OIKV5   PDO-50107          56.00         Male           RP-3781   \n",
      "SM-OIKV6   PDO-50107          99.50       Female           RP-3781   \n",
      "SM-OIKV7   PDO-50107          99.43                        RP-3781   \n",
      "SM-OIKV8   PDO-50107          99.46         Male           RP-3781   \n",
      "SM-OIKV9   PDO-50107          95.70         Male           RP-3781   \n",
      "SM-OIKVA   PDO-50107          57.70         Male           RP-3781   \n",
      "SM-OIKVB   PDO-50107          99.41                        RP-3781   \n",
      "\n",
      "          root_sample_ids sample_type        sequencer_model  tdr:sample_id  \n",
      "sample_id                                                                    \n",
      "SM-JKMLF       [SM-JKMLF]      Normal  Illumina NovaSeq 6000       SM-JKMLF  \n",
      "SM-JKMLG       [SM-JKMLG]      Normal  Illumina NovaSeq 6000       SM-JKMLG  \n",
      "SM-JKMLH       [SM-JKMLH]      Normal  Illumina NovaSeq 6000       SM-JKMLH  \n",
      "SM-LYFLX       [SM-LYFLX]      Normal  Illumina NovaSeq 6000       SM-LYFLX  \n",
      "SM-M1DNP       [SM-M1DNP]      Normal  Illumina NovaSeq 6000       SM-M1DNP  \n",
      "SM-N1LYI       [SM-N1LYI]      Normal  Illumina NovaSeq 6000       SM-N1LYI  \n",
      "SM-N1LYJ       [SM-N1LYJ]      Normal  Illumina NovaSeq 6000       SM-N1LYJ  \n",
      "SM-NEJCO       [SM-NEJCO]      Normal  Illumina NovaSeq 6000       SM-NEJCO  \n",
      "SM-OHWXY       [SM-OHWXY]       Tumor  Illumina NovaSeq 6000       SM-OHWXY  \n",
      "SM-OHWXZ       [SM-OHWXZ]       Tumor  Illumina NovaSeq 6000       SM-OHWXZ  \n",
      "SM-OHWY2       [SM-OHWY2]       Tumor  Illumina NovaSeq 6000       SM-OHWY2  \n",
      "SM-OHWY3       [SM-OHWY3]       Tumor  Illumina NovaSeq 6000       SM-OHWY3  \n",
      "SM-OHWY4       [SM-OHWY4]       Tumor  Illumina NovaSeq 6000       SM-OHWY4  \n",
      "SM-OHWY5       [SM-OHWY5]       Tumor  Illumina NovaSeq 6000       SM-OHWY5  \n",
      "SM-OHWY6       [SM-OHWY6]       Tumor  Illumina NovaSeq 6000       SM-OHWY6  \n",
      "SM-OHWY8       [SM-OHWY8]       Tumor  Illumina NovaSeq 6000       SM-OHWY8  \n",
      "SM-OHWY9       [SM-OHWY9]       Tumor  Illumina NovaSeq 6000       SM-OHWY9  \n",
      "SM-OHWYA       [SM-OHWYA]       Tumor  Illumina NovaSeq 6000       SM-OHWYA  \n",
      "SM-OHWYB       [SM-OHWYB]       Tumor  Illumina NovaSeq 6000       SM-OHWYB  \n",
      "SM-OHWYC       [SM-OHWYC]       Tumor  Illumina NovaSeq 6000       SM-OHWYC  \n",
      "SM-OHWYD       [SM-OHWYD]       Tumor  Illumina NovaSeq 6000       SM-OHWYD  \n",
      "SM-OHWYE       [SM-OHWYE]       Tumor  Illumina NovaSeq 6000       SM-OHWYE  \n",
      "SM-OHWYF       [SM-OHWYF]       Tumor  Illumina NovaSeq 6000       SM-OHWYF  \n",
      "SM-OHWYG       [SM-OHWYG]       Tumor  Illumina NovaSeq 6000       SM-OHWYG  \n",
      "SM-OHWYI       [SM-OHWYI]       Tumor  Illumina NovaSeq 6000       SM-OHWYI  \n",
      "SM-OIKUP       [SM-OIKUP]       Tumor  Illumina NovaSeq 6000       SM-OIKUP  \n",
      "SM-OIKUQ       [SM-OIKUQ]       Tumor  Illumina NovaSeq 6000       SM-OIKUQ  \n",
      "SM-OIKUR       [SM-OIKUR]       Tumor  Illumina NovaSeq 6000       SM-OIKUR  \n",
      "SM-OIKUS       [SM-OIKUS]       Tumor  Illumina NovaSeq 6000       SM-OIKUS  \n",
      "SM-OIKUT       [SM-OIKUT]       Tumor  Illumina NovaSeq 6000       SM-OIKUT  \n",
      "SM-OIKUU       [SM-OIKUU]       Tumor  Illumina NovaSeq 6000       SM-OIKUU  \n",
      "SM-OIKUV       [SM-OIKUV]       Tumor  Illumina NovaSeq 6000       SM-OIKUV  \n",
      "SM-OIKUW       [SM-OIKUW]       Tumor  Illumina NovaSeq 6000       SM-OIKUW  \n",
      "SM-OIKUX       [SM-OIKUX]       Tumor  Illumina NovaSeq 6000       SM-OIKUX  \n",
      "SM-OIKUY       [SM-OIKUY]       Tumor  Illumina NovaSeq 6000       SM-OIKUY  \n",
      "SM-OIKUZ       [SM-OIKUZ]       Tumor  Illumina NovaSeq 6000       SM-OIKUZ  \n",
      "SM-OIKV1       [SM-OIKV1]       Tumor  Illumina NovaSeq 6000       SM-OIKV1  \n",
      "SM-OIKV2       [SM-OIKV2]       Tumor  Illumina NovaSeq 6000       SM-OIKV2  \n",
      "SM-OIKV3       [SM-OIKV3]       Tumor  Illumina NovaSeq 6000       SM-OIKV3  \n",
      "SM-OIKV4       [SM-OIKV4]       Tumor  Illumina NovaSeq 6000       SM-OIKV4  \n",
      "SM-OIKV5       [SM-OIKV5]       Tumor  Illumina NovaSeq 6000       SM-OIKV5  \n",
      "SM-OIKV6       [SM-OIKV6]       Tumor  Illumina NovaSeq 6000       SM-OIKV6  \n",
      "SM-OIKV7       [SM-OIKV7]       Tumor  Illumina NovaSeq 6000       SM-OIKV7  \n",
      "SM-OIKV8       [SM-OIKV8]       Tumor  Illumina NovaSeq 6000       SM-OIKV8  \n",
      "SM-OIKV9       [SM-OIKV9]       Tumor  Illumina NovaSeq 6000       SM-OIKV9  \n",
      "SM-OIKVA       [SM-OIKVA]       Tumor  Illumina NovaSeq 6000       SM-OIKVA  \n",
      "SM-OIKVB       [SM-OIKVB]       Tumor  Illumina NovaSeq 6000       SM-OIKVB  \n",
      "\n",
      "[47 rows x 30 columns]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  bt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipping 1 hidden frame(s)]\u001b[0m\n",
      "\n",
      "  \u001b[0;32m/tmp/ipykernel_67456/705190218.py\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      1 \u001b[0m\u001b[0;31m# create preliminary versions of the sample and metadata tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 2 \u001b[0;31m\u001b[0mnewsamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_preliminary_sample_and_metadata_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesetnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_sheets_url_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgsheeturllist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcohorts2id_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcohorts2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/ubuntu/projects/CCLF_TWIST/src/helper.py\u001b[0m(135)\u001b[0;36mcreate_preliminary_sample_and_metadata_tables\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    133 \u001b[0;31m    \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Collaborator Sample ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sample Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Exported DNA SM-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    134 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 135 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    136 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    137 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"individual_alias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  \u001b[0;32m/home/ubuntu/projects/CCLF_TWIST/src/helper.py\u001b[0m(135)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m    133 \u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Collaborator Sample ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sample Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Exported DNA SM-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    134 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 135 \u001b[0;31m    \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    136 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    137 \u001b[0m    \u001b[0msamples1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"individual_alias\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required metadata columns\n",
    "We do not include samples that are missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Participant ID <- This is the patient identifier\n",
    "- Sample Type\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- Only the technical controls won't have this information.\n",
    "- Collection\n",
    "- Tissue Site\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes GP delivers entries that have broken bam file paths\n",
    "# catch and filter out invalid entries\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "def check_broken_path(row):\n",
    "    bucket_name = row['cram_or_bam_path'].split(\"/\")[2]\n",
    "    file_name = \"/\".join(row['cram_or_bam_path'].split(\"/\")[3:])\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    return storage.Blob(bucket=bucket, name=file_name).exists(storage_client)\n",
    "\n",
    "newsamples = newsamples[newsamples.apply(check_broken_path, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newmetadata.external_id_validation.is_unique, \"duplicated external IDs, ask ops to rename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)\n",
    "\n",
    "# specify required metadata columns\n",
    "tolook = ['Collaborator Participant ID','Exported DNA SM-ID', 'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site', 'Original Material Type', 'Material Type','Primary Disease', 'Collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any samples are missing some of the required metadata, stop now and ask the CCLF team to fill out the missing values in the External Sheet.\n",
    "check_required_metadata_columns(df, tolook, cohorts2id_url=cohorts2id, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: check weirdly formatted NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further check weirdly formatted NAs:\n",
    "for c in tolook:\n",
    "    assert(df[c].str.startswith(\"#N/A\").any() == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep samples that have all the appropriate metadata information\n",
    "df = check_required_metadata_columns(df, required_metadata_cols=tolook, cohorts2id_url=cohorts2id, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample df to upload to Terra\n",
    "sample_info = create_sample_df_for_terra(wto, df, cohorts2id_url=cohorts2id)\n",
    "\n",
    "# sanity check: this should be what you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "\n",
    "## check: create directory \"data/sample_infos\" if does not exist\n",
    "filepath = 'data/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file you just saved\n",
    "filepath = 'data/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if there are duplicate external ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_samples = wto.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ids = existing_samples.external_id_validation.tolist()# + sample_info.external_id_validation.tolist()\n",
    "dups = set([x for x in existing_ids if existing_ids.count(x) > 1])\n",
    "if len(dups) > 0:\n",
    "    raise Exception(\"Found duplicate external validation ids: \", dups, \" , check with ops to verify if these need to be renamed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpairs = create_pairs_table(sample_info, wto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading samples and pairs to Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))\n",
    "    wto.upload_samples(sample_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uploading pairs...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairs per batch dictionary\n",
    "dict_pairs_per_batch = create_dict_of_pairs_per_sampleset(newpairs, sample_info, samplesetnames, save=True)\n",
    "\n",
    "# Load from saved file\n",
    "dict_pairs_per_batch = np.load('dict_pairs_per_batch.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  functionalized version\n",
    "create_sample_sets_per_batch(sample_info, samplesetnames, samplesetnames_all, samplesetnames_tumors, samplesetnames_normals, proc_workspace)\n",
    "\n",
    "create_pair_sets_per_batch(samplesetnames, samplesetnames_pairs, proc_workspace, dict_pairs_per_batch)\n",
    "\n",
    "create_samplesets_and_pairsets_per_cohort(sample_info, samplesetnames, proc_workspace, cohorts2id, newpairs)\n",
    "\n",
    "# get a list of all normals in the processing workspace\n",
    "# TODO: I think by this point in the pipeline all of the samples will have been uploaded to Terra. Thus, I think we may only need the following one-liner to get a list of all normals:\n",
    "# all_normals = [i for i, r in wto.get_samples().iterrows() if r['sample_type'] == \"Normal\"]\n",
    "\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "print(len(normalsid))\n",
    "\n",
    "refsamples = wto.get_samples()\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"])\n",
    "print(len(normalsid))\n",
    "\n",
    "create_aggregate_samplesets(normalsid, proc_workspace, wto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Push to Git repo (CCLF_TWIST) now!\n",
    "This way, other people can easily take over the process of running the pipelines and feel confident that they have the most up-to-date version of this Jupyter notebook.\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- **do manual step here on local machine: need to merge two fingerprinting tables**\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(proc_workspace, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(proc_workspace, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(proc_workspace, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updates the config for each batch id\n",
    "CreatePanelOfNormalsGATK_PANCAN = []\n",
    "DepthOfCovQC_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config for workflow that creates the PON for CNV calling\n",
    "    createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "    # edit the config\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    createPON_config['outputs']\n",
    "    # update the config in Terra\n",
    "    wto.update_config(createPON_config)\n",
    "    \n",
    "    # create batch-specific PON to be used for CNVs\n",
    "    CreatePanelOfNormalsGATK_PANCAN.append(wto.create_submission(\"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals[ind]))\n",
    "    DepthOfCovQC_PANCAN.append(wto.create_submission(\"DepthOfCovQC_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config\n",
    "    CNV_config = wto.get_config('CallSomaticCNV_PANCAN')\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon']\n",
    "\n",
    "    # edit the config\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    CNV_config['inputs']\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(CNV_config)\n",
    "    CallSomaticCNV_PANCAN.append(wto.create_submission(\"CallSomaticCNV_PANCAN\", samplesetnames_all[ind], etype='sample_set', expression='this.samples', use_callcache = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to rerun the notebook and generate the sample key variables\n",
    "sample_info, all_pairsets, cohorts_per_batch, cohort_pairsets, all_changed_cohorts = \\\n",
    "    regenerate_variables(wto, samplesetnames_all, cohorts2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_pairsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNV map for each batch\n",
    "terra.createManySubmissions(proc_workspace, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all, use_callcache = False)\n",
    "# create CNV map for each cohort\n",
    "terra.createManySubmissions(proc_workspace, \"PlotSomaticCNVMaps_PANCAN\", list(all_changed_cohorts), use_callcache = False)\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(proc_workspace, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(proc_workspace, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=True)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current config\n",
    "mutect1_config = wto.get_config('CreatePoNSNV_Mutect1')\n",
    "mutect2_config = wto.get_config('CreatePoN_SNV_MuTect2')\n",
    "\n",
    "# edit the config\n",
    "mutect1_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1\"'\n",
    "mutect2_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2\"'\n",
    "mutect1_config['outputs']['CreatePanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "mutect2_config['outputs']['CreatePanelOfNormals.createPanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "\n",
    "# update the config in Terra\n",
    "wto.update_config(mutect1_config)\n",
    "wto.update_config(mutect2_config)\n",
    "\n",
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: It may be okay if some samples fail the MutationCalling_Tumors_TWIST workflow. Samples will fail if no mutations made it through Mutect1 and Mutect2's filters.\n",
    "The MutationCalling_Tumors_TWIST pipeline has been updated to use GATK4, and there are many more pre-filters for Mutect2 that greatly reduce the computation time required. As part of this change, however, we discovered that the next step (FilterMutectCalls) will fail if the vcf it gets from Mutect2 is empty. This can happen if all the variants are filtered out. Thus, long story short, if the sample fails at the FilterMutectCalls step and the log file shows that there were no variants left after Mutect1 and Mutect2, then this failure is not something to worry about.\n",
    "\n",
    "The details can be found at https://gatk.broadinstitute.org/hc/en-us/articles/360037593851-Mutect2 by searching for \"Read filters\". In addition to the prefilters described in that section, Mutect2 also prefilters sites that are in the matched normal and the PoN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = []\n",
    "MutationCalling_Tumors_TWIST = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    \n",
    "    # get config \n",
    "    mutcall_tumor = wto.get_config('MutationCalling_Tumors_TWIST')\n",
    "\n",
    "    # edit the config\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "    \n",
    "    # check config\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'])\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutcall_tumor)\n",
    "\n",
    "    # create submission\n",
    "    SNV_PostProcessing_Normals += [wto.create_submission(\"SNV_PostProcessing_Normals\", samplesetnames_normals[ind])]\n",
    "    \n",
    "    MutationCalling_Tumors_TWIST += [wto.create_submission(\"MutationCalling_Tumors_TWIST\", samplesetnames_pairs[ind], etype='pair_set', expression='this.pairs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'SNV_PostProcessing_Normals' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: you might see that some of the cohorts fail on this workflow. That can be expected: the workflow needs cohorts with at least 2 acceptable cell lines to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(proc_workspace, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregate SNV tsvs for each batch\n",
    "terra.createManySubmissions(proc_workspace, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "# create aggregate SNV tsvs for each cohort\n",
    "terra.createManySubmissions(proc_workspace, \"SNVPostProcessing_TWIST\", list(cohort_pairsets))\n",
    "print(\"Submitted final jobs for SNV pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingerprinting (FNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the FNG pileup counts for each sample\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(proc_workspace, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the proper \"super\" set of samples to run through the FNG compiler\n",
    "# if processing multiple batches at once, this will correspond to a sample \"super\" set containing the sample IDs from all the new batches. If processing a single batch, this will just be that batch's sample set.\n",
    "samples_to_add = []\n",
    "for set_name in samplesetnames_all:\n",
    "#     samples_to_add += wto.get_sample_attributes_in_set(set_name).index.tolist()\n",
    "    samples_to_add += wto.get_sample_sets().loc[set_name, 'samples']\n",
    "samples_to_add\n",
    "print(samples_to_add)\n",
    "\n",
    "fng_sampleset_id = \"_\".join(samplesetnames)\n",
    "print(fng_sampleset_id)\n",
    "\n",
    "terra.addToSampleSet(workspace = proc_workspace, samplesetid = fng_sampleset_id, samples = samples_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the output config to create the new FNG database\n",
    "# get current config for the FNG compiling workflow\n",
    "fngCompile_config = wto.get_config('FNG_Compile_db_slow_download')\n",
    "# edit the config\n",
    "# TODO: would be nice to be able to change the name of the outputted FNG database. Right now, all named the same.\n",
    "# fngCompile_config['inputs']['FNG_Compile_db.compile_db.output_file_name'] = '\"fingerprinting_db_through_' + samplesetnames[-1] + '.txt\"'\n",
    "fngCompile_config['outputs']['FNG_Compile_db.compile_db.fingerprinting_db'] = 'workspace.fingerprinting_db_through_' + samplesetnames[-1]\n",
    "fngCompile_config['outputs']['FNG_Compile_db.compile_db.fingerprinting_db_current'] = 'workspace.fingerprinting_db'\n",
    "fngCompile_config['outputs']\n",
    "\n",
    "print(fngCompile_config)\n",
    "# update the config in Terra\n",
    "wto.update_config(fngCompile_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FNG db using Method Version 7\n",
    "# TODO: update method to change the output file name to something more descriptive\n",
    "# pass in a sample set containing all of the new samples you're processing\n",
    "# do not use call cache; we need to see if the github repo has been updated and thus must clone each time\n",
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", fng_sampleset_id, use_callcache=False)\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"FNG_Compile_db_slow_download\" command is problematic currently\n",
    "**NOTE**: The \"FNG_Compile_db_slow_download\" command is problematic currently in this workspace because this workspace only contains TWIST samples, but we want to be able to look at fingerprinting data from both TSCA and TWIST. Currently, Gwen merges the previous fingerprinting_db.txt file with the newly created fingerprinting_db.txt. We'll have to repeat this merging procedure unless we edit the workflow. Gwen has started this process, but hasn't finished the edits (just need to build the proper docker container). So for now (1/15/20), still need to do the merging locally (unfortunately).\n",
    "\n",
    "See the R file: \"src/FNG_TWIST_and_TSCA_merge\" in the CCLF_TWIST GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: this is where the merging work in R needs to be performed on local.\n",
    "See the R file: `CCLF_TWIST/src/FNG_TWIST_and_TSCA_merge.Rmd` in the CCLF_TWIST GitHub repo. \n",
    "-> The details of what you need to do are in this Rmd file.\n",
    "-> Once the local work is done, continue running through the chunks below.\n",
    "\n",
    "I also have the same merging script in `CCLF_TWIST/workflow/scripts/merge_fingerprinting_dfs.R`, and a WDL script to run this at `CCLF_TWIST/workflow/scripts/merge_fng_databases.wdl`. This has been uploaded to Terra, too, and I have made a workflow for this in the data processing workspace: \"merge_FNGs\". However, the Docker image is missing the Argparse package. I think once this is added to the Docker image, the workflow should run smoothly on Terra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8/4/2021, Javad's efforts to implement the R code in the python cell\n",
    "# This seems to be redundant though. For some reason, the binding of the\n",
    "# data already has happened in the FNG_Compile_db_slow_download output\n",
    "# This seems to at least be the case for TWIST35\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "workspace_attr = wto.get_workspace_metadata()['workspace']['attributes']\n",
    "fp_old = pd.read_csv(workspace_attr['fingerprinting_db_through_CCLF_TWIST34'], sep='\\t')\n",
    "fp_new = pd.read_csv(workspace_attr['fingerprinting_db_through_' + samplesetnames[-1]], sep='\\t')\n",
    "assert not fp_new.duplicated().any()\n",
    "assert not fp_old.duplicated().any()\n",
    "assert set(fp_new.columns) == set(fp_old.columns)\n",
    "assert (set(fp_old['batch']) - set(fp_new['batch'])) == set([])\n",
    "\n",
    "# not sure why this is necessary. It produces identical results\n",
    "fp_all = pd.concat([fp_new, fp_old])\n",
    "fp_all.drop_duplicates(inplace=True)\n",
    "fp_all.reset_index(drop=True, inplace=True)\n",
    "assert fp_new.shape == fp_all.shape\n",
    "assert fp_all.equals(fp_new)\n",
    "fp_all.to_csv('/tmp/fingerprinting_db_through_{}.txt'.format(samplesetnames[-1]), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each batch, query the FNG database\n",
    "FNG_Query_db = terra.createManySubmissions(proc_workspace, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra. Update the Asana task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # If got a new cohort label / abbreviation and need to update data that already exists in Terra: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data from Gsheet metadata\n",
    "# metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "# metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "# metadata.index = metadata['Exported DNA SM-ID']\n",
    "\n",
    "# display(metadata.head())\n",
    "\n",
    "# # Pull relevant sample_info from Terra\n",
    "# sample_info = wto.get_samples()\n",
    "# sample_info = sample_info[sample_info[\"batch\"].isin(samplesetnames)]\n",
    "# display(sample_info.head())\n",
    "# # display(sample_info.loc[:,[\"cohorts\", \"Collection\"]])\n",
    "\n",
    "# # Merge new Metadata with stuff existing in Terra (in particular, we often want to update the Collections and cohorts columns)\n",
    "# updated = pd.concat([sample_info.drop(columns=['Collection']), metadata[\"Collection\"].reindex(sample_info.index)], axis=1)\n",
    "# updated.head()\n",
    "# updated.columns.tolist()\n",
    "# updated = updated.reindex(columns=(['Collection', 'cohorts'] + list([a for a in updated.columns if a not in ['Collection', 'cohorts']]) ))\n",
    "# updated\n",
    "\n",
    "# updated = getCohortAbbreviations(updated)\n",
    "# updated\n",
    "# print(\"Final 'updated' df, looking at just the cohorts and Collection columns:\")\n",
    "# display(updated.loc[:,[\"cohorts\", \"Collection\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
