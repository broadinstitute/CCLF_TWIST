{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import sys\n",
    "pathtoJK = \"../JKBio\"\n",
    "sys.path.insert(0, pathtoJK)\n",
    "import TerraFunction as terra\n",
    "import CCLF_processing as cclf\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from Helper import *\n",
    "import numpy as np\n",
    "from gsheets import Sheets\n",
    "# https://github.com/jkobject/JKBIO\n",
    "\n",
    "\"\"\"\n",
    "Log into the Google Developers Console with the Google account whose spreadsheets you want to access.\n",
    "Create (or select) a project and enable the Drive API and Sheets API (under Google Apps APIs).\n",
    "\n",
    "https://console.developers.google.com/\n",
    "\n",
    "Go to the Credentials for your project and create New credentials > OAuth client ID > of type Other.\n",
    "In the list of your OAuth 2.0 client IDs click Download JSON for the Client ID you just created.\n",
    "Save the file as client_secrets.json in your home directory (user directory).\n",
    "Another file, named storage.json in this example, will be created after successful authorization\n",
    "to cache OAuth data.\n",
    "\n",
    "On you first usage of gsheets with this file (holding the client secrets),\n",
    "your webbrowser will be opened, asking you to log in with your Google account to authorize\n",
    "this client read access to all its Google Drive files and Google Sheets.\n",
    "\"\"\"\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCLF TWIST Pipeline\n",
    "\n",
    "*go to the [readme](./README.md) to see more about execution*\n",
    "\n",
    "**Note that the comment \"# TO EDIT:\" is included before any line of code that needs to be changed between different runs of this notebook**\n",
    "\n",
    "\n",
    "This pipeline has the following major steps:\n",
    "1. Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "2. Create a TSV of the new sample information\n",
    "3. Create a TSV of the new sample set information (e.g. cohorts)\n",
    "4. Upload the sample information and sample set TSVs to the Terra workspace \n",
    "5. Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Pull in information about the TWIST batch(es) from Google sheet(s).\n",
    "\n",
    "**Note:** The following cell contains a lot of information that needs to be changed each time this pipeline is run.\n",
    "\n",
    "You would want to write the samplesetnames you are interested in and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample set names for each batch\n",
    "# if you only have one batch to run, still make it a list e.g. [\"CCLF_TWIST1\"]\n",
    "# this ensures that the pipeline will run as designed\n",
    "# TO EDIT:\n",
    "samplesetnames = [\"CCLF_TWIST6\"]\n",
    "\n",
    "# list of the external sheets produced for each batch you want to run through the pipeline\n",
    "# TO EDIT:\n",
    "gsheeturllist = [\"https://docs.google.com/spreadsheets/d/1LK-BL6uF9mrzeCQaGb5rmyt1wr6X5JXeIPM9vo8VAL4\"]\n",
    "\n",
    "# generate the sample set names we will use in Terra\n",
    "samplesetnames_normals = [s + '_normals' for s in samplesetnames]\n",
    "samplesetnames_tumors = [s + '_tumors' for s in samplesetnames]\n",
    "samplesetnames_pairs = [s + '_pairs' for s in samplesetnames]\n",
    "samplesetnames_all = [s + '_all' for s in samplesetnames]\n",
    "\n",
    "# workspace where we are pulling in the data from\n",
    "data_workspace=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_PanCancer_PanelSeq\"\n",
    "# workspace where we are running the workflows\n",
    "proc_workspace=\"nci-mimoun-bi-org/PANCAN_TWIST copy\"\n",
    "\n",
    "source=\"CCLF\"\n",
    "\n",
    "picard_aggregation_type_validation=\"PCR\"\n",
    "forcekeep=[]\n",
    "\n",
    "# mapping abbreviations to full names/descriptions\n",
    "cohorts2id=\"https://docs.google.com/spreadsheets/d/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfrom = dm.WorkspaceManager(data_workspace)\n",
    "wto = dm.WorkspaceManager(proc_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the samples\n",
    "\n",
    "- we load the samples from data workspace and load the metadata files\n",
    "- we remove data that has already been processed\n",
    "- we create the final ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA'], 'external_id_validation':['NA'], 'sample_type':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-01 09:31:56::INFO  URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1LK-BL6uF9mrzeCQaGb5rmyt1wr6X5JXeIPM9vo8VAL4?alt=json\n",
      "2019-12-01 09:31:56::INFO  URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1LK-BL6uF9mrzeCQaGb5rmyt1wr6X5JXeIPM9vo8VAL4/values:batchGet?majorDimension=ROWS&valueRenderOption=UNFORMATTED_VALUE&dateTimeRenderOption=FORMATTED_STRING&ranges=Plate+1&ranges=Sheet5&ranges=Sheet3&ranges=Sheet2&ranges=Email&ranges=Changelog&alt=json\n"
     ]
    }
   ],
   "source": [
    "# we look at all the samples we already have in the TWIST workspace\n",
    "refsamples = wto.get_samples()\n",
    "refids = refsamples.index\n",
    "\n",
    "# get the External sheet data from google sheets\n",
    "gsheets = [sheets.get(url).sheets[0].to_frame() for url in gsheeturllist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-01 09:31:58::INFO  URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU?alt=json\n",
      "2019-12-01 09:31:58::INFO  URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1R97pgzoX0YClGDr5nmQYQwimnKXxDBGnGzg7YPlhZJU/values:batchGet?majorDimension=ROWS&valueRenderOption=UNFORMATTED_VALUE&dateTimeRenderOption=FORMATTED_STRING&ranges=0&alt=json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped indices: {0}\n",
      "new length: 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a column with batch information (e.g. CCLF_TWIST1 vs CCLF_TWIST2)\n",
    "metadata = pd.concat(gsheets,sort=False, keys = samplesetnames)\n",
    "metadata = metadata.reset_index().rename(columns = {'level_0':'batch', \"External ID\":'external_id_validation'}).drop(['level_1'], axis = 'columns')\n",
    "print(len(metadata))\n",
    "\n",
    "# we use this gsheet package to get all the sheets into one dataframe\n",
    "cohorts = sheets.get(cohorts2id).sheets[0].to_frame()\n",
    "\n",
    "# we look at all the samples we already have in Terra\n",
    "# we do some corrections just in case\n",
    "samples1 = wfrom.get_samples().replace(np.nan, '', regex=True)\n",
    "\n",
    "# creating sample_id (like in processing workspace) for metadata and samples1\n",
    "newmetadata = metadata.dropna(0, subset=['Collaborator Sample ID','Sample Type','Exported DNA SM-ID'])\n",
    "print(\"dropped indices: \"+str(set(metadata.index.tolist())-set(newmetadata.index.tolist())))\n",
    "print('new length: '+str(len(newmetadata)))\n",
    "metadata=newmetadata\n",
    "\n",
    "ttype = [i for i in metadata[\"Sample Type\"]]\n",
    "metadata['sample_id'] = [str(val['Collaborator Sample ID'][:-1]) + '-' + str(val['Sample Type']) + '-' + str(val['Exported DNA SM-ID']) for i, val in metadata.iterrows()]\n",
    "\n",
    "samples1.index = [i.split('_')[2] for i, val in samples1.iterrows()]\n",
    "\n",
    "samples1['sample_id'] = [str(val[\"individual_alias\"]) + '-' + str(val['sample_type']) + '-' + i for i, val in samples1.iterrows()]\n",
    "metadata.index = metadata['Exported DNA SM-ID']\n",
    "# filtering on what already exists in the processing workspace (refids)\n",
    "newsamples = samples1[(~samples1.index.isin(refids)) | samples1.index.isin(forcekeep)]\n",
    "tokeep = set(metadata.index) & set(newsamples.index)\n",
    "len(tokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we could not add these samples from the data workspace as we don't have metadata for them: \n",
      "Index(['LCSET-15778', 'LCSET-16192', 'LCSET-16427', 'LCSET-16681',\n",
      "       'LCSET-16681', 'LCSET-16782', 'LCSET-16859', 'LCSET-16923',\n",
      "       'LCSET-17042', 'SM-JL77H',\n",
      "       ...\n",
      "       'SM-JNF7V', 'SM-JNF7W', 'SM-JNF7X', 'SM-JNF7Y', 'SM-JNF7Z', 'SM-JNF81',\n",
      "       'SM-JNF82', 'SM-JNF83', 'SM-JNF84', 'SM-JNF85'],\n",
      "      dtype='object', length=149)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47, 222)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# useful to merge the two df, sm-id is one of the only unique id here\n",
    "if len(newsamples[~newsamples.index.isin(tokeep)]) > 0:\n",
    "    print('we could not add these samples from the data workspace as we don\\'t have metadata for them: ' + '\\n' \n",
    "          + str(newsamples[~newsamples.index.isin(tokeep)].index))\n",
    "newsamples = newsamples[newsamples.index.isin(tokeep)]\n",
    "newmetadata = metadata[metadata.index.isin(tokeep)].sort_index().drop_duplicates(\"Exported DNA SM-ID\")\n",
    "newsamples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the sample information dataframe\n",
    "Create a dataframe of the new sample information\n",
    "\n",
    "**Note:** It can be difficult to recreate the sample_info variable below after you have already uploaded TSVs to Terra since this pipeline specifically looks for samples that do not already exist in the workspace. When running the pipeline on a new batch of data, **I recommend writing the final sample_info to a file.**\n",
    "\n",
    "**Note 2:** We replace all \"/\" in the External IDs with \"_\". This prevents errors when filepaths are created using the external IDs in Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          batch external_id_validation\n",
      "Exported DNA SM-ID                                    \n",
      "SM-JJ5R1            CCLF_TWIST6         CY017T_Primary\n",
      "SM-JJ5R2            CCLF_TWIST6         CY016T_Primary\n",
      "SM-JJ5R3            CCLF_TWIST6   CCLF_SS1041T_Primary\n",
      "SM-JJ5R4            CCLF_TWIST6   CCLF_CY1025T_Primary\n",
      "SM-JJ5R5            CCLF_TWIST6   CCLF_CY1024T_Primary\n"
     ]
    }
   ],
   "source": [
    "# sanity check: this should match some of the data in the External Sheet\n",
    "print(newmetadata[['batch','external_id_validation']].head().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think this cell and the following cell needs to be moved prior to the creation of the sample_info dataframe - we didn't remove *any* samples from TWIST1-4 that were missing these metadata columns\n",
    "We do not include samples that were missing information in any of the following columns in the external sheet:\n",
    "- Collaborator Participant ID\n",
    "- Exported DNA SM-ID\n",
    "- Stock DNA SM-ID\n",
    "- Patient ID <- not sure about adding this requirement, but it will be used when plotting the CNV heat maps\n",
    "- Sample Type\n",
    "- ~~Tumor Type~~ <- this won't be populated for normals.\n",
    "- Original Material Type\n",
    "- Material Type\n",
    "- Primary Disease <- this only works if the normals also have a primary disease associated with them, which they should. Only the technical controls won't have this information.\n",
    "- ~~Media on Tube~~ <- tissue samples won't have a media but we do want to include them\n",
    "- Collection\n",
    "- Tissue Site <- This column should eventually be populated\n",
    "\n",
    "Without this list of metadata, the samples will not be added to Terra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data from the External Sheet(s) and the data from the data source (e.g. Broad genomics delivery)\n",
    "df = pd.concat([newmetadata, newsamples], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since they don't have full data, we will be dropping 0 samples: \n",
      "[]\n",
      "\n",
      "Number of NAs for each required column:\n",
      "Collaborator Participant ID    0\n",
      "Exported DNA SM-ID             0\n",
      "Stock DNA SM-ID                0\n",
      "Participant ID                 0\n",
      "Sample Type                    0\n",
      "Tissue Site                    0\n",
      "Original Material Type         0\n",
      "Material Type                  0\n",
      "Primary Disease                0\n",
      "Collection                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Since they don\\'t have full data, we will be dropping ' + str(len(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist())) + ' samples: \\n' + \n",
    "      str(df.iloc[[j for j,i in enumerate(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().values.sum(1)) if i !=0]].index.tolist()))\n",
    "\n",
    "\n",
    "\n",
    "# examine which columns in the External Sheet had missing information\n",
    "print('\\nNumber of NAs for each required column:')\n",
    "print(df[['Collaborator Participant ID','Exported DNA SM-ID',\n",
    "                                              'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                              'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                              'Collection']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have now dropped the samples for which we didn't have full data\n"
     ]
    }
   ],
   "source": [
    "print('We have now dropped the samples for which we didn\\'t have full data')\n",
    "# only keep samples that have all the appropriate information\n",
    "df = df.iloc[[j for j,i in enumerate(df[['Exported DNA SM-ID','Collaborator Participant ID',\n",
    "                                         'Stock DNA SM-ID', 'Participant ID', 'Sample Type','Tissue Site',\n",
    "                                         'Original Material Type', 'Material Type','Primary Disease',\n",
    "                                         'Collection']].isna().values.sum(1)) if i ==0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new sample information df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we do not have a corresponding cohort for this collection for sample: SM-JJ5S5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "print('creating new sample information df')\n",
    "# from this filtered set of samples (df) we create a dataframe which will get uploaded to terra\n",
    "sample_info = df[['crai_or_bai_path', 'cram_or_bam_path']].copy\n",
    "\n",
    "sample_info['batch'] = df['batch'].astype(str)\n",
    "sample_info['individual_id'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['reference_id'] = df['Exported DNA SM-ID'].astype(str)\n",
    "sample_info['patient_id'] = df['Participant ID'].astype(str) ## cehck: this is a new column in the external sheet. The name may change.\n",
    "sample_info['participant'] = df['Collaborator Participant ID'].astype(str)\n",
    "sample_info['aggregation_product_name_validation'] = df['bait_set'].astype(str)\n",
    "# here we add this number as the reference id might be present many times already for different samples\n",
    "# in the processing workspace\n",
    "\n",
    "# start building external_id_validation column\n",
    "sample_info['external_id_validation'] = 'nan'\n",
    "# for each SM-ID:\n",
    "for i in range(len(sample_info['reference_id'])):\n",
    "    # get the external id for the sample\n",
    "    ext_id_for_sample = df[df.index == sample_info['reference_id'][i]]['external_id_validation'].values[0]\n",
    "    # replace any \"/\" that exist with \"_\"; otherwise get errors because looks like new directory when try to build file paths\n",
    "    ext_id_for_sample = ext_id_for_sample.replace('/', '_') #[ext_id_for_sample.replace('/', '_') for ext_id in ext_id_for_sample]\n",
    "    \n",
    "    # tack on a number to distinguish external IDs that we have run more than once\n",
    "    # using str.contains because we want to ignore the tacked on numbers we've added to the ext_id (e.g. _1, _2)\n",
    "    # num of samples with this ext_id already in the workspace\n",
    "    num_in_workspace = refsamples[refsamples.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "    # num of samples with this ext_id that we've already seen in the data we're adding\n",
    "    try:\n",
    "        num_already_seen_here = sample_info[sample_info.external_id_validation.str.contains(ext_id_for_sample)].shape[0]\n",
    "        num_to_add = num_in_workspace + num_already_seen_here + 1\n",
    "    except:\n",
    "        num_to_add = num_in_workspace + 1\n",
    "    sample_info['external_id_validation'][i] = ext_id_for_sample + '_' + str(num_to_add)\n",
    "\n",
    "sample_info['bsp_sample_id_validation'] = df.index.astype(str)\n",
    "sample_info['stock_sample_id_validation'] = df['Stock DNA SM-ID'].astype(str)\n",
    "sample_info['sample_type'] = df['Sample Type'].astype(str)\n",
    "sample_info['picard_aggregation_type_validation'] = [picard_aggregation_type_validation] * sample_info.shape[0]\n",
    "sample_info['tumor_subtype'] = df['Tumor Type'].astype(str)\n",
    "sample_info['squid_sample_id_validation'] = sample_info['external_id_validation']\n",
    "sample_info['source_subtype_validation'] = df['Original Material Type'].astype(str)\n",
    "sample_info['processed_subtype_validation'] = df['Material Type'].astype(str)\n",
    "sample_info['primary_disease'] = df['Primary Disease'].astype(str)\n",
    "sample_info['media'] = df['Media on Tube'].astype(str)\n",
    "sample_info['Collection'] = df['Collection'].astype(str)\n",
    "# match collection data and error out\n",
    "cohortlist = []\n",
    "for k, val in sample_info['Collection'].iteritems():\n",
    "    res = cohorts[cohorts['Name'] == val]\n",
    "    if len(res) == 0:\n",
    "        print(\"we do not have a corresponding cohort for this collection for sample: \" + str(k))\n",
    "        cohortlist.append('nan')\n",
    "    else:\n",
    "        cohortlist.append(res['ID'].values[0])\n",
    "sample_info['cohorts'] = cohortlist\n",
    "\n",
    "sample_info['tissue_site'] = df['Tissue Site'].astype(str)\n",
    "sample_info['source'] = [source] * sample_info.shape[0]\n",
    "sample_info['sample_id'] = df.index.astype(str)\n",
    "\n",
    "sample_info = sample_info.set_index('sample_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crai_or_bai_path</th>\n",
       "      <th>cram_or_bam_path</th>\n",
       "      <th>batch</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>participant</th>\n",
       "      <th>aggregation_product_name_validation</th>\n",
       "      <th>external_id_validation</th>\n",
       "      <th>bsp_sample_id_validation</th>\n",
       "      <th>...</th>\n",
       "      <th>tumor_subtype</th>\n",
       "      <th>squid_sample_id_validation</th>\n",
       "      <th>source_subtype_validation</th>\n",
       "      <th>processed_subtype_validation</th>\n",
       "      <th>primary_disease</th>\n",
       "      <th>media</th>\n",
       "      <th>Collection</th>\n",
       "      <th>cohorts</th>\n",
       "      <th>tissue_site</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R1</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CY017</td>\n",
       "      <td>SM-JJ5R1</td>\n",
       "      <td>PT-25ESH</td>\n",
       "      <td>CY017</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CY017T_Primary_1</td>\n",
       "      <td>SM-JJ5R1</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CY017T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Melanoma</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R2</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CY016</td>\n",
       "      <td>SM-JJ5R2</td>\n",
       "      <td>PT-25EQR</td>\n",
       "      <td>CY016</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CY016T_Primary_1</td>\n",
       "      <td>SM-JJ5R2</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CY016T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Melanoma</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R3</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_SS1041</td>\n",
       "      <td>SM-JJ5R3</td>\n",
       "      <td>PT-2OS3W</td>\n",
       "      <td>CCLF_SS1041</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_SS1041T_Primary_1</td>\n",
       "      <td>SM-JJ5R3</td>\n",
       "      <td>...</td>\n",
       "      <td>Primary</td>\n",
       "      <td>CCLF_SS1041T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Kidney renal clear cell carcinoma [KIRC]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Sabina Signo...</td>\n",
       "      <td>COHORT_SS</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R4</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_CY1025</td>\n",
       "      <td>SM-JJ5R4</td>\n",
       "      <td>PT-2P6BC</td>\n",
       "      <td>CCLF_CY1025</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_CY1025T_Primary_1</td>\n",
       "      <td>SM-JJ5R4</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CCLF_CY1025T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Skin Cutaneous Melanoma [SKCM]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R5</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_CY1024</td>\n",
       "      <td>SM-JJ5R5</td>\n",
       "      <td>PT-2OSXX</td>\n",
       "      <td>CCLF_CY1024</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_CY1024T_Primary_1</td>\n",
       "      <td>SM-JJ5R5</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CCLF_CY1024T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Skin Cutaneous Melanoma [SKCM]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            crai_or_bai_path  \\\n",
       "sample_id                                                      \n",
       "SM-JJ5R1   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R2   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R3   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R4   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R5   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "\n",
       "                                            cram_or_bam_path        batch  \\\n",
       "sample_id                                                                   \n",
       "SM-JJ5R1   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R2   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R3   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R4   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R5   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "\n",
       "          individual_id reference_id patient_id  participant  \\\n",
       "sample_id                                                      \n",
       "SM-JJ5R1          CY017     SM-JJ5R1   PT-25ESH        CY017   \n",
       "SM-JJ5R2          CY016     SM-JJ5R2   PT-25EQR        CY016   \n",
       "SM-JJ5R3    CCLF_SS1041     SM-JJ5R3   PT-2OS3W  CCLF_SS1041   \n",
       "SM-JJ5R4    CCLF_CY1025     SM-JJ5R4   PT-2P6BC  CCLF_CY1025   \n",
       "SM-JJ5R5    CCLF_CY1024     SM-JJ5R5   PT-2OSXX  CCLF_CY1024   \n",
       "\n",
       "          aggregation_product_name_validation  external_id_validation  \\\n",
       "sample_id                                                               \n",
       "SM-JJ5R1                   BroadPanCancer2019        CY017T_Primary_1   \n",
       "SM-JJ5R2                   BroadPanCancer2019        CY016T_Primary_1   \n",
       "SM-JJ5R3                   BroadPanCancer2019  CCLF_SS1041T_Primary_1   \n",
       "SM-JJ5R4                   BroadPanCancer2019  CCLF_CY1025T_Primary_1   \n",
       "SM-JJ5R5                   BroadPanCancer2019  CCLF_CY1024T_Primary_1   \n",
       "\n",
       "          bsp_sample_id_validation  ... tumor_subtype  \\\n",
       "sample_id                           ...                 \n",
       "SM-JJ5R1                  SM-JJ5R1  ...    Metastatic   \n",
       "SM-JJ5R2                  SM-JJ5R2  ...    Metastatic   \n",
       "SM-JJ5R3                  SM-JJ5R3  ...       Primary   \n",
       "SM-JJ5R4                  SM-JJ5R4  ...    Metastatic   \n",
       "SM-JJ5R5                  SM-JJ5R5  ...    Metastatic   \n",
       "\n",
       "          squid_sample_id_validation source_subtype_validation  \\\n",
       "sample_id                                                        \n",
       "SM-JJ5R1            CY017T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R2            CY016T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R3      CCLF_SS1041T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R4      CCLF_CY1025T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R5      CCLF_CY1024T_Primary_1       Tissue:Fresh Tissue   \n",
       "\n",
       "          processed_subtype_validation  \\\n",
       "sample_id                                \n",
       "SM-JJ5R1               DNA:DNA Somatic   \n",
       "SM-JJ5R2               DNA:DNA Somatic   \n",
       "SM-JJ5R3               DNA:DNA Somatic   \n",
       "SM-JJ5R4               DNA:DNA Somatic   \n",
       "SM-JJ5R5               DNA:DNA Somatic   \n",
       "\n",
       "                                    primary_disease media  \\\n",
       "sample_id                                                   \n",
       "SM-JJ5R1                                   Melanoma   nan   \n",
       "SM-JJ5R2                                   Melanoma   nan   \n",
       "SM-JJ5R3   Kidney renal clear cell carcinoma [KIRC]   nan   \n",
       "SM-JJ5R4             Skin Cutaneous Melanoma [SKCM]   nan   \n",
       "SM-JJ5R5             Skin Cutaneous Melanoma [SKCM]   nan   \n",
       "\n",
       "                                                  Collection    cohorts  \\\n",
       "sample_id                                                                 \n",
       "SM-JJ5R1   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R2   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R3   Cancer Cell Line Factory (CCLF) / Sabina Signo...  COHORT_SS   \n",
       "SM-JJ5R4   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R5   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "\n",
       "          tissue_site source  \n",
       "sample_id                     \n",
       "SM-JJ5R1   Lymph Node   CCLF  \n",
       "SM-JJ5R2   Lymph Node   CCLF  \n",
       "SM-JJ5R3       Kidney   CCLF  \n",
       "SM-JJ5R4   Lymph Node   CCLF  \n",
       "SM-JJ5R5   Lymph Node   CCLF  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check: this should be the sample TSV you plan on uploading to Terra\n",
    "print(sample_info.shape)\n",
    "display(sample_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this chunk to save the sample_info TSV to a file. I highly recommend this when running a pipeline on a new batch.\n",
    "# This way, if anything goes wrong in the workspace, you can fall back to this.\n",
    "\n",
    "## check: create dir if does not exist\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info.to_csv(filepath, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crai_or_bai_path</th>\n",
       "      <th>cram_or_bam_path</th>\n",
       "      <th>batch</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>reference_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>participant</th>\n",
       "      <th>aggregation_product_name_validation</th>\n",
       "      <th>external_id_validation</th>\n",
       "      <th>bsp_sample_id_validation</th>\n",
       "      <th>...</th>\n",
       "      <th>tumor_subtype</th>\n",
       "      <th>squid_sample_id_validation</th>\n",
       "      <th>source_subtype_validation</th>\n",
       "      <th>processed_subtype_validation</th>\n",
       "      <th>primary_disease</th>\n",
       "      <th>media</th>\n",
       "      <th>Collection</th>\n",
       "      <th>cohorts</th>\n",
       "      <th>tissue_site</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R1</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CY017</td>\n",
       "      <td>SM-JJ5R1</td>\n",
       "      <td>PT-25ESH</td>\n",
       "      <td>CY017</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CY017T_Primary_1</td>\n",
       "      <td>SM-JJ5R1</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CY017T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Melanoma</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R2</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CY016</td>\n",
       "      <td>SM-JJ5R2</td>\n",
       "      <td>PT-25EQR</td>\n",
       "      <td>CY016</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CY016T_Primary_1</td>\n",
       "      <td>SM-JJ5R2</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CY016T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Melanoma</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R3</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_SS1041</td>\n",
       "      <td>SM-JJ5R3</td>\n",
       "      <td>PT-2OS3W</td>\n",
       "      <td>CCLF_SS1041</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_SS1041T_Primary_1</td>\n",
       "      <td>SM-JJ5R3</td>\n",
       "      <td>...</td>\n",
       "      <td>Primary</td>\n",
       "      <td>CCLF_SS1041T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Kidney renal clear cell carcinoma [KIRC]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Sabina Signo...</td>\n",
       "      <td>COHORT_SS</td>\n",
       "      <td>Kidney</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R4</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_CY1025</td>\n",
       "      <td>SM-JJ5R4</td>\n",
       "      <td>PT-2P6BC</td>\n",
       "      <td>CCLF_CY1025</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_CY1025T_Primary_1</td>\n",
       "      <td>SM-JJ5R4</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CCLF_CY1025T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Skin Cutaneous Melanoma [SKCM]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM-JJ5R5</th>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...</td>\n",
       "      <td>CCLF_TWIST6</td>\n",
       "      <td>CCLF_CY1024</td>\n",
       "      <td>SM-JJ5R5</td>\n",
       "      <td>PT-2OSXX</td>\n",
       "      <td>CCLF_CY1024</td>\n",
       "      <td>BroadPanCancer2019</td>\n",
       "      <td>CCLF_CY1024T_Primary_1</td>\n",
       "      <td>SM-JJ5R5</td>\n",
       "      <td>...</td>\n",
       "      <td>Metastatic</td>\n",
       "      <td>CCLF_CY1024T_Primary_1</td>\n",
       "      <td>Tissue:Fresh Tissue</td>\n",
       "      <td>DNA:DNA Somatic</td>\n",
       "      <td>Skin Cutaneous Melanoma [SKCM]</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cancer Cell Line Factory (CCLF) / Charles Yoon...</td>\n",
       "      <td>COHORT_CY</td>\n",
       "      <td>Lymph Node</td>\n",
       "      <td>CCLF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            crai_or_bai_path  \\\n",
       "sample_id                                                      \n",
       "SM-JJ5R1   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R2   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R3   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R4   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "SM-JJ5R5   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...   \n",
       "\n",
       "                                            cram_or_bam_path        batch  \\\n",
       "sample_id                                                                   \n",
       "SM-JJ5R1   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R2   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R3   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R4   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "SM-JJ5R5   gs://fc-51648132-739d-4b71-9800-03fb9f9990c6/C...  CCLF_TWIST6   \n",
       "\n",
       "          individual_id reference_id patient_id  participant  \\\n",
       "sample_id                                                      \n",
       "SM-JJ5R1          CY017     SM-JJ5R1   PT-25ESH        CY017   \n",
       "SM-JJ5R2          CY016     SM-JJ5R2   PT-25EQR        CY016   \n",
       "SM-JJ5R3    CCLF_SS1041     SM-JJ5R3   PT-2OS3W  CCLF_SS1041   \n",
       "SM-JJ5R4    CCLF_CY1025     SM-JJ5R4   PT-2P6BC  CCLF_CY1025   \n",
       "SM-JJ5R5    CCLF_CY1024     SM-JJ5R5   PT-2OSXX  CCLF_CY1024   \n",
       "\n",
       "          aggregation_product_name_validation  external_id_validation  \\\n",
       "sample_id                                                               \n",
       "SM-JJ5R1                   BroadPanCancer2019        CY017T_Primary_1   \n",
       "SM-JJ5R2                   BroadPanCancer2019        CY016T_Primary_1   \n",
       "SM-JJ5R3                   BroadPanCancer2019  CCLF_SS1041T_Primary_1   \n",
       "SM-JJ5R4                   BroadPanCancer2019  CCLF_CY1025T_Primary_1   \n",
       "SM-JJ5R5                   BroadPanCancer2019  CCLF_CY1024T_Primary_1   \n",
       "\n",
       "          bsp_sample_id_validation  ... tumor_subtype  \\\n",
       "sample_id                           ...                 \n",
       "SM-JJ5R1                  SM-JJ5R1  ...    Metastatic   \n",
       "SM-JJ5R2                  SM-JJ5R2  ...    Metastatic   \n",
       "SM-JJ5R3                  SM-JJ5R3  ...       Primary   \n",
       "SM-JJ5R4                  SM-JJ5R4  ...    Metastatic   \n",
       "SM-JJ5R5                  SM-JJ5R5  ...    Metastatic   \n",
       "\n",
       "          squid_sample_id_validation source_subtype_validation  \\\n",
       "sample_id                                                        \n",
       "SM-JJ5R1            CY017T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R2            CY016T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R3      CCLF_SS1041T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R4      CCLF_CY1025T_Primary_1       Tissue:Fresh Tissue   \n",
       "SM-JJ5R5      CCLF_CY1024T_Primary_1       Tissue:Fresh Tissue   \n",
       "\n",
       "          processed_subtype_validation  \\\n",
       "sample_id                                \n",
       "SM-JJ5R1               DNA:DNA Somatic   \n",
       "SM-JJ5R2               DNA:DNA Somatic   \n",
       "SM-JJ5R3               DNA:DNA Somatic   \n",
       "SM-JJ5R4               DNA:DNA Somatic   \n",
       "SM-JJ5R5               DNA:DNA Somatic   \n",
       "\n",
       "                                    primary_disease media  \\\n",
       "sample_id                                                   \n",
       "SM-JJ5R1                                   Melanoma   nan   \n",
       "SM-JJ5R2                                   Melanoma   nan   \n",
       "SM-JJ5R3   Kidney renal clear cell carcinoma [KIRC]   nan   \n",
       "SM-JJ5R4             Skin Cutaneous Melanoma [SKCM]   nan   \n",
       "SM-JJ5R5             Skin Cutaneous Melanoma [SKCM]   nan   \n",
       "\n",
       "                                                  Collection    cohorts  \\\n",
       "sample_id                                                                 \n",
       "SM-JJ5R1   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R2   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R3   Cancer Cell Line Factory (CCLF) / Sabina Signo...  COHORT_SS   \n",
       "SM-JJ5R4   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "SM-JJ5R5   Cancer Cell Line Factory (CCLF) / Charles Yoon...  COHORT_CY   \n",
       "\n",
       "          tissue_site source  \n",
       "sample_id                     \n",
       "SM-JJ5R1   Lymph Node   CCLF  \n",
       "SM-JJ5R2   Lymph Node   CCLF  \n",
       "SM-JJ5R3       Kidney   CCLF  \n",
       "SM-JJ5R4   Lymph Node   CCLF  \n",
       "SM-JJ5R5   Lymph Node   CCLF  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the file you just saved\n",
    "filepath = 'temp/sample_infos/%s_sample_info.tsv' % '_'.join(samplesetnames)\n",
    "sample_info = pd.read_csv(filepath, sep = '\\t', na_filter = False)\n",
    "sample_info = sample_info.set_index('sample_id')\n",
    "print(sample_info.shape)\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the pairs\n",
    "Create a TSV of the new pairs information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "normals = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "normalsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "tumors = [r[\"participant\"] for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "tumorsid = [i for i, r in sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "prevtumors = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Tumor\"]\n",
    "prevnormals = [val[\"participant\"] for k, val in refsamples.iterrows() if val.sample_type == \"Normal\"]\n",
    "\n",
    "print(\"creating new pairs...\")\n",
    "# do we have new tumors/normals for our previous ones\n",
    "newpairs = {'pair_id': [], 'case_sample': [], 'control_sample': [], 'participant': [], 'match_type':[]}\n",
    "\n",
    "toreprocess_normals = set(tumors) & set(prevnormals)\n",
    "for val in toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = refsamples[refsamples['participant'] == val][refsamples[\n",
    "              'sample_type'] == 'Normal'].index.tolist()[0]\n",
    "            newpairs['pair_id'].append(tumor_id + '_' + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\")\n",
    "\n",
    "paired = set(tumors) & set(normals)\n",
    "for val in set(tumors) - toreprocess_normals:\n",
    "    if val != 'nan':\n",
    "        for tumor_id in sample_info[sample_info['participant'] == val][sample_info[\n",
    "                'sample_type'] == 'Tumor'].index.tolist():\n",
    "            normal_id = sample_info[(sample_info['participant'] == val) & (sample_info[\n",
    "              'sample_type'] == 'Normal')].index.tolist()[0] if val in paired else 'NA'\n",
    "            newpairs['pair_id'].append(tumor_id + \"_\" + normal_id)\n",
    "            newpairs['case_sample'].append(tumor_id)\n",
    "            newpairs['control_sample'].append(normal_id)\n",
    "            newpairs['participant'].append(val)\n",
    "            newpairs['match_type'].append(\"Tumor_Normal\" if val in paired else 'Tumor_NA')\n",
    "\n",
    "newpairs = pd.DataFrame(newpairs).set_index('pair_id')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pair sets and sample sets\n",
    "\n",
    "In the following cell, we create:\n",
    "- a pair set for each batch\n",
    "- sample sets for each batch \n",
    "- sample sets for each cohort\n",
    "\n",
    "And then we upload these entities to the Terra workspace.\n",
    "\n",
    "**Note:** all the entities (e.g. sample, sample set, participant tsv) need to exist! Else it will raise an error and block further uploads to Terra. You can do this by just uploading TSVs with NA. The below code does this automatically for the sample TSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading new samples...\n",
      "Successfully imported 44 participants.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmarenco/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported 47 samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"uploading new samples...\")\n",
    "wto.upload_samples(sample_info)\n",
    "if not \"NA\" in wto.get_samples().index.tolist():\n",
    "    wto.upload_samples(pd.DataFrame({'sample_id':['NA'], 'participant_id':['NA']}).set_index('sample_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating pairs and pairsets...\n",
      "Successfully imported 39 pairs.\n"
     ]
    }
   ],
   "source": [
    "print(\"creating pairs and pairsets...\")\n",
    "wto.upload_entities('pair', newpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair => case_sample => look into samples => retrieve the batch, assign to the key\n",
    "dict_pairs_per_batch = {}\n",
    "for samplesetname in samplesetnames:\n",
    "    dict_pairs_per_batch[samplesetname] = []\n",
    "    \n",
    "for pair_id, pair in newpairs.iterrows():\n",
    "    case_sample = pair['case_sample']\n",
    "    # Retrieve the batch which sample belongs to\n",
    "    pair_s_batch = sample_info.loc[case_sample]['batch']\n",
    "    dict_pairs_per_batch[pair_s_batch].append(pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WorkspaceManager' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-81aced21b618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplesetnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamplesetnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mterra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddToPairSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesetnames_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_pairs_per_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get appropriate subset of the samples for each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/JKBio/TerraFunction.py\u001b[0m in \u001b[0;36maddToPairSet\u001b[0;34m(workspace, pairsetid, pairs)\u001b[0m\n\u001b[1;32m    213\u001b[0m   \u001b[0;31m# will create new if doesn't already exist, else adds to existing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mprevpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWorkspaceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pair_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpairsetid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/dalmatian/wmanager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, namespace, workspace, timezone, credentials, user_project)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/miniconda3/envs/twist/lib/python3.7/site-packages/dalmatian/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, namespace, workspace, timezone, credentials, user_project)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'America/New_York'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_project\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworkspace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WorkspaceManager' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# create a pair set for each batch. \n",
    "cohorts_per_batch = {}\n",
    "for i in range(len(samplesetnames)):\n",
    "    current_batch = samplesetnames[i]\n",
    "    terra.addToPairSet(wto, samplesetnames_pairs[i], dict_pairs_per_batch[current_batch])\n",
    "    \n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = []\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        pairsamples = newpairs[newpairs['case_sample'].isin(tumorsamplesincohort)].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.append(val)\n",
    "            try:\n",
    "                terra.addToSampleSet(proc_workspace, val, cohortsamples)\n",
    "            except KeyError: # we may not have this set yet\n",
    "                print(\"KeyError for sampleset: \" + str(val))\n",
    "                wto.update_sample_set(val, cohortsamples)\n",
    "        if len(pairsamples)>0:\n",
    "            cohorts_with_pairs.append(val)\n",
    "            try:\n",
    "                terra.addToPairSet(proc_workspace,val, pairsamples)\n",
    "            except KeyError: \n",
    "                # we may not have this set yet\n",
    "                print(\"KeyError for pairset: \" + str(val))\n",
    "                wto.update_pair_set(val, pairsamples)\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch.update(batch_name = cohorts_in_batch)\n",
    "            \n",
    "print(\"creating sample sets...\")\n",
    "# want to create a sample set for each batch\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    # define batch-specific tumors and normals\n",
    "    batch_normals = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Normal\"]\n",
    "    batch_normalsid = [k for k, _ in batch_sample_info.iterrows() if _['sample_type'] == \"Normal\"]\n",
    "    batch_tumors = [r[\"participant\"] for _, r in batch_sample_info.iterrows() if r['sample_type'] == \"Tumor\"]\n",
    "    batch_tumorsid = [k for k,_ in batch_sample_info.iterrows() if _['sample_type'] == \"Tumor\"]\n",
    "    # create batch-level sample sets\n",
    "    ## check: worried that I'll just overwrite whatever samples sets I've made previously.\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_all[i], samples=batch_sample_info.index.tolist())\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_tumors[i], samples=batch_tumorsid)\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=samplesetnames_normals[i], samples=batch_normalsid)\n",
    "\n",
    "# create sample sets for all samples in workspace, and all normals in workspace\n",
    "# Same as cum pon but better\n",
    "normalsid.extend([k for k, _ in refsamples.iterrows() if _.sample_type == \"Normal\"])\n",
    "\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_normals_TWIST\", samples=normalsid)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_normals_TWIST\", sample_ids=normalsid)\n",
    "all_samples = wto.get_samples().index.tolist()\n",
    "all_samples.remove('NA')\n",
    "try:\n",
    "    terra.addToSampleSet(proc_workspace, samplesetid=\"All_samples_TWIST\", samples=all_samples)\n",
    "except KeyError:\n",
    "    wto.update_sample_set(sample_set_id=\"All_samples_TWIST\", sample_ids=all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Terra Worlflows\n",
    "Run Terra workflows to get copy number (CNV) and mutation (SNV) information, and to create copy number heat maps by batch and by cohort.\n",
    "\n",
    "The order of running the workflows is as follows:\n",
    "- RenameBAM_TWIST\n",
    "- CalculateTargetCoverage_PANCAN, \n",
    "    + DepthOfCov_PANCAN\n",
    "- CreatePanelOfNormalsGATK_PANCAN, (edit the output config \"normals_pon attribute\"))\n",
    "    + DepthOfCovQC_PANCAN\n",
    "- CallSomaticCNV_PANCAN (edit the input config to match the output from CreatePanelOfNormalsGATK_PANCAN)\n",
    "- MutationCalling_Normals_TWIST\n",
    "- FilterGermlineVariants_NormalSample_TWIST\n",
    "(edit the \"PoN_name\" config for CreatePoNSNV_Mutect1 and CreatePoNSNV_Mutect2)\n",
    "- CreatePoNSNV_Mutect1, \n",
    "    + CreatePoNSNV_Mutect2\n",
    "- PlotSomaticCNVMaps_PANCAN: we plot CN heat maps for each batch and also for each cohort\n",
    "- SNV_PostProcessing_Normals, \n",
    "    + MutationCalling_Tumors_TWIST (edit the input config to match pon_mutect1, pon_mutect2)\n",
    "- FilterGermlineEvents_TumorSample\n",
    "- SNVPostProcessing_TWIST, \n",
    "    + FNG_Compile_Pileup_Cnt\n",
    "- FNG_Compile_db_slow_download\n",
    "- FNG_Query_db\n",
    "\n",
    "More information about the pipeline exist here: https://cclf.gitbook.io/tsca/\n",
    "\n",
    "**Note 1:** If for som reason, one of the terra submission function gives no output and it does not seem to submit anything to terra, it might be that you have been logged out of terra you will have to reload the workspace manager and package.\n",
    "\n",
    "**Note 2:** If you get the preflight error \"expression and etype must BOTH be None or a string value\", check the workflow configuration using wto.get_config(\"NAME_OF_WORKFLOW\"). This error usually occurs when you pass in expression and etype information, but the etype is already set as the \"rootEntity\" aka the default for the workflow. You can fix this by either changing the workflow configuration in Terra, or by not passing in the etype or expression. If you want to see why this error occurs, look at the preflight function in lapdog.py (https://github.com/broadinstitute/lapdog/blob/master/lapdog/lapdog.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Terra submissions: remember you can only cancel \\n or interact with terra submissions from the Terra website. \\n https://app.terra.bio/#workspaces/\"+proc_workspace.replace(\" \", \"%20\")+\"/job_history\")\n",
    "\n",
    "RenameBAM_TWIST = terra.createManySubmissions(wto, \"RenameBAM_TWIST\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'Rename'\")\n",
    "terra.waitForSubmission(proc_workspace, RenameBAM_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateTargetCoverage_PANCAN = terra.createManySubmissions(wto, \"CalculateTargetCoverage_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "DepthOfCov_PANCAN = terra.createManySubmissions(wto, \"DepthOfCov_PANCAN\", samplesetnames_all, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "\n",
    "print(\"waiting for 'CalculateTargetCoverage' & 'DepthOfCov_PANCAN'\")\n",
    "combined_list = CalculateTargetCoverage_PANCAN + DepthOfCov_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the output config \"normals_pon attribute\" CreatePanelOfNormalsGATK_PANCAN\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated configuration nci-mimoun-bi-org/CreatePanelOfNormalsGATK_PANCAN\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WorkspaceManager' object has no attribute 'create_submissions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1244abe3d81f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# create batch-specific PON to be used for CNVs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mCreatePanelOfNormalsGATK_PANCAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_submissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CreatePanelOfNormalsGATK_PANCAN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesetnames_normals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mDepthOfCovQC_PANCAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_submissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DepthOfCovQC_PANCAN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesetnames_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample_set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'this.samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WorkspaceManager' object has no attribute 'create_submissions'"
     ]
    }
   ],
   "source": [
    "## Update the config for each batch id\n",
    "CreatePanelOfNormalsGATK_PANCAN = []\n",
    "DepthOfCovQC_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config for workflow that creates the PON for CNV calling\n",
    "    createPON_config = wto.get_config('CreatePanelOfNormalsGATK_PANCAN')\n",
    "    # edit the config\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.combined_normals'] = 'workspace.combined_normals_' + batch_id\n",
    "    createPON_config['outputs']['CreatePanelOfNormals.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    createPON_config['outputs']\n",
    "    # update the config in Terra\n",
    "    wto.update_config(createPON_config)\n",
    "    \n",
    "    # create batch-specific PON to be used for CNVs\n",
    "    CreatePanelOfNormalsGATK_PANCAN.append(wto.create_submissions(\"CreatePanelOfNormalsGATK_PANCAN\", samplesetnames_normals[ind]))\n",
    "    DepthOfCovQC_PANCAN.append(wto.create_submissions(\"DepthOfCovQC_PANCAN\", samplesetnames_all[ind], entity='sample_set', expression='this.samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'DepthOfCovQC_PANCAN' & 'CNV_CreatePoNForCNV'\")\n",
    "combined_list = DepthOfCovQC_PANCAN + CreatePanelOfNormalsGATK_PANCAN\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the inputs config for CallSomaticCNV_PANCAN so that it uses the correct CallSomaticCNV.normals_pon (batch-specific PONs)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CallSomaticCNV_PANCAN = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    # get current config\n",
    "    CNV_config = wto.get_config('CallSomaticCNV_PANCAN')\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon']\n",
    "\n",
    "    # edit the config\n",
    "    CNV_config['inputs']['CallSomaticCNV.normals_pon'] = 'workspace.pon_normals_' + batch_id\n",
    "    CNV_config['inputs']\n",
    "\n",
    "    # update the config in Terra\n",
    "    wto.update_config(CNV_config)\n",
    "    CallSomaticCNV_PANCAN.append(wto.create_submissions(\"CallSomaticCNV_PANCAN\", samplesetnames_all[ind], entity='sample_set', expression='this.samples', use_callcache = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CallSomaticCNV_PANCAN'\")\n",
    "terra.waitForSubmission(proc_workspace, CallSomaticCNV_PANCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create / re-create cohorts_per_batch dictionary\n",
    "# will use \"cohort_pairsets\" to create cohort-specific SNV tsv and CN heat map\n",
    "cohorts_per_batch = {} # will be dict of cohorts in each batch \n",
    "all_changed_cohorts = set()\n",
    "for i in range(len(samplesetnames)):\n",
    "    # get appropriate subset of the samples for each batch\n",
    "    batch_sample_info = sample_info[sample_info['batch'] == samplesetnames[i]]\n",
    "    cohorts_in_batch = set()\n",
    "    cohorts_with_pairs = [] # check: currently not used.\n",
    "    # for each batch, make pairsets by cohort\n",
    "    for val in cohorts['ID'].values:\n",
    "        cohortsamples = batch_sample_info[batch_sample_info[\"cohorts\"] == val].index.tolist()\n",
    "        tumorsamplesincohort = batch_sample_info[batch_sample_info[\"cohorts\"] == val][batch_sample_info['sample_type']==\"Tumor\"].index.tolist()\n",
    "        if len(cohortsamples)>0:\n",
    "            cohorts_in_batch.update([val])\n",
    "    batch_name = samplesetnames[i]\n",
    "    cohorts_per_batch[batch_name] = cohorts_in_batch\n",
    "    all_changed_cohorts.update(cohorts_in_batch) # add all the new cohorts in this batch to the full list\n",
    "\n",
    "# list of the cohort pairsets affected by the new samples we added\n",
    "all_pairsets = wto.get_pair_sets().index.tolist()\n",
    "cohort_pairsets = set(all_changed_cohorts) - (set(all_changed_cohorts) - set(all_pairsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNV map for each batch\n",
    "terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", samplesetnames_all)\n",
    "# create CNV map for each cohort\n",
    "terra.createManySubmissions(wto, \"PlotSomaticCNVMaps_PANCAN\", list(all_changed_cohorts))\n",
    "\n",
    "print(\"submitted final jobs for CNV pipeline\")\n",
    "print(\"you don't need to wait before moving onto the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MutationCalling_Normals_TWIST = terra.createManySubmissions(wto, \"MutationCalling_Normals_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'MutationCalling_Normals_TWIST'\")\n",
    "terra.waitForSubmission(proc_workspace, MutationCalling_Normals_TWIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had errors when using call caching on TWIST1-3. No errors for TWIST4\n",
    "FilterGermlineVariants_NormalSample_TWIST = terra.createManySubmissions(wto, \"FilterGermlineVariants_NormalSample_TWIST\", samplesetnames_normals, \n",
    "                                              entity='sample_set', expression='this.samples', use_callcache=False)\n",
    "print(\"waiting for 'SNV_FilterGermline'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineVariants_NormalSample_TWIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the \"PoN_name\" config (input) and the \"normals_pon_vcf\" config (output) for both CreatePoNSNV_Mutect1 and CreatePoN_SNV_MuTect2\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current config\n",
    "mutect1_config = wto.get_config('CreatePoNSNV_Mutect1')\n",
    "mutect2_config = wto.get_config('CreatePoN_SNV_MuTect2')\n",
    "\n",
    "# edit the config\n",
    "mutect1_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1\"'\n",
    "mutect2_config['inputs']['CreatePanelOfNormals.PoN_name'] = '\"Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2\"'\n",
    "mutect1_config['outputs']['CreatePanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect1'\n",
    "mutect2_config['outputs']['CreatePanelOfNormals.createPanelOfNormals.normals_pon_vcf'] = 'workspace.Cum_PoN_' + samplesetnames[-1] + '_all_vcf_mutect2'\n",
    "\n",
    "# update the config in Terra\n",
    "wto.update_config(mutect1_config)\n",
    "wto.update_config(mutect2_config)\n",
    "\n",
    "# create PON for SNV from all the normals we have in the workspace so far\n",
    "CreatePoNSNV_Mutect1 = wto.create_submission('CreatePoNSNV_Mutect1', \"All_normals_TWIST\")\n",
    "CreatePoN_SNV_MuTect2 = wto.create_submission('CreatePoN_SNV_MuTect2', \"All_normals_TWIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'CreatePoN_SNV_MuTect2' & 'CreatePoNSNV_Mutect1'\")\n",
    "terra.waitForSubmission(proc_workspace, [CreatePoNSNV_Mutect1, CreatePoN_SNV_MuTect2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: we edit the config for MutationCalling_Tumors_TWIST so that it uses the correct pon_mutect1 and pon_mutect2 (cumulative PONs from the first batch through the current batch)\n",
    "We do this directly in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNV_PostProcessing_Normals = []\n",
    "MutationCalling_Tumors_TWIST = []\n",
    "for ind, batch_id in enumerate(samplesetnames):\n",
    "    \n",
    "    # get config \n",
    "    mutcall_tumor = wto.get_config('MutationCalling_Tumors_TWIST')\n",
    "\n",
    "    # edit the config\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect1'\n",
    "    mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'] = 'workspace.Cum_PoN_' + batch_id + '_all_vcf_mutect2'\n",
    "    \n",
    "    # check config\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect1'])\n",
    "    print(mutcall_tumor['inputs']['MutationCalling_Tumor.pon_mutect2'])\n",
    "    \n",
    "    # update the config in Terra\n",
    "    wto.update_config(mutcall_tumor)\n",
    "\n",
    "    # create submission\n",
    "    SNV_PostProcessing_Normals += wto.create_submissions(\"SNV_PostProcessing_Normals\", samplesetnames_normals[ind])\n",
    "    \n",
    "    MutationCalling_Tumors_TWIST += wto.create_submissions(\"MutationCalling_Tumors_TWIST\", samplesetnames_pairs[ind], entity='pair_set', expression='this.pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"waiting for 'SNV_PostProcessing_Normals' & 'MutationCalling_Tumors_TWIST'\")\n",
    "combined_list = SNV_PostProcessing_Normals + MutationCalling_Tumors_TWIST\n",
    "terra.waitForSubmission(proc_workspace, combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: the workflow needs cohorts with at least 2 acceptable CL to run (if only 1, then the workflow will fail)\n",
    "FilterGermlineEvents_TumorSample = terra.createManySubmissions(wto, 'FilterGermlineEvents_TumorSample', samplesetnames_pairs, 'pair_set', expression='this.pairs')\n",
    "print(\"waiting for 'FilterGermlineEvents_TumorSample'\")\n",
    "terra.waitForSubmission(proc_workspace, FilterGermlineEvents_TumorSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregate SNV tsvs for each batch\n",
    "terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", samplesetnames_pairs)\n",
    "# create aggregate SNV tsvs for each cohort\n",
    "terra.createManySubmissions(wto, \"SNVPostProcessing_TWIST\", list(cohort_pairsets))\n",
    "print(\"Submitted final jobs for SNV pipeline\")\n",
    "\n",
    "# sometimes get space errors when run FNG_Compile_Pileup_Cnt if use 4 GB; changed to 10 GB\n",
    "FNG_Compile_Pileup_Cnt = terra.createManySubmissions(wto, \"FNG_Compile_Pileup_Cnt\", samplesetnames_all, entity='sample_set', expression='this.samples')\n",
    "print(\"waiting for 'FNG_Compile_Pileup_Cnt'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_Pileup_Cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Compile_db_slow_download = wto.create_submission(\"FNG_Compile_db_slow_download\", \"All_samples_TWIST\")\n",
    "print(\"waiting for 'FNG_Compile_db'\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Compile_db_slow_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNG_Query_db = terra.createManySubmissions(wto, \"FNG_Query_db\", samplesetnames_all)\n",
    "print(\"Submitted final FNG Job\")\n",
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(proc_workspace, FNG_Query_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You've finished running through the pipeline!\n",
    "You should have all the SNV, CNV, and FNG results ready in Terra."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "416.8px",
    "left": "812.2px",
    "right": "20px",
    "top": "120px",
    "width": "319.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
