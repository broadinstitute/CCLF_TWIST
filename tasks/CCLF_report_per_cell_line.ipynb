{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce CCLF report with all information for each specified cell line\n",
    "The goal of this notebook is to be able to create a unified HTML report for either:\n",
    "1. All CN and SNV data for a single participant (e.g. PEDS172) across the targeted probe data and WES data\n",
    "    + Different culture conditions, passage number, tumor tissue vs cell line, etc.\n",
    "2. All CN and SNV data for a single patient ID across the targeted probe data and WES data\n",
    "\n",
    "Both of these will make it easier for collaborators and Moony Tseng to analyse the existing data and determine what the next steps should be. The goal is to best serve these individuals and groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire / produce all the data for mutations and copy number\n",
    "Pull from CCLF_WES and the most updated TSCA workspace. Currently, trying to transition to CCLF_targeted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "# import os\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from JKBio import TerraFunction as terra\n",
    "from CCLF_TWIST import CCLF_processing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython\n",
    "from IPython.display import Image, display, HTML\n",
    "import ipdb\n",
    "\n",
    "# Import requirements for making CNV plots\n",
    "from matplotlib import pyplot as plt\n",
    "import cnvlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgrid # interactive tables\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import gcsfs # to be able to read in files from GCS in Python\n",
    "import re # used for regex\n",
    "\n",
    "# Extra options\n",
    "qgrid.set_grid_option('maxVisibleRows', 10)\n",
    "\n",
    "# # Show all code cells outputs\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificSamples_both = [\"CCLF_PEDS1012\",\n",
    "                   \"PEDS172\",\n",
    "                   \"PEDS182\",\n",
    "                   \"PEDS196\",\n",
    "                   \"PEDS204\"]\n",
    "specificSamples_onlyWES = [\"PEDS012\",\n",
    "                   \"PEDS018\",\n",
    "                   \"PEDS110\",\n",
    "                   \"PEDS117\"]\n",
    "## kim sept lines:\n",
    "# specificSamples = specificSamples_both + specificSamples_onlyWES\n",
    "\n",
    "## on hold CCLF lines:\n",
    "specificSamples = [\"CCLF_PEDS1012\",\n",
    "                   \"PEDS012\",\n",
    "                   \"PEDS018\",\n",
    "                   \"PEDS157\",\n",
    "                   \"PEDS167\",\n",
    "                   \"PEDS182\",\n",
    "                   \"PEDS195\",\n",
    "                   \"PEDS196\",\n",
    "                   \"PEDS204\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = \"/Users/gmiller/Documents/Work/GitHub/ccle_processing/ccle_tasks/data/cclf_on_hold/cclf_on_hold.csv\"\n",
    "\n",
    "## kim sept line info:\n",
    "# df = \"/Users/gmiller/Documents/Work/GitHub/ccle_processing/ccle_tasks/data/kim_sept/kim_sample_disease_info.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/3/2020 gather all the existing files\n",
    "CCLF_processing.getReport(datadir = \"gs://cclf_results/\", specificlist = specificSamples, specificlist_disease=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3/3/2020 gather all the existing files\n",
    "CCLF_processing.getReport(datadir = \"gs://cclf_results/targeted/neekesh_TEST/\", specificlist = specificSamples, specificlist_disease=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gather all the existing files\n",
    "# CCLF_processing.getReport(datadir = \"gs://cclf_results/targeted/neekesh_201912/\", specificlist = specificSamples, specificlist_disease=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all the existing files\n",
    "CCLF_processing.getReport(datadir = \"gs://cclf_results/targeted/test/\", specificlist = [\"PEDS172\"], specificlist_disease=df)\n",
    "# CCLF_processing.getReport(datadir = \"gs://cclf_results/targeted/kim_sept_6/\", specificlist = specificSamples, specificlist_disease=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create heat map style copy number plots for each participant. Want to have all the culture conditions, primary tissue, matched normal that exist side by side.\n",
    "\n",
    "We might have to make separate CN heat map for TSCA vs WES samples because can't create sample set containing both since they're in separate workspaces... or at least I think this is problematic. But maybe there's a workaround.\n",
    "\n",
    "* step 1: create sample set for each participant (add each sample_id to a sample set list?)\n",
    "   \n",
    "* step 2: create submission for each participant to generate the CN heat map\n",
    "    + Terra.waitForSubmission needed before step 3\n",
    "    + try/except style?\n",
    "* step 3: copy the image from the workspace into the output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heat map style copy number plots for each participant\n",
    "# want to have all the culture conditions, primary tissue, matched normal that exist side by side\n",
    "\n",
    "# step 1: create sample set for each participant (add each sample_id to a sample set list?)\n",
    "# step 2: create submission for each participant to generate the CN heat map\n",
    "# - Terra.waitForSubmission needed before step 3\n",
    "# - try/except style?\n",
    "# step 3: copy the image from the workspace into the output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m rm -r 'gs://cclf_results/targeted/neekesh_TEST/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty report generation\n",
    "After grabbing and making all of the files we want for a given participant (e.g. PEDS182), we want to make a pretty, interactive report. This will be similar to a README except that we will directly embed tables and images. This involves using Jupyter widgets to create dropdown menus and the like. Here are the main functionalities I'd like:\n",
    "\n",
    "1. kable-like tables that are interactive: sorting, filtering, typing in text or numbers to search, (ability to download sorted/filtered table as a CSV?)\n",
    "2. ability to quickly go to any image in the directory. I want this so that the user can quickly look through the copy number maps (horizontal plots). Ideally, I'd like to be able to select which one(s) I'd like to view. This could be useful if they want to see two or more at once (i.e. to compare two treatment conditions).\n",
    "\n",
    "## Automate generation of separate Jupyter notebook for each participant\n",
    "To do this, we will use Papermill. Papermill automates notebook to notebook generation, and also executes the generated notebook. We may also want to convert the generated notebook to HTML. We can use *nbconvert* for this operation (see https://github.com/jupyter/nbconvert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate automated reports by passing links\n",
    "paths = [\"gs://cclf_results/Neuroblastoma/PEDS157/\",\n",
    "\"gs://cclf_results/Neuroblastoma/PEDS167/\",\n",
    "\"gs://cclf_results/Neuroblastoma/PEDS195/\",\n",
    "\"gs://cclf_results/Osteosarcoma/PEDS182/\",\n",
    "\"gs://cclf_results/Spindle_Cell_Sarcoma/PEDS018/\",\n",
    "\"gs://cclf_results/Wilms_Tumor/CCLF_PEDS1012/\",\n",
    "\"gs://cclf_results/Wilms_Tumor/PEDS012/\",\n",
    "\"gs://cclf_results/Wilms_Tumor/PEDS196/\",\n",
    "\"gs://cclf_results/Wilms_Tumor/PEDS204/\"]      \n",
    "\n",
    "# HTML report name format: 'PEDS157-Neuroblastoma'\n",
    "outname = [os.path.basename(os.path.dirname(i)) + \"-\" + os.path.basename(os.path.dirname(os.path.dirname(i))) for i in paths]\n",
    "outname\n",
    "\n",
    "# # Alternative format: 'Neuroblastoma-PEDS157'\n",
    "# alternate_outname = [i.split(\"cclf_results/\")[1][:-1].replace(\"/\", \"-\") for i in paths]\n",
    "# alternate_outname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.execute_notebook(\n",
    "   'notebooks/notebookA.ipynb', # notebook to execute\n",
    "   'notebooks/temp.ipynb', # temporary notebook that will contains the outputs and will be used to save HTML\n",
    "   report_mode=True, # To hide ingested parameters cells, but not working for me\n",
    "   parameters=dict(filename_ipynb='notebooks/temp.ipynb', \n",
    "                   filename_html='output/notebookA.html')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "\n",
    "for i, path in enumerate(paths[:1]):\n",
    "    report_title = outname[i] \n",
    "    report_html = report_title + \".html\"\n",
    "    \n",
    "    pm.execute_notebook(\n",
    "   'CCLF_report_template-no-mutation-summary.ipynb',\n",
    "        'temp.ipynb',\n",
    "#    report_title + \".ipynb\",\n",
    "   parameters = dict(path=path,\n",
    "                    filename_ipynb='temp.ipynb', \n",
    "                    filename_html='output/'+report_html))\n",
    "    \n",
    "#     # Convert ipynb to HTML with nbconvert\n",
    "#     ! jupyter nbconvert --to html_toc --no-input world_facts_2017.ipynb --output report_html\n",
    "#     ! jupyter nbconvert --ExecutePreprocessor.store_widget_state=True  CCLF_report_template.ipynb --no-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: I may want to show the conversion between gs:// and https://console.cloud.google.com/storage/browser/ so that people who are not comfortable with using terminal will be able to easily browse and download the data in the Google bucket. I just need to make sure we won't have privacy issues (we shouldn't, right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_tables(filepaths):\n",
    "    \"\"\" takes list of filepaths to TSVs and returns dataframes read in by Pandas\"\"\"\n",
    "    return [pd.read_csv(f, sep='\\t') for f in filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path would be the participant-specific path\n",
    "path = \"gs://cclf_results/targeted/neekesh_201912/\" \n",
    "\n",
    "# A list of file paths for the selected participant\n",
    "filepaths = ! gsutil ls -r {path}**\n",
    "\n",
    "# Get all the table filepaths in the bucket\n",
    "table_filepaths = ! gsutil ls -r {path}*.txt # check: will this search recursively for all .txt files?\n",
    "to_add = ! gsutil ls -r {path}**.tsv\n",
    "table_filepaths += to_add\n",
    "\n",
    "# Get all the png filepaths in the bucket\n",
    "img_filepaths = ! gsutil ls -r {path}**.png\n",
    "\n",
    "# Copy all the pngs in the bucket to a tmp folder\n",
    "# TODO: need to delete the files afterwards\n",
    "tempdir='../temp/cclfreport/images/'\n",
    "! gsutil cp -r {path}**.png {tempdir} # copy images from google bucket to local temp folder\n",
    "# local_img_filepaths = ! ls {tempdir}*.png\n",
    "local_img_filepaths = [tempdir + os.path.basename(i) for i in img_filepaths]\n",
    "print(local_img_filepaths)\n",
    "\n",
    "os.chdir(tempdir)\n",
    "local_img_file_names = [os.path.basename(i) for i in local_img_filepaths]\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE\n",
    "# these should match up\n",
    "display(local_img_filepaths[:5])\n",
    "display(local_img_file_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interactive_table(filepath, cols_to_include = None):\n",
    "    \"\"\"Takes single pd dataframe as input\"\"\"\n",
    "    if type(filepath) != pd.core.frame.DataFrame:\n",
    "        raise Exception(\"The function expected a pandas dataframe as input, but got: \", str(type(filepath)))\n",
    "    data = filepath\n",
    "    \n",
    "    # Subset the data to include the specified columns, if any passed in\n",
    "    if cols_to_include is not None:\n",
    "        # The index is the first column listed\n",
    "        index_name = cols_to_include[0]\n",
    "        data = data[cols_to_include]\n",
    "        data.set_index(index_name, inplace=True, drop=True)\n",
    "        if 'keep' in cols_to_include:\n",
    "            data = data.loc[data['keep'] == True]\n",
    "            \n",
    "    # Create and display interactive table\n",
    "    qgrid_widget = qgrid.show_grid(data, show_toolbar=False, grid_options = {'forceFitColumns': False,\n",
    "    'defaultColumnWidth': 150})\n",
    "    display(qgrid_widget)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample information and identifiers\n",
    "This section details the external IDs for all the samples we discovered when searching the existing targeted probe data and WES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_external_ids = ! gsutil ls -r {path}**all_external_ids.tsv\n",
    "all_failed_external_ids = ! gsutil ls -r {path}**all_failed_external_ids.tsv\n",
    "\n",
    "# Read in the tables\n",
    "all_external_ids_df = read_in_tables(all_external_ids)\n",
    "all_failed_external_ids_df = read_in_tables(all_failed_external_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table: all external IDs & associated metadata\n",
    "The below table is sortable and filterable. You can double-click on the cells in the table if you want to copy the contents, like if you wanted to copy the link to the file in the Google storage console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of interactive for each participant, might be nice to combine all into one and add a column for the participant ID. This makes for less waiting and clicking overall.\n",
    "\n",
    "# df1 = pd.read_csv(all_external_ids[0], sep='\\t')\n",
    "df1 = all_external_ids_df[0]\n",
    "df1['participant'] = str(os.path.basename(os.path.dirname(all_external_ids[0])))\n",
    "df1['disease'] = str(os.path.basename(os.path.dirname(os.path.dirname(all_external_ids[0]))))\n",
    "df1['filepath'] = str(os.path.dirname(all_external_ids[0]))\n",
    "df1['link'] = str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', os.path.dirname(all_external_ids[0])))\n",
    "# for filepath in all_external_ids[1:]:\n",
    "dfs_remaining = all_external_ids_df[1:]\n",
    "filepaths_remaining = all_external_ids[1:]\n",
    "for table,filepath in zip(dfs_remaining, filepaths_remaining):\n",
    "    df2 = table\n",
    "    df2['participant'] = str(os.path.basename(os.path.dirname(filepath)))\n",
    "    df2['disease'] = str(os.path.basename(os.path.dirname(os.path.dirname(filepath))))\n",
    "    df2['filepath'] = str(os.path.dirname(filepath))\n",
    "    df2['link'] = str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', os.path.dirname(filepath)))\n",
    "    df1 = pd.concat([df1, df2], ignore_index=True)\n",
    "df1.set_index('participant', drop=True, inplace=True)\n",
    "\n",
    "# print some summary information\n",
    "print(\"We found a total of\", df1.shape[0],\"external IDs that passed the depth of coverage QC.\")\n",
    "\n",
    "# allow for filtering\n",
    "qgrid_widget = qgrid.show_grid(df1, show_toolbar=False, grid_options = {'forceFitColumns': False})\n",
    "display(qgrid_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples that failed the depth of coverage QC\n",
    "This summary details all the the external IDs of each sample that failed the depth of coverage QC in the targeted probe pipeline. The depth of coverage QC in the targeted probe pipeline requires that the average gene-level or interval-level coverage is >=50x. \n",
    "\n",
    "The summary also lists the participants for which no samples failed the depth of coverage QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_failed = []\n",
    "for table,filepath in zip(all_failed_external_ids_df, all_failed_external_ids):\n",
    "    tmp_df = table\n",
    "    participant_name = str(os.path.basename(os.path.dirname(filepath)))\n",
    "    if tmp_df.shape[0] ==1:\n",
    "        print(\"There was\", str(tmp_df.shape[0]), \"failed sample for participant\", participant_name,\":\")\n",
    "        display(sorted(tmp_df.iloc[:,0].tolist()))\n",
    "    elif tmp_df.shape[0] >1:\n",
    "        print(\"There were\", str(tmp_df.shape[0]), \"failed samples for participant\", participant_name)\n",
    "    else:\n",
    "        no_failed += [participant_name]\n",
    "print(\"There were no failed samples for participant(s):\")\n",
    "display(sorted(no_failed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy number data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy number heat maps\n",
    "There are two plots in this section, one for CN data from the targeted probe data and a second for CN data from WES data. To look at any one sample in more detail, you can look either at the corresponding horizontal CN plot in the next section titled \"Copy number horizontal plots\" or look at the CN table (see either the tables below or the TSV available at the link specified in the \"Sample information and identifiers\" section.\n",
    "\n",
    "These tables are searchable and filterable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted CN heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a heat map specific to the samples requested (either per participant basis or per list of participants) using the plotSomaticCNV workflow in Terra\n",
    "# Steps: create a new sample set with the appropriate samples, submit a new job, wait for it to finish, copy the picture to the temp dir (and add it to the list of local files), then display it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WES CN heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the best way to create a CN heat map for the WES samples? create just using the segmented CN tsv I pull in from Terra? create new workflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wes_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TO DELETE\n",
    "table_filepaths = ! gsutil ls -r {path}*.txt # check: will this search recursively for all .txt files?\n",
    "to_add = ! gsutil ls -r {path}**.tsv\n",
    "table_filepaths += to_add\n",
    "\n",
    "# Get all the png filepaths in the bucket\n",
    "img_filepaths = ! gsutil ls -r {path}**.png\n",
    "\n",
    "# Copy all the pngs in the bucket to a tmp folder\n",
    "# TODO: need to delete the files afterwards\n",
    "tempdir='../temp/cclfreport/images/'\n",
    "! gsutil cp -r {path}**.png {tempdir} # copy images from google bucket to local temp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntempdir = '../temp/cclfreport/tables/'\n",
    "local_cn_filepaths = [cntempdir + os.path.basename(i) for i in wes_cn]\n",
    "\n",
    "# Create dict of dicts: {participant_id: {external_id: seg file}}\n",
    "seg_file_dict = dict()\n",
    "seg_columns = [\"external_id\",\"Chromosome\", \"Start\", \"End\", \"Num_Probes\", \"Segment_Mean\"]\n",
    "for filepath in wes_cn:\n",
    "    participant_id = str(os.path.basename(os.path.dirname(filepath)))\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "    df = df.loc[:,seg_columns]\n",
    "    \n",
    "    # Write each external_id level table to a seg file (TSV format)\n",
    "    tmp_path = cntempdir + participant_id + \".tsv\"\n",
    "    df.to_csv(tmp_path, sep = \"\\t\", index = False)\n",
    "    \n",
    "    # Add key:value pair for the seg files for each participant\n",
    "    seg_file_dict[participant_id] = tmp_path\n",
    "    \n",
    "#     # Create dict with external ID and associated seg file (with correct columns) in TSV format\n",
    "#     for ext_id in set(df.loc[:,\"external_id\"]):\n",
    "#         tmp = df[df.loc[:,\"external_id\"] == ext_id]\n",
    "#         tmp = tmp.drop(columns = [\"external_id\"])\n",
    "#         # Write each external_id level table to a seg file (TSV format)\n",
    "#         tmp_path = cntempdir + ext_id + \".tsv\"\n",
    "#         tmp.to_csv(tmp_path, sep = \"\\t\")\n",
    "#         # Add key:value pair for the seg files for each participant\n",
    "#         seg_file_dict[participant_id] = {ext_id: tmp_path}\n",
    "\n",
    "seg_file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert seg files to .cns format so CNVkit can create CN plots\n",
    "for participant, seg_file in seg_file_dict.items():\n",
    "    print(seg_file)\n",
    "    ! cnvkit.py import-seg {seg_file} -d {cntempdir}\n",
    "\n",
    "\n",
    "# cntempdir = '../temp/cclfreport/tables/'\n",
    "# local_cn_filepaths = [cntempdir + os.path.basename(i) for i in wes_cn]\n",
    "\n",
    "# for i,gspath in enumerate(wes_cn):\n",
    "#     # Temporarily download CN seg file locally\n",
    "#     ! gsutil cp {gspath} {cntempdir}\n",
    "    \n",
    "#     # Convert to .cns format\n",
    "#     ! cnvkit.py import-seg {local_cn_filepaths[i]}\n",
    "    \n",
    "# # Remove CN seg files from temporary location\n",
    "# ! rm {local_cn_filepaths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all code cells outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cnvkit.readthedocs.io/en/stable/plots.html\n",
    "from glob import glob\n",
    "segments = [cnvlib.read(f) for f in glob(cntempdir+\"*.cns\")]\n",
    "ax = cnvlib.do_heatmap(segments) # cnvlib.heatmap.do_heatmap(cnarrs, show_range=None, do_desaturate=False\n",
    "ax.set_title(\"All my samples\")\n",
    "plt.rcParams[\"font.size\"] = 9.0\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy number horizontal plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the copy number plot you would like to display from the dropdown menu. The dropdown menu includes CN plots from both targeted probe (TSCA and TWIST) and WES data. The source of the data will be displayed on the title of the image. You can also refer to the table of all external IDs that maps each external ID to the source of the data (see \"Sample information and identifiers\").\n",
    "\n",
    "The dropdown menu also includes merged copy number maps for each participant. This PNG file contains all the horizontal CN plots for a given participant in a single place for ease of quick comparison.\n",
    "\n",
    "**check:** can I add a linked reference to this table so that they can quickly jump there? Might be best to just make it it's own section so that it shows up in the TOC.\n",
    "\n",
    "<!-- Note that to get nice dropdown menu names, I'm changing directories for now. There's probably a better way to do this. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this isn't the most helpful format / layout currently. Make it possible to select by WES vs Targeted, and select by participant as well. I don't currently know how to do this.\n",
    "# select image to display from dropdown menu    \n",
    "@interact\n",
    "def show_images(file=local_img_file_names):\n",
    "    print(\"File name:\", file)\n",
    "    display(Image(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must change back to the main directory\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CN tables from the Google storage bucket\n",
    "tsca_cn = ! gsutil ls -r {path}**copy_number.tsv\n",
    "wes_cn = ! gsutil ls -r {path}**wes_copy_number.tsv\n",
    "\n",
    "# Create dictionary with filepaths as keys and pandas DF as the values\n",
    "tsca_cn_dict = {f:pd.read_csv(f, sep=\"\\t\") for f in tsca_cn}\n",
    "wes_cn_dict = {f:pd.read_csv(f, sep=\"\\t\") for f in wes_cn}\n",
    "\n",
    "# # Read in the tables\n",
    "# tsca_cn_dfs = read_in_tables(tsca_cn)\n",
    "# wes_cn_dfs = read_in_tables(wes_cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted CN table\n",
    "Select from the dropdown menu to get the targeted CN table for each participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old, working version: pass in list of filepaths (\"make_interactive_table_orig_READINFILE\" reads in the file inside the function\n",
    "# @interact\n",
    "# def show_tables(filepath = tsca_cn):\n",
    "#     cn_col_names = ['external_id', 'Sample', 'condition','Chromosome', 'Start', 'End','Segment_Mean', 'Segment_Call', 'Num_Probes']\n",
    "#     print(\"Participant: \", str(os.path.basename(os.path.dirname(filepath))))\n",
    "#     print(\"Filepath: \"+ filepath)\n",
    "#     print(\"Link:\", str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', filepath)))\n",
    "#     make_interactive_table_orig_READINFILE(filepath, cols_to_include = cn_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_tables(filepath = tsca_cn):\n",
    "    \"\"\" pass in dict with keys as filepaths and values as DFs\"\"\"\n",
    "    # Choose columns to include\n",
    "    cn_col_names = ['external_id', 'Sample', 'condition','Chromosome', 'Start', 'End','Segment_Mean', 'Segment_Call', 'Num_Probes']\n",
    "    \n",
    "    # Print key information about the file\n",
    "    print(\"Participant: \", str(os.path.basename(os.path.dirname(filepath))))\n",
    "    print(\"Filepath: \"+ filepath)\n",
    "    print(\"Link:\", str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', filepath)))\n",
    "    \n",
    "    # Get the TSV from the dict and display it\n",
    "    df = tsca_cn_dict[filepath]\n",
    "    make_interactive_table(df, cols_to_include = cn_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WES CN table\n",
    "Select from the dropdown menu to get the WES CN table for each participant, when available. The TSV will contain the data for all the different external IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_tables(filepath = wes_cn):\n",
    "    \"\"\" pass in dict with keys as filepaths and values as DFs\"\"\"\n",
    "    # Choose columns to include\n",
    "    cn_col_names = ['external_id', 'Sample', 'condition','Chromosome', 'Start', 'End','Segment_Mean', 'Segment_Call', 'Num_Probes']\n",
    "    \n",
    "    # Print key information about the file\n",
    "    print(\"Participant: \", str(os.path.basename(os.path.dirname(filepath))))\n",
    "    print(\"Filepath: \"+ filepath)\n",
    "    print(\"Link:\", str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', filepath)))\n",
    "    \n",
    "    # Get the TSV from the dict and display it\n",
    "    df = wes_cn_dict[filepath]\n",
    "    make_interactive_table(df, cols_to_include = cn_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe think about including mutations found in targeted that WEREN'T found in WES. Or, alternatively, just plot a venn diagram. I can only do this for samples where we have both WES and Targeted data - this shouldn't be difficult to figure out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are interactive tables containing *select* mutation information from the targeted probe data and the WES data. If there were multiple external IDs in either dataset, they have been combined into one table. The external_id column can be used to filter the data so only the mutations for a single external ID is displayed.\n",
    "\n",
    "Note that this report only includes samples from the targeted data that pass the depth of coverage QC. Samples that did not pass this QC are not included in this report, and their data is not included in the Google bucket. A list of the samples that failed this QC is included earlier in this document (search for \"Table: failed QC external IDs\").\n",
    "\n",
    "Also, note that the below tables have been filtered such that the keep column equals True. What this means is that only the variants that passed the filtering steps in the pipeline are included in the tables below. However, the raw mutation TSVs included in the Google bucket contain all the variants regardless of whether keep is True or False if you are interested in that information. This TSV will also contain columns explaining why a mutation was removed during filtration.\n",
    "\n",
    "Generally speaking, if you are looking for more detailed information about why a mutation you expected to see was filtered out or if you want to get access to all of the columns available in the mutation TSV rather than the ones selected here, you can download the raw mutation TSV from the Google bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mutation TSVs\n",
    "tsca_mut = ! gsutil ls -r {path}**mutation.tsv\n",
    "wes_mut = ! gsutil ls -r {path}**wes_mutations.tsv\n",
    "\n",
    "# Create dictionary with filepaths as keys and pandas DF as the values\n",
    "tsca_mut_dict = {f:pd.read_csv(f, sep=\"\\t\") for f in tsca_mut}\n",
    "wes_mut_dict = {f:pd.read_csv(f, sep=\"\\t\") for f in wes_mut}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted mutation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code allows for the display of interactive tables with a dropdown menu to switch between participants\n",
    "@interact\n",
    "def show_tables(file=tsca_mut):\n",
    "    # Choose columns to include\n",
    "    mut_col_names = ['external_id', 'Genome_Change', 'Protein_Change','Variant_Classification', 'Variant_Type', 'tumor_f', 't_alt_count', 't_ref_count', 'COSMIC_total_alterations_in_gene',\n",
    "                     'CGC_Tumor_Types_Somatic', 'CGC_Tumor_Types_Germline',\n",
    "                     'Hugo_Symbol','Matched_Norm_Sample_Barcode','condition','Chromosome', 'Start_position', 'End_position','keep']\n",
    "    \n",
    "    # Print key information about the file\n",
    "    print(\"Participant: \", str(os.path.basename(os.path.dirname(file))))\n",
    "    print(\"Filepath: \"+ file)\n",
    "    print(\"Link:\", str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', file)))\n",
    "    \n",
    "    # Get the TSV from the dict and display it\n",
    "    df = tsca_mut_dict[file]\n",
    "    try:\n",
    "        make_interactive_table(df, cols_to_include = mut_col_names)\n",
    "    except:\n",
    "        print(\"\\nAt least one of this participant's mutation file is in a different format from the output of the newest pipeline. This data may be old, and have different column names. No filtering is performed on the displayed table, but you can add additional filters if desired:\")\n",
    "        make_interactive_table(df, cols_to_include = None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WES mutation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code allows for the display of interactive tables with a dropdown menu to switch between participants\n",
    "@interact\n",
    "def show_tables(file=wes_mut):\n",
    "    # Choose columns to include\n",
    "    mut_col_names = ['external_id', 'Genome_Change', 'Protein_Change','Variant_Classification', 'Variant_Type', 'tumor_f', 't_alt_count', 't_ref_count', 'COSMIC_total_alterations_in_gene',\n",
    "                     'CGC_Tumor_Types_Somatic', 'CGC_Tumor_Types_Germline',\n",
    "                     'Hugo_Symbol','Matched_Norm_Sample_Barcode','condition','Chromosome', 'Start_position', 'End_position','keep']\n",
    "    \n",
    "    # Print key information about the file\n",
    "    print(\"Participant: \", str(os.path.basename(os.path.dirname(file))))\n",
    "    print(\"Filepath: \"+ file)\n",
    "    print(\"Link:\", str(re.sub('gs://', 'https://console.cloud.google.com/storage/browser/', file)))\n",
    "    \n",
    "    # Get the TSV from the dict and display it\n",
    "    df = wes_mut_dict[file]\n",
    "    try:\n",
    "        make_interactive_table(df, cols_to_include = mut_col_names)\n",
    "    except:\n",
    "        print(\"\\nAt least one of this participant's mutation file is in a different format from the output of the newest pipeline. This data may be old, and have different column names. No filtering is performed on the displayed table, but you can add additional filters if desired:\")\n",
    "        make_interactive_table(df, cols_to_include = None)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
